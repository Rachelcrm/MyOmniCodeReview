--- a/tests/test_feedexport_empty_files.py
+++ b/tests/test_feedexport_empty_files.py
-# Empty if creating new file
+import os
+import tempfile
+import pytest
+from unittest.mock import Mock, patch
+from scrapy import Spider
+from scrapy.extensions.feedexport import FeedExporter, FileFeedStorage, IFeedStorage
+from scrapy.utils.test import get_crawler
+from scrapy.settings import Settings
+
+
+class TestFileFeedStorageEmptyFiles:
+    """Test FileFeedStorage behavior with empty files when FEED_STORE_EMPTY is False."""
+
+    def test_file_feed_storage_creates_empty_file_bug(self):
+        """Test that demonstrates the bug: FileFeedStorage creates empty file even when no items are scraped."""
+        with tempfile.TemporaryDirectory() as tmpdir:
+            file_path = os.path.join(tmpdir, "test_output.json")
+            file_uri = f"file://{file_path}"
+            
+            # Create storage and open file (simulating what happens in _start_new_batch)
+            storage = FileFeedStorage(file_uri)
+            spider = Spider("test_spider")
+            
+            # This creates the file
+            file_handle = storage.open(spider)
+            file_handle.close()
+            
+            # File should exist after open/close even with no items
+            assert os.path.exists(file_path), "File should be created by storage.open()"
+            
+            # This is the bug: file exists even though no items were written
+            # and FEED_STORE_EMPTY should prevent this
+            storage.store(file_handle)
+            assert os.path.exists(file_path), "Bug: Empty file exists when it shouldn't"
+
+    def test_file_feed_storage_cleanup_method_removes_empty_file(self):
+        """Test that FileFeedStorage.cleanup() removes empty files."""
+        with tempfile.TemporaryDirectory() as tmpdir:
+            file_path = os.path.join(tmpdir, "test_output.json")
+            file_uri = f"file://{file_path}"
+            
+            storage = FileFeedStorage(file_uri)
+            spider = Spider("test_spider")
+            
+            # Create empty file
+            file_handle = storage.open(spider)
+            file_handle.close()
+            assert os.path.exists(file_path), "File should be created"
+            
+            # Test cleanup method (should be implemented to fix the bug)
+            if hasattr(storage, 'cleanup'):
+                storage.cleanup()
+                assert not os.path.exists(file_path), "cleanup() should remove empty file"
+            else:
+                pytest.skip("cleanup() method not yet implemented")
+
+    def test_file_feed_storage_cleanup_method_safe_when_file_not_exists(self):
+        """Test that FileFeedStorage.cleanup() is safe when file doesn't exist."""
+        with tempfile.TemporaryDirectory() as tmpdir:
+            file_path = os.path.join(tmpdir, "nonexistent.json")
+            file_uri = f"file://{file_path}"
+            
+            storage = FileFeedStorage(file_uri)
+            
+            # Test cleanup on non-existent file
+            if hasattr(storage, 'cleanup'):
+                # Should not raise exception
+                storage.cleanup()
+                assert not os.path.exists(file_path)
+            else:
+                pytest.skip("cleanup() method not yet implemented")
+
+    def test_ifeed_storage_interface_has_cleanup_method(self):
+        """Test that IFeedStorage interface includes cleanup method."""
+        # Check if cleanup method is defined in interface
+        if hasattr(IFeedStorage, 'cleanup'):
+            assert callable(getattr(IFeedStorage, 'cleanup'))
+        else:
+            pytest.skip("cleanup() method not yet added to IFeedStorage interface")
+
+
+class TestFeedExporterEmptyFileHandling:
+    """Test FeedExporter behavior with empty files and FEED_STORE_EMPTY setting."""
+
+    def setup_method(self):
+        """Set up test fixtures."""
+        self.tmpdir = tempfile.mkdtemp()
+        self.file_path = os.path.join(self.tmpdir, "test_output.json")
+        self.file_uri = f"file://{self.file_path}"
+
+    def teardown_method(self):
+        """Clean up test fixtures."""
+        import shutil
+        shutil.rmtree(self.tmpdir, ignore_errors=True)
+
+    def test_feed_exporter_creates_empty_file_when_store_empty_false_bug(self):
+        """Test that demonstrates the bug: FeedExporter creates empty files when FEED_STORE_EMPTY=False."""
+        settings = Settings({
+            'FEEDS': {self.file_uri: {'format': 'json'}},
+            'FEED_STORE_EMPTY': False
+        })
+        
+        crawler = get_crawler(settings_dict=settings)
+        exporter = FeedExporter.from_crawler(crawler)
+        spider = Spider("test_spider")
+        
+        # Open spider (creates file)
+        exporter.open_spider(spider)
+        
+        # Close spider without scraping any items
+        import asyncio
+        asyncio.run(exporter.close_spider(spider))
+        
+        # Bug: File should not exist when FEED_STORE_EMPTY=False and no items scraped
+        if os.path.exists(self.file_path):
+            pytest.fail("Bug: Empty file created when FEED_STORE_EMPTY=False")
+
+    def test_feed_exporter_respects_store_empty_true(self):
+        """Test that FeedExporter creates empty files when FEED_STORE_EMPTY=True."""
+        settings = Settings({
+            'FEEDS': {self.file_uri: {'format': 'json'}},
+            'FEED_STORE_EMPTY': True
+        })
+        
+        crawler = get_crawler(settings_dict=settings)
+        exporter = FeedExporter.from_crawler(crawler)
+        spider = Spider("test_spider")
+        
+        # Open spider
+        exporter.open_spider(spider)
+        
+        # Close spider without scraping any items
+        import asyncio
+        asyncio.run(exporter.close_spider(spider))
+        
+        # File should exist when FEED_STORE_EMPTY=True
+        assert os.path.exists(self.file_path), "Empty file should be created when FEED_STORE_EMPTY=True"
+
+    def test_feed_exporter_creates_file_with_items_regardless_of_store_empty(self):
+        """Test that FeedExporter creates files with items regardless of FEED_STORE_EMPTY setting."""
+        settings = Settings({
+            'FEEDS': {self.file_uri: {'format': 'json'}},
+            'FEED_STORE_EMPTY': False
+        })
+        
+        crawler = get_crawler(settings_dict=settings)
+        exporter = FeedExporter.from_crawler(crawler)
+        spider = Spider("test_spider")
+        
+        # Open spider
+        exporter.open_spider(spider)
+        
+        # Scrape an item
+        test_item = {'name': 'test', 'value': 123}
+        exporter.item_scraped(test_item, spider)
+        
+        # Close spider
+        import asyncio
+        asyncio.run(exporter.close_spider(spider))
+        
+        # File should exist when items were scraped
+        assert os.path.exists(self.file_path), "File should be created when items are scraped"
+        
+        # Verify content
+        with open(self.file_path, 'r') as f:
+            content = f.read()
+            assert 'test' in content, "File should contain scraped item data"
+
+    def test_feed_exporter_close_slot_calls_cleanup_when_no_items_and_store_empty_false(self):
+        """Test that _close_slot calls cleanup() when no items scraped and store_empty=False."""
+        settings = Settings({
+            'FEEDS': {self.file_uri: {'format': 'json'}},
+            'FEED_STORE_EMPTY': False
+        })
+        
+        crawler = get_crawler(settings_dict=settings)
+        exporter = FeedExporter.from_crawler(crawler)
+        spider = Spider("test_spider")
+        
+        # Mock storage with cleanup method
+        mock_storage = Mock()
+        mock_storage.cleanup = Mock()
+        mock_storage.store = Mock()
+        
+        # Create a slot with no items
+        from scrapy.extensions.feedexport import FeedSlot
+        mock_file = Mock()
+        mock_exporter = Mock()
+        
+        slot = FeedSlot(
+            file=mock_file,
+            exporter=mock_exporter,
+            storage=mock_storage,
+            uri=self.file_uri,
+            format='json',
+            store_empty=False,
+            batch_id=1,
+            uri_template=self.file_uri,
+            filter=Mock()
+        )
+        slot.itemcount = 0  # No items scraped
+        slot.finish_exporting = Mock()
+        
+        # Test _close_slot behavior
+        result = exporter._close_slot(slot, spider)
+        
+        # Should call cleanup if available, not store
+        if hasattr(mock_storage, 'cleanup'):
+            # After fix: should call cleanup instead of store
+            # Before fix: calls store even when no items and store_empty=False
+            pass  # This test will need to be updated based on actual implementation
+
+    def test_feed_exporter_close_slot_calls_store_when_items_exist(self):
+        """Test that _close_slot calls store() when items were scraped."""
+        settings = Settings({
+            'FEEDS': {self.file_uri: {'format': 'json'}},
+            'FEED_STORE_EMPTY': False
+        })
+        
+        crawler = get_crawler(settings_dict=settings)
+        exporter = FeedExporter.from_crawler(crawler)
+        spider = Spider("test_spider")
+        
+        # Mock storage
+        mock_storage = Mock()
+        mock_storage.store = Mock()
+        
+        # Create a slot with items
+        from scrapy.extensions.feedexport import FeedSlot
+        mock_file = Mock()
+        mock_exporter = Mock()
+        
+        slot = FeedSlot(
+            file=mock_file,
+            exporter=mock_exporter,
+            storage=mock_storage,
+            uri=self.file_uri,
+            format='json',
+            store_empty=False,
+            batch_id=1,
+            uri_template=self.file_uri,
+            filter=Mock()
+        )
+        slot.itemcount = 5  # Items were scraped
+        slot.finish_exporting = Mock()
+        
+        # Test _close_slot behavior
+        exporter._close_slot(slot, spider)
+        
+        # Should always call store when items exist
+        mock_storage.store.assert_called_once()
+
+    def test_feed_exporter_close_slot_calls_store_when_store_empty_true(self):
+        """Test that _close_slot calls store() when store_empty=True even with no items."""
+        settings = Settings({
+            'FEEDS': {self.file_uri: {'format': 'json'}},
+            'FEED_STORE_EMPTY': True
+        })
+        
+        crawler = get_crawler(settings_dict=settings)
+        exporter = FeedExporter.from_crawler(crawler)
+        spider = Spider("test_spider")
+        
+        # Mock storage
+        mock_storage = Mock()
+        mock_storage.store = Mock()
+        
+        # Create a slot with no items but store_empty=True
+        from scrapy.extensions.feedexport import FeedSlot
+        mock_file = Mock()
+        mock_exporter = Mock()
+        
+        slot = FeedSlot(
+            file=mock_file,
+            exporter=mock_exporter,
+            storage=mock_storage,
+            uri=self.file_uri,
+            format='json',
+            store_empty=True,
+            batch_id=1,
+            uri_template=self.file_uri,
+            filter=Mock()
+        )
+        slot.itemcount = 0  # No items scraped
+        slot.finish_exporting = Mock()
+        
+        # Test _close_slot behavior
+        exporter._close_slot(slot, spider)
+        
+        # Should call store when store_empty=True
+        mock_storage.store.assert_called_once()
+
+
+class TestBackwardCompatibility:
+    """Test that the fix maintains backward compatibility."""
+
+    def test_storage_without_cleanup_method_still_works(self):
+        """Test that storage implementations without cleanup() method still work."""
+        # Create a storage class without cleanup method
+        class LegacyStorage:
+            def __init__(self, uri, *, feed_options=None):
+                self.uri = uri
+                
+            def open(self, spider):
+                return Mock()
+                
+            def store(self, file):
+                pass
+        
+        settings = Settings({
+            'FEEDS': {'legacy://test': {'format': 'json'}},
+            'FEED_STORE_EMPTY': False
+        })
+        
+        crawler = get_crawler(settings_dict=settings)
+        exporter = FeedExporter.from_crawler(crawler)
+        
+        # Mock the storage loading to return our legacy storage
+        with patch.object(exporter, '_get_storage', return_value=LegacyStorage('legacy://test')):
+            spider = Spider("test_spider")
+            
+            # Should not raise exception even without cleanup method
+            exporter.open_spider(spider)
+            import asyncio
+            asyncio.run(exporter.close_spider(spider))
