{
  "instance_id": "scikit-learn__scikit-learn-26194",
  "model": "openrouter/anthropic/claude-sonnet-4",
  "response": "```\n<file>sklearn/metrics/tests/test_ranking.py</file>\n<original>\n</original>\n<patched>\nimport numpy as np\nimport pytest\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics._ranking import _binary_clf_curve\n\n\ndef test_roc_curve_with_probability_estimates():\n    \"\"\"Test that roc_curve doesn't produce thresholds > 1 with probability estimates.\n    \n    This is a regression test for the issue where thresholds can exceed 1\n    when providing probability estimates to roc_curve.\n    \"\"\"\n    rng = np.random.RandomState(42)\n    y_true = rng.randint(0, 2, size=10)\n    y_score = rng.rand(10)  # Probability estimates between 0 and 1\n    \n    fpr, tpr, thresholds = roc_curve(y_true, y_score)\n    \n    # All thresholds should be between 0 and 1 for probability estimates\n    assert np.all(thresholds >= 0), f\"Found negative thresholds: {thresholds[thresholds < 0]}\"\n    assert np.all(thresholds <= 1), f\"Found thresholds > 1: {thresholds[thresholds > 1]}\"\n\n\ndef test_roc_curve_with_probability_estimates_edge_cases():\n    \"\"\"Test edge cases for probability estimates in roc_curve.\"\"\"\n    # Test with all zeros\n    y_true = np.array([0, 1, 0, 1])\n    y_score = np.array([0.0, 0.0, 0.0, 0.0])\n    fpr, tpr, thresholds = roc_curve(y_true, y_score)\n    assert np.all(thresholds >= 0)\n    assert np.all(thresholds <= 1)\n    \n    # Test with all ones\n    y_score = np.array([1.0, 1.0, 1.0, 1.0])\n    fpr, tpr, thresholds = roc_curve(y_true, y_score)\n    assert np.all(thresholds >= 0)\n    assert np.all(thresholds <= 1)\n    \n    # Test with mixed probability values\n    y_score = np.array([0.1, 0.9, 0.3, 0.7])\n    fpr, tpr, thresholds = roc_curve(y_true, y_score)\n    assert np.all(thresholds >= 0)\n    assert np.all(thresholds <= 1)\n\n\ndef test_roc_curve_with_decision_function_scores():\n    \"\"\"Test that roc_curve still works correctly with decision function scores.\n    \n    Decision function scores can exceed 1, so this tests backward compatibility.\n    \"\"\"\n    y_true = np.array([0, 1, 0, 1, 0, 1])\n    y_score = np.array([-2.5, 3.2, -1.1, 2.8, -0.5, 4.1])  # Decision function scores\n    \n    fpr, tpr, thresholds = roc_curve(y_true, y_score)\n    \n    # For decision function scores, thresholds can exceed 1\n    # The first threshold should be max(y_score) + 1 to ensure (0,0) start\n    expected_first_threshold = y_score.max() + 1\n    assert thresholds[0] == expected_first_threshold\n    \n    # Verify ROC curve properties\n    assert fpr[0] == 0.0  # Should start at (0, 0)\n    assert tpr[0] == 0.0\n    assert fpr[-1] == 1.0  # Should end at (1, 1)\n    assert tpr[-1] == 1.0\n\n\ndef test_roc_curve_threshold_behavior_comparison():\n    \"\"\"Compare threshold behavior between probability and decision function scores.\"\"\"\n    y_true = np.array([0, 1, 0, 1, 0, 1])\n    \n    # Probability estimates\n    y_prob = np.array([0.1, 0.9, 0.2, 0.8, 0.3, 0.7])\n    fpr_prob, tpr_prob, thresh_prob = roc_curve(y_true, y_prob)\n    \n    # Decision function scores (scaled version of probabilities)\n    y_decision = y_prob * 10 - 5  # Scale to [-5, 5] range\n    fpr_dec, tpr_dec, thresh_dec = roc_curve(y_true, y_decision)\n    \n    # Both should produce valid ROC curves starting at (0,0)\n    assert fpr_prob[0] == 0.0 and tpr_prob[0] == 0.0\n    assert fpr_dec[0] == 0.0 and tpr_dec[0] == 0.0\n    \n    # Probability thresholds should be <= 1\n    assert np.all(thresh_prob <= 1)\n    \n    # Decision function thresholds can exceed 1\n    assert thresh_dec[0] > 1  # First threshold should be max + 1\n\n\ndef test_roc_curve_preserves_curve_properties():\n    \"\"\"Test that the fix preserves essential ROC curve properties.\"\"\"\n    rng = np.random.RandomState(123)\n    y_true = rng.randint(0, 2, size=50)\n    y_score = rng.rand(50)\n    \n    fpr, tpr, thresholds = roc_curve(y_true, y_score)\n    \n    # Essential ROC curve properties\n    assert len(fpr) == len(tpr) == len(thresholds)\n    assert fpr[0] == 0.0  # Starts at (0, 0)\n    assert tpr[0] == 0.0\n    assert fpr[-1] == 1.0  # Ends at (1, 1)\n    assert tpr[-1] == 1.0\n    \n    # FPR and TPR should be non-decreasing\n    assert np.all(np.diff(fpr) >= 0)\n    assert np.all(np.diff(tpr) >= 0)\n    \n    # Thresholds should be non-increasing\n    assert np.all(np.diff(thresholds) <= 0)\n    \n    # For probability estimates, all thresholds should be valid\n    assert np.all(thresholds >= 0)\n    assert np.all(thresholds <= 1)\n\n\ndef test_roc_curve_drop_intermediate_with_probabilities():\n    \"\"\"Test drop_intermediate parameter works correctly with probability estimates.\"\"\"\n    y_true = np.array([0, 0, 1, 1, 0, 1, 0, 1])\n    y_score = np.array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8])\n    \n    # Test with drop_intermediate=True (default)\n    fpr1, tpr1, thresh1 = roc_curve(y_true, y_score, drop_intermediate=True)\n    \n    # Test with drop_intermediate=False\n    fpr2, tpr2, thresh2 = roc_curve(y_true, y_score, drop_intermediate=False)\n    \n    # Both should have valid thresholds for probability estimates\n    assert np.all(thresh1 >= 0) and np.all(thresh1 <= 1)\n    assert np.all(thresh2 >= 0) and np.all(thresh2 <= 1)\n    \n    # drop_intermediate=False should have more or equal points\n    assert len(thresh2) >= len(thresh1)\n\n\ndef test_roc_curve_binary_clf_curve_consistency():\n    \"\"\"Test that _binary_clf_curve output is handled correctly in roc_curve.\"\"\"\n    y_true = np.array([0, 1, 0, 1, 0])\n    y_score = np.array([0.2, 0.8, 0.3, 0.9, 0.1])\n    \n    # Get raw output from _binary_clf_curve\n    fps, tps, raw_thresholds = _binary_clf_curve(y_true, y_score)\n    \n    # Get processed output from roc_curve\n    fpr, tpr, processed_thresholds = roc_curve(y_true, y_score)\n    \n    # The processed thresholds should include the added threshold\n    assert len(processed_thresholds) == len(raw_thresholds) + 1\n    \n    # For probability estimates, the first threshold should not exceed 1\n    assert processed_thresholds[0] <= 1.0\n    \n    # The remaining thresholds should match the raw thresholds\n    np.testing.assert_array_equal(processed_thresholds[1:], raw_thresholds)\n</patched>\n```",
  "patch": "--- a/sklearn/metrics/tests/test_ranking.py\n+++ b/sklearn/metrics/tests/test_ranking.py\n+import numpy as np\n+import pytest\n+from sklearn.metrics import roc_curve\n+from sklearn.metrics._ranking import _binary_clf_curve\n+\n+\n+def test_roc_curve_with_probability_estimates():\n+    \"\"\"Test that roc_curve doesn't produce thresholds > 1 with probability estimates.\n+    \n+    This is a regression test for the issue where thresholds can exceed 1\n+    when providing probability estimates to roc_curve.\n+    \"\"\"\n+    rng = np.random.RandomState(42)\n+    y_true = rng.randint(0, 2, size=10)\n+    y_score = rng.rand(10)  # Probability estimates between 0 and 1\n+    \n+    fpr, tpr, thresholds = roc_curve(y_true, y_score)\n+    \n+    # All thresholds should be between 0 and 1 for probability estimates\n+    assert np.all(thresholds >= 0), f\"Found negative thresholds: {thresholds[thresholds < 0]}\"\n+    assert np.all(thresholds <= 1), f\"Found thresholds > 1: {thresholds[thresholds > 1]}\"\n+\n+\n+def test_roc_curve_with_probability_estimates_edge_cases():\n+    \"\"\"Test edge cases for probability estimates in roc_curve.\"\"\"\n+    # Test with all zeros\n+    y_true = np.array([0, 1, 0, 1])\n+    y_score = np.array([0.0, 0.0, 0.0, 0.0])\n+    fpr, tpr, thresholds = roc_curve(y_true, y_score)\n+    assert np.all(thresholds >= 0)\n+    assert np.all(thresholds <= 1)\n+    \n+    # Test with all ones\n+    y_score = np.array([1.0, 1.0, 1.0, 1.0])\n+    fpr, tpr, thresholds = roc_curve(y_true, y_score)\n+    assert np.all(thresholds >= 0)\n+    assert np.all(thresholds <= 1)\n+    \n+    # Test with mixed probability values\n+    y_score = np.array([0.1, 0.9, 0.3, 0.7])\n+    fpr, tpr, thresholds = roc_curve(y_true, y_score)\n+    assert np.all(thresholds >= 0)\n+    assert np.all(thresholds <= 1)\n+\n+\n+def test_roc_curve_with_decision_function_scores():\n+    \"\"\"Test that roc_curve still works correctly with decision function scores.\n+    \n+    Decision function scores can exceed 1, so this tests backward compatibility.\n+    \"\"\"\n+    y_true = np.array([0, 1, 0, 1, 0, 1])\n+    y_score = np.array([-2.5, 3.2, -1.1, 2.8, -0.5, 4.1])  # Decision function scores\n+    \n+    fpr, tpr, thresholds = roc_curve(y_true, y_score)\n+    \n+    # For decision function scores, thresholds can exceed 1\n+    # The first threshold should be max(y_score) + 1 to ensure (0,0) start\n+    expected_first_threshold = y_score.max() + 1\n+    assert thresholds[0] == expected_first_threshold\n+    \n+    # Verify ROC curve properties\n+    assert fpr[0] == 0.0  # Should start at (0, 0)\n+    assert tpr[0] == 0.0\n+    assert fpr[-1] == 1.0  # Should end at (1, 1)\n+    assert tpr[-1] == 1.0\n+\n+\n+def test_roc_curve_threshold_behavior_comparison():\n+    \"\"\"Compare threshold behavior between probability and decision function scores.\"\"\"\n+    y_true = np.array([0, 1, 0, 1, 0, 1])\n+    \n+    # Probability estimates\n+    y_prob = np.array([0.1, 0.9, 0.2, 0.8, 0.3, 0.7])\n+    fpr_prob, tpr_prob, thresh_prob = roc_curve(y_true, y_prob)\n+    \n+    # Decision function scores (scaled version of probabilities)\n+    y_decision = y_prob * 10 - 5  # Scale to [-5, 5] range\n+    fpr_dec, tpr_dec, thresh_dec = roc_curve(y_true, y_decision)\n+    \n+    # Both should produce valid ROC curves starting at (0,0)\n+    assert fpr_prob[0] == 0.0 and tpr_prob[0] == 0.0\n+    assert fpr_dec[0] == 0.0 and tpr_dec[0] == 0.0\n+    \n+    # Probability thresholds should be <= 1\n+    assert np.all(thresh_prob <= 1)\n+    \n+    # Decision function thresholds can exceed 1\n+    assert thresh_dec[0] > 1  # First threshold should be max + 1\n+\n+\n+def test_roc_curve_preserves_curve_properties():\n+    \"\"\"Test that the fix preserves essential ROC curve properties.\"\"\"\n+    rng = np.random.RandomState(123)\n+    y_true = rng.randint(0, 2, size=50)\n+    y_score = rng.rand(50)\n+    \n+    fpr, tpr, thresholds = roc_curve(y_true, y_score)\n+    \n+    # Essential ROC curve properties\n+    assert len(fpr) == len(tpr) == len(thresholds)\n+    assert fpr[0] == 0.0  # Starts at (0, 0)\n+    assert tpr[0] == 0.0\n+    assert fpr[-1] == 1.0  # Ends at (1, 1)\n+    assert tpr[-1] == 1.0\n+    \n+    # FPR and TPR should be non-decreasing\n+    assert np.all(np.diff(fpr) >= 0)\n+    assert np.all(np.diff(tpr) >= 0)\n+    \n+    # Thresholds should be non-increasing\n+    assert np.all(np.diff(thresholds) <= 0)\n+    \n+    # For probability estimates, all thresholds should be valid\n+    assert np.all(thresholds >= 0)\n+    assert np.all(thresholds <= 1)\n+\n+\n+def test_roc_curve_drop_intermediate_with_probabilities():\n+    \"\"\"Test drop_intermediate parameter works correctly with probability estimates.\"\"\"\n+    y_true = np.array([0, 0, 1, 1, 0, 1, 0, 1])\n+    y_score = np.array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8])\n+    \n+    # Test with drop_intermediate=True (default)\n+    fpr1, tpr1, thresh1 = roc_curve(y_true, y_score, drop_intermediate=True)\n+    \n+    # Test with drop_intermediate=False\n+    fpr2, tpr2, thresh2 = roc_curve(y_true, y_score, drop_intermediate=False)\n+    \n+    # Both should have valid thresholds for probability estimates\n+    assert np.all(thresh1 >= 0) and np.all(thresh1 <= 1)\n+    assert np.all(thresh2 >= 0) and np.all(thresh2 <= 1)\n+    \n+    # drop_intermediate=False should have more or equal points\n+    assert len(thresh2) >= len(thresh1)\n+\n+\n+def test_roc_curve_binary_clf_curve_consistency():\n+    \"\"\"Test that _binary_clf_curve output is handled correctly in roc_curve.\"\"\"\n+    y_true = np.array([0, 1, 0, 1, 0])\n+    y_score = np.array([0.2, 0.8, 0.3, 0.9, 0.1])\n+    \n+    # Get raw output from _binary_clf_curve\n+    fps, tps, raw_thresholds = _binary_clf_curve(y_true, y_score)\n+    \n+    # Get processed output from roc_curve\n+    fpr, tpr, processed_thresholds = roc_curve(y_true, y_score)\n+    \n+    # The processed thresholds should include the added threshold\n+    assert len(processed_thresholds) == len(raw_thresholds) + 1\n+    \n+    # For probability estimates, the first threshold should not exceed 1\n+    assert processed_thresholds[0] <= 1.0\n+    \n+    # The remaining thresholds should match the raw thresholds\n+    np.testing.assert_array_equal(processed_thresholds[1:], raw_thresholds)\n"
}