{
  "patch": "diff --git a/examples/decomposition/plot_sparse_coding.py b/examples/decomposition/plot_sparse_coding.py\nindex 528817ad0c..4bb4dd3409 100644\n--- a/examples/decomposition/plot_sparse_coding.py\n+++ b/examples/decomposition/plot_sparse_coding.py\n@@ -24,21 +24,21 @@ import matplotlib.pyplot as plt\n from sklearn.decomposition import SparseCoder\n \n \n-def ricker_function(resolution, center, width):\n+def ricker_function(res, center, w):\n     \"\"\"Discrete sub-sampled Ricker (Mexican hat) wavelet\"\"\"\n-    x = np.linspace(0, resolution - 1, resolution)\n-    x = ((2 / ((np.sqrt(3 * width) * np.pi ** 1 / 4)))\n-         * (1 - ((x - center) ** 2 / width ** 2))\n-         * np.exp((-(x - center) ** 2) / (2 * width ** 2)))\n+    x = np.linspace(0, res - 1, res)\n+    x = ((2 / ((np.sqrt(3 * w) * np.pi ** 1 / 4)))\n+         * (1 - ((x - center) ** 2 / w ** 2))\n+         * np.exp((-(x - center) ** 2) / (2 * w ** 2)))\n     return x\n \n \n-def ricker_matrix(width, resolution, n_components):\n+def ricker_matrix(w, res, n_comp):\n     \"\"\"Dictionary of Ricker (Mexican hat) wavelets\"\"\"\n-    centers = np.linspace(0, resolution - 1, n_components)\n-    D = np.empty((n_components, resolution))\n+    centers = np.linspace(0, res - 1, n_comp)\n+    D = np.empty((n_comp, res))\n     for i, center in enumerate(centers):\n-        D[i] = ricker_function(resolution, center, width)\n+        D[i] = ricker_function(res, center, w)\n     D /= np.sqrt(np.sum(D ** 2, axis=1))[:, np.newaxis]\n     return D\n \n@@ -71,13 +71,13 @@ lw = 2\n lstsq_rcond = None if LooseVersion(np.__version__) >= '1.14' else -1\n \n plt.figure(figsize=(13, 6))\n-for subplot, (D, title) in enumerate(zip((D_fixed, D_multi),\n+for subplot, (D, dict_title) in enumerate(zip((D_fixed, D_multi),\n                                          ('fixed width', 'multiple widths'))):\n     plt.subplot(1, 2, subplot + 1)\n-    plt.title('Sparse coding against %s dictionary' % title)\n+    plt.title('Sparse coding against %s dictionary' % dict_title)\n     plt.plot(y, lw=lw, linestyle='--', label='Original signal')\n     # Do a wavelet approximation\n-    for title, algo, alpha, n_nonzero, color in estimators:\n+    for method_title, algo, alpha, n_nonzero, color in estimators:\n         coder = SparseCoder(dictionary=D, transform_n_nonzero_coefs=n_nonzero,\n                             transform_alpha=alpha, transform_algorithm=algo)\n         x = coder.transform(y.reshape(1, -1))\n@@ -85,8 +85,8 @@ for subplot, (D, title) in enumerate(zip((D_fixed, D_multi),\n         x = np.ravel(np.dot(x, D))\n         squared_error = np.sum((y - x) ** 2)\n         plt.plot(x, color=color, lw=lw,\n-                 label='%s: %s nonzero coefs,\\n%.2f error'\n-                 % (title, density, squared_error))\n+                 label='%s: %d nonzero coefs,\\n%.2f error'\n+                 % (method_title, density, squared_error))\n \n     # Soft thresholding debiasing\n     coder = SparseCoder(dictionary=D, transform_algorithm='threshold',\ndiff --git a/sklearn/decomposition/dict_learning.py b/sklearn/decomposition/dict_learning.py\nindex a318c957fe..83d786c2c9 100644\n--- a/sklearn/decomposition/dict_learning.py\n+++ b/sklearn/decomposition/dict_learning.py\n@@ -119,6 +119,7 @@ def _sparse_encode(X, dictionary, gram, cov=None, algorithm='lasso_lars',\n \n     if algorithm == 'lasso_lars':\n         alpha = float(regularization) / n_features  # account for scaling\n+        err_mgt = None\n         try:\n             err_mgt = np.seterr(all='ignore')\n \n@@ -131,7 +132,8 @@ def _sparse_encode(X, dictionary, gram, cov=None, algorithm='lasso_lars',\n             lasso_lars.fit(dictionary.T, X.T, Xy=cov)\n             new_code = lasso_lars.coef_\n         finally:\n-            np.seterr(**err_mgt)\n+            if err_mgt is not None:\n+                np.seterr(**err_mgt)\n \n     elif algorithm == 'lasso_cd':\n         alpha = float(regularization) / n_features  # account for scaling\n@@ -150,6 +152,7 @@ def _sparse_encode(X, dictionary, gram, cov=None, algorithm='lasso_lars',\n         new_code = clf.coef_\n \n     elif algorithm == 'lars':\n+        err_mgt = None\n         try:\n             err_mgt = np.seterr(all='ignore')\n \n@@ -161,7 +164,8 @@ def _sparse_encode(X, dictionary, gram, cov=None, algorithm='lasso_lars',\n             lars.fit(dictionary.T, X.T, Xy=cov)\n             new_code = lars.coef_\n         finally:\n-            np.seterr(**err_mgt)\n+            if err_mgt is not None:\n+                np.seterr(**err_mgt)\n \n     elif algorithm == 'threshold':\n         new_code = ((np.sign(cov) *\n@@ -1191,7 +1195,7 @@ class DictionaryLearning(BaseEstimator, SparseCodingMixin):\n         else:\n             n_components = self.n_components\n \n-        V, U, E, self.n_iter_ = dict_learning(\n+        _, U, E, self.n_iter_ = dict_learning(\n             X, n_components, self.alpha,\n             tol=self.tol, max_iter=self.max_iter,\n             method=self.fit_algorithm,\n"
}