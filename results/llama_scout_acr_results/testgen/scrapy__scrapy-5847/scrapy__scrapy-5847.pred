{
  "instance_id": "scrapy__scrapy-5847",
  "model": "openrouter/meta-llama/llama-4-scout",
  "response": "To address the issue of `FileFeedStorage` creating an empty file when no items are scraped, and to ensure that the `FEED_STORE_EMPTY` setting is respected, we need to modify the `FeedExporter` and `FileFeedStorage` classes.\n\nHere are the test modifications:\n\n```\n<file>tests/test_feedexport.py</file>\n<original>\n</original>\n<patched>\nimport pytest\nfrom scrapy.crawler import CrawlerProcess\nfrom scrapy.exceptions import ScrapyException\nfrom scrapy.utils.testproc import ProcessTest\n\nclass TestFeedExport:\n    @pytest.fixture\n    def crawler_process(self):\n        return CrawlerProcess(settings_dict={\"FEED_FORMAT\": \"json\"})\n\n    def test_store_empty(self, tmp_path):\n        # Test that store_empty=False does not create an empty file\n        process = CrawlerProcess(\n            settings_dict={\n                \"FEED_FORMAT\": \"json\",\n                \"FEED_FILE\": str(tmp_path / \"empty.json\"),\n                \"FEED_STORE_EMPTY\": False,\n            }\n        )\n        process.crawl(DummySpider)\n        process.start()\n\n        assert not (tmp_path / \"empty.json\").exists()\n\n    def test_store_non_empty(self, tmp_path):\n        # Test that store_empty=True creates a non-empty file\n        process = CrawlerProcess(\n            settings_dict={\n                \"FEED_FORMAT\": \"json\",\n                \"FEED_FILE\": str(tmp_path / \"non_empty.json\"),\n                \"FEED_STORE_EMPTY\": True,\n            }\n        )\n        process.crawl(ItemSpider)\n        process.start()\n\n        assert (tmp_path / \"non_empty.json\").exists()\n\n    def test_default_store_empty(self, tmp_path):\n        # Test that default store_empty=False does not create an empty file\n        process = CrawlerProcess(\n            settings_dict={\n                \"FEED_FORMAT\": \"json\",\n                \"FEED_FILE\": str(tmp_path / \"default_empty.json\"),\n            }\n        )\n        process.crawl(DummySpider)\n        process.start()\n\n        assert not (tmp_path / \"default_empty.json\").exists()\n\n\nclass DummySpider:\n    def start_requests(self):\n        yield {\"dummy\": \"item\"}\n\n\nclass ItemSpider:\n    def start_requests(self):\n        yield {\"item\": \"1\"}\n        yield {\"item\": \"2\"}\n</patched>\n```\n\n```\n<file>tests/test_filefeedstorage.py</file>\n<original>\n</original>\n<patched>\nimport pytest\nfrom scrapy.storage import FileFeedStorage\n\ndef test_file_feed_storage_cleanup(tmp_path):\n    # Test that FileFeedStorage cleanup removes empty files\n    storage = FileFeedStorage(str(tmp_path / \"test.json\"))\n    file = storage.open(None)\n    storage.store(file)\n    storage.cleanup()\n\n    assert not (tmp_path / \"test.json\").exists()\n</patched>\n```",
  "patch": "--- a/tests/test_feedexport.py\n+++ b/tests/test_feedexport.py\n+import pytest\n+from scrapy.crawler import CrawlerProcess\n+from scrapy.exceptions import ScrapyException\n+from scrapy.utils.testproc import ProcessTest\n+\n+class TestFeedExport:\n+    @pytest.fixture\n+    def crawler_process(self):\n+        return CrawlerProcess(settings_dict={\"FEED_FORMAT\": \"json\"})\n+\n+    def test_store_empty(self, tmp_path):\n+        # Test that store_empty=False does not create an empty file\n+        process = CrawlerProcess(\n+            settings_dict={\n+                \"FEED_FORMAT\": \"json\",\n+                \"FEED_FILE\": str(tmp_path / \"empty.json\"),\n+                \"FEED_STORE_EMPTY\": False,\n+            }\n+        )\n+        process.crawl(DummySpider)\n+        process.start()\n+\n+        assert not (tmp_path / \"empty.json\").exists()\n+\n+    def test_store_non_empty(self, tmp_path):\n+        # Test that store_empty=True creates a non-empty file\n+        process = CrawlerProcess(\n+            settings_dict={\n+                \"FEED_FORMAT\": \"json\",\n+                \"FEED_FILE\": str(tmp_path / \"non_empty.json\"),\n+                \"FEED_STORE_EMPTY\": True,\n+            }\n+        )\n+        process.crawl(ItemSpider)\n+        process.start()\n+\n+        assert (tmp_path / \"non_empty.json\").exists()\n+\n+    def test_default_store_empty(self, tmp_path):\n+        # Test that default store_empty=False does not create an empty file\n+        process = CrawlerProcess(\n+            settings_dict={\n+                \"FEED_FORMAT\": \"json\",\n+                \"FEED_FILE\": str(tmp_path / \"default_empty.json\"),\n+            }\n+        )\n+        process.crawl(DummySpider)\n+        process.start()\n+\n+        assert not (tmp_path / \"default_empty.json\").exists()\n+\n+\n+class DummySpider:\n+    def start_requests(self):\n+        yield {\"dummy\": \"item\"}\n+\n+\n+class ItemSpider:\n+    def start_requests(self):\n+        yield {\"item\": \"1\"}\n+        yield {\"item\": \"2\"}\n\n--- a/tests/test_filefeedstorage.py\n+++ b/tests/test_filefeedstorage.py\n+import pytest\n+from scrapy.storage import FileFeedStorage\n+\n+def test_file_feed_storage_cleanup(tmp_path):\n+    # Test that FileFeedStorage cleanup removes empty files\n+    storage = FileFeedStorage(str(tmp_path / \"test.json\"))\n+    file = storage.open(None)\n+    storage.store(file)\n+    storage.cleanup()\n+\n+    assert not (tmp_path / \"test.json\").exists()\n"
}