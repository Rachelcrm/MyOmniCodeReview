{
  "instance_id": "scrapy__scrapy-5833",
  "model": "openrouter/meta-llama/llama-4-scout",
  "response": "To address the issue of the S3 Feed Export throwing a \"Connection Reset By Peer\" error when uploading large files, we need to modify the `S3FeedStorage` class to use `boto3` instead of `botocore` and implement a multipart upload mechanism.\n\nHere's a comprehensive test suite that covers the described issue:\n\n```\n<file>tests/test_feedexport.py</file>\n<original>\n# Empty if creating new file\n</original>\n<patched>\nimport pytest\nimport boto3\nfrom moto import mock_s3\nfrom scrapy.exceptions import NotConfigured\nfrom scrapy.utils.test import get_crawler\nfrom scrapy.settings import Settings\nfrom scrapy.extensions.feedexport import S3FeedStorage\n\n@pytest.fixture\ndef crawler():\n    return get_crawler(settings_dict={\"FEED_STORAGE_S3_ACL\": \"private\"})\n\ndef test_s3_feed_storage_init(crawler):\n    storage = S3FeedStorage(\"s3://bucket/key\", crawler=crawler)\n    assert storage.bucketname == \"bucket\"\n    assert storage.keyname == \"key\"\n\ndef test_s3_feed_storage_from_crawler(crawler):\n    storage = S3FeedStorage.from_crawler(crawler, \"s3://bucket/key\")\n    assert storage.bucketname == \"bucket\"\n    assert storage.keyname == \"key\"\n\ndef test_s3_feed_storage_store_in_thread(tmp_path):\n    bucket_name = \"bucket\"\n    key_name = \"key\"\n    file_path = tmp_path / \"test_file\"\n\n    with open(file_path, \"w\") as f:\n        f.write(\"Test content\")\n\n    with mock_s3():\n        s3 = boto3.client(\"s3\", region_name=\"us-east-1\")\n        s3.create_bucket(Bucket=bucket_name)\n\n        storage = S3FeedStorage(f\"s3://{bucket_name}/{key_name}\")\n        storage._store_in_thread(file_path)\n\n        response = s3.get_object(Bucket=bucket_name, Key=key_name)\n        assert response[\"Body\"].read().decode() == \"Test content\"\n\ndef test_s3_feed_storage_store_in_thread_multipart(tmp_path):\n    bucket_name = \"bucket\"\n    key_name = \"key\"\n    file_path = tmp_path / \"test_file\"\n\n    with open(file_path, \"w\") as f:\n        f.write(\"Test content\" * 1000000)  # Create a large file\n\n    with mock_s3():\n        s3 = boto3.client(\"s3\", region_name=\"us-east-1\")\n        s3.create_bucket(Bucket=bucket_name)\n\n        storage = S3FeedStorage(f\"s3://{bucket_name}/{key_name}\")\n        storage._store_in_thread(file_path)\n\n        response = s3.get_object(Bucket=bucket_name, Key=key_name)\n        assert response[\"Body\"].read().decode() == \"Test content\" * 1000000\n\ndef test_s3_feed_storage_not_configured():\n    with pytest.raises(NotConfigured):\n        S3FeedStorage(\"s3://bucket/key\")\n\ndef test_s3_feed_storage_acl(crawler):\n    settings = Settings({\"FEED_STORAGE_S3_ACL\": \"public-read\"})\n    crawler = get_crawler(settings_dict=settings)\n    storage = S3FeedStorage.from_crawler(crawler, \"s3://bucket/key\")\n    assert storage.acl == \"public-read\"\n\ndef test_s3_feed_storage_endpoint_url(crawler):\n    settings = Settings({\"AWS_ENDPOINT_URL\": \"https://example.com\"})\n    crawler = get_crawler(settings_dict=settings)\n    storage = S3FeedStorage.from_crawler(crawler, \"s3://bucket/key\")\n    assert storage.endpoint_url == \"https://example.com\"\n</patched>\n```",
  "patch": "--- a/tests/test_feedexport.py\n+++ b/tests/test_feedexport.py\n-# Empty if creating new file\n+import pytest\n+import boto3\n+from moto import mock_s3\n+from scrapy.exceptions import NotConfigured\n+from scrapy.utils.test import get_crawler\n+from scrapy.settings import Settings\n+from scrapy.extensions.feedexport import S3FeedStorage\n+\n+@pytest.fixture\n+def crawler():\n+    return get_crawler(settings_dict={\"FEED_STORAGE_S3_ACL\": \"private\"})\n+\n+def test_s3_feed_storage_init(crawler):\n+    storage = S3FeedStorage(\"s3://bucket/key\", crawler=crawler)\n+    assert storage.bucketname == \"bucket\"\n+    assert storage.keyname == \"key\"\n+\n+def test_s3_feed_storage_from_crawler(crawler):\n+    storage = S3FeedStorage.from_crawler(crawler, \"s3://bucket/key\")\n+    assert storage.bucketname == \"bucket\"\n+    assert storage.keyname == \"key\"\n+\n+def test_s3_feed_storage_store_in_thread(tmp_path):\n+    bucket_name = \"bucket\"\n+    key_name = \"key\"\n+    file_path = tmp_path / \"test_file\"\n+\n+    with open(file_path, \"w\") as f:\n+        f.write(\"Test content\")\n+\n+    with mock_s3():\n+        s3 = boto3.client(\"s3\", region_name=\"us-east-1\")\n+        s3.create_bucket(Bucket=bucket_name)\n+\n+        storage = S3FeedStorage(f\"s3://{bucket_name}/{key_name}\")\n+        storage._store_in_thread(file_path)\n+\n+        response = s3.get_object(Bucket=bucket_name, Key=key_name)\n+        assert response[\"Body\"].read().decode() == \"Test content\"\n+\n+def test_s3_feed_storage_store_in_thread_multipart(tmp_path):\n+    bucket_name = \"bucket\"\n+    key_name = \"key\"\n+    file_path = tmp_path / \"test_file\"\n+\n+    with open(file_path, \"w\") as f:\n+        f.write(\"Test content\" * 1000000)  # Create a large file\n+\n+    with mock_s3():\n+        s3 = boto3.client(\"s3\", region_name=\"us-east-1\")\n+        s3.create_bucket(Bucket=bucket_name)\n+\n+        storage = S3FeedStorage(f\"s3://{bucket_name}/{key_name}\")\n+        storage._store_in_thread(file_path)\n+\n+        response = s3.get_object(Bucket=bucket_name, Key=key_name)\n+        assert response[\"Body\"].read().decode() == \"Test content\" * 1000000\n+\n+def test_s3_feed_storage_not_configured():\n+    with pytest.raises(NotConfigured):\n+        S3FeedStorage(\"s3://bucket/key\")\n+\n+def test_s3_feed_storage_acl(crawler):\n+    settings = Settings({\"FEED_STORAGE_S3_ACL\": \"public-read\"})\n+    crawler = get_crawler(settings_dict=settings)\n+    storage = S3FeedStorage.from_crawler(crawler, \"s3://bucket/key\")\n+    assert storage.acl == \"public-read\"\n+\n+def test_s3_feed_storage_endpoint_url(crawler):\n+    settings = Settings({\"AWS_ENDPOINT_URL\": \"https://example.com\"})\n+    crawler = get_crawler(settings_dict=settings)\n+    storage = S3FeedStorage.from_crawler(crawler, \"s3://bucket/key\")\n+    assert storage.endpoint_url == \"https://example.com\"\n"
}