[
  {
    "repo": "scrapy/scrapy",
    "pull_number": 6579,
    "instance_id": "scrapy__scrapy-6579",
    "issue_numbers": [
      "6578"
    ],
    "base_commit": "c330a399dcc69f6d51fcfbe397fbc42b5a9ee323",
    "patch": "diff --git a/pyproject.toml b/pyproject.toml\nindex 8c985753fce..571a61f1c81 100644\n--- a/pyproject.toml\n+++ b/pyproject.toml\n@@ -72,9 +72,9 @@ version = {file = \"./scrapy/VERSION\"}\n \n [tool.mypy]\n ignore_missing_imports = true\n+implicit_reexport = false\n \n # Interface classes are hard to support\n-\n [[tool.mypy.overrides]]\n module = \"twisted.internet.interfaces\"\n follow_imports = \"skip\"\n@@ -92,6 +92,14 @@ follow_imports = \"skip\"\n module = \"scrapy.settings.default_settings\"\n ignore_errors = true\n \n+[[tool.mypy.overrides]]\n+module = \"itemadapter\"\n+implicit_reexport = true\n+\n+[[tool.mypy.overrides]]\n+module = \"twisted\"\n+implicit_reexport = true\n+\n [tool.bumpversion]\n current_version = \"2.12.0\"\n commit = true\n@@ -359,13 +367,9 @@ ignore = [\n ]\n \n [tool.ruff.lint.per-file-ignores]\n-# Exclude files that are meant to provide top-level imports\n-\"scrapy/__init__.py\" = [\"E402\"]\n-\"scrapy/core/downloader/handlers/http.py\" = [\"F401\"]\n-\"scrapy/http/__init__.py\" = [\"F401\"]\n-\"scrapy/linkextractors/__init__.py\" = [\"E402\", \"F401\"]\n-\"scrapy/selector/__init__.py\" = [\"F401\"]\n-\"scrapy/spiders/__init__.py\" = [\"E402\", \"F401\"]\n+# Circular import workarounds\n+\"scrapy/linkextractors/__init__.py\" = [\"E402\"]\n+\"scrapy/spiders/__init__.py\" = [\"E402\"]\n \n # Skip bandit in tests\n \"tests/**\" = [\"S\"]\ndiff --git a/scrapy/core/downloader/handlers/http.py b/scrapy/core/downloader/handlers/http.py\nindex 52535bd8b58..93b96c779d1 100644\n--- a/scrapy/core/downloader/handlers/http.py\n+++ b/scrapy/core/downloader/handlers/http.py\n@@ -2,3 +2,8 @@\n from scrapy.core.downloader.handlers.http11 import (\n     HTTP11DownloadHandler as HTTPDownloadHandler,\n )\n+\n+__all__ = [\n+    \"HTTP10DownloadHandler\",\n+    \"HTTPDownloadHandler\",\n+]\ndiff --git a/scrapy/http/__init__.py b/scrapy/http/__init__.py\nindex d0b726bad90..0e5c2b53b05 100644\n--- a/scrapy/http/__init__.py\n+++ b/scrapy/http/__init__.py\n@@ -15,3 +15,16 @@\n from scrapy.http.response.json import JsonResponse\n from scrapy.http.response.text import TextResponse\n from scrapy.http.response.xml import XmlResponse\n+\n+__all__ = [\n+    \"FormRequest\",\n+    \"Headers\",\n+    \"HtmlResponse\",\n+    \"JsonRequest\",\n+    \"JsonResponse\",\n+    \"Request\",\n+    \"Response\",\n+    \"TextResponse\",\n+    \"XmlResponse\",\n+    \"XmlRpcRequest\",\n+]\ndiff --git a/scrapy/linkextractors/__init__.py b/scrapy/linkextractors/__init__.py\nindex 1c7e96ae0df..b39859f7b31 100644\n--- a/scrapy/linkextractors/__init__.py\n+++ b/scrapy/linkextractors/__init__.py\n@@ -126,3 +126,8 @@ def _is_valid_url(url: str) -> bool:\n \n # Top-level imports\n from scrapy.linkextractors.lxmlhtml import LxmlLinkExtractor as LinkExtractor\n+\n+__all__ = [\n+    \"IGNORED_EXTENSIONS\",\n+    \"LinkExtractor\",\n+]\ndiff --git a/scrapy/selector/__init__.py b/scrapy/selector/__init__.py\nindex 85c500d6665..7cfa3c36439 100644\n--- a/scrapy/selector/__init__.py\n+++ b/scrapy/selector/__init__.py\n@@ -4,3 +4,8 @@\n \n # top-level imports\n from scrapy.selector.unified import Selector, SelectorList\n+\n+__all__ = [\n+    \"Selector\",\n+    \"SelectorList\",\n+]\ndiff --git a/scrapy/spiders/__init__.py b/scrapy/spiders/__init__.py\nindex 6136dabc70a..e255e91cc1f 100644\n--- a/scrapy/spiders/__init__.py\n+++ b/scrapy/spiders/__init__.py\n@@ -117,3 +117,12 @@ def __repr__(self) -> str:\n from scrapy.spiders.crawl import CrawlSpider, Rule\n from scrapy.spiders.feed import CSVFeedSpider, XMLFeedSpider\n from scrapy.spiders.sitemap import SitemapSpider\n+\n+__all__ = [\n+    \"CSVFeedSpider\",\n+    \"CrawlSpider\",\n+    \"Rule\",\n+    \"SitemapSpider\",\n+    \"Spider\",\n+    \"XMLFeedSpider\",\n+]\n",
    "test_patch": "diff --git a/tests/test_item.py b/tests/test_item.py\nindex 5a8ee095e61..4804128417a 100644\n--- a/tests/test_item.py\n+++ b/tests/test_item.py\n@@ -1,7 +1,8 @@\n import unittest\n+from abc import ABCMeta\n from unittest import mock\n \n-from scrapy.item import ABCMeta, Field, Item, ItemMeta\n+from scrapy.item import Field, Item, ItemMeta\n \n \n class ItemTest(unittest.TestCase):\ndiff --git a/tests/test_utils_url.py b/tests/test_utils_url.py\nindex 94a59f8835e..314082742cf 100644\n--- a/tests/test_utils_url.py\n+++ b/tests/test_utils_url.py\n@@ -6,7 +6,7 @@\n from scrapy.linkextractors import IGNORED_EXTENSIONS\n from scrapy.spiders import Spider\n from scrapy.utils.misc import arg_to_iter\n-from scrapy.utils.url import (\n+from scrapy.utils.url import (  # type: ignore[attr-defined]\n     _is_filesystem_path,\n     _public_w3lib_objects,\n     add_http_if_no_scheme,\n",
    "problem_statement": "pyright reports scrapy imports as private\n<!--\r\n\r\nThanks for taking an interest in Scrapy!\r\n\r\nIf you have a question that starts with \"How to...\", please see the Scrapy Community page: https://scrapy.org/community/.\r\nThe GitHub issue tracker's purpose is to deal with bug reports and feature requests for the project itself.\r\n\r\nKeep in mind that by filing an issue, you are expected to comply with Scrapy's Code of Conduct, including treating everyone with respect: https://github.com/scrapy/scrapy/blob/master/CODE_OF_CONDUCT.md\r\n\r\nThe following is a suggested template to structure your issue, you can find more guidelines at https://doc.scrapy.org/en/latest/contributing.html#reporting-bugs\r\n\r\n-->\r\n\r\n### Description\r\n\r\nWhen importing various classes and methods from the documented import locations, pyright reports that the class/method isn't exported from that module with the `reportPrivateImportUsage` error. These classes include, but are not limited to:\r\n\r\n* `scrapy.http.Request`\r\n* `scrapy.http.Response`\r\n* `itemadapter.ItemAdapter`\r\n* `itemadapter.is_item`\r\n\r\n### Steps to Reproduce\r\n\r\n1. Create a new scrapy project with `scrapy startproject`\r\n2. Optionally, create a new spider or otherwise add an import `from scrapy.http import Response` somewhere\r\n3. Install pyright (`pip install pyright`) \r\n4. Run pyright \r\n\r\nI created a sample here: https://github.com/paulcwatts/scrapy-pyright-bug\r\n\r\n**Expected behavior:** \r\n\r\npyright to run without errors.\r\n\r\n**Actual behavior:** \r\n\r\npyright returns errors regarding scrapy imports:\r\n\r\n\r\n```\r\n/.../pyright_bug/middlewares.py\r\n  /.../pyright_bug/middlewares.py:9:25 - error: \"is_item\" is not exported from module \"itemadapter\"\r\n  \u00a0\u00a0Import from \"itemadapter.utils\" instead (reportPrivateImportUsage)\r\n  /.../pyright_bug/middlewares.py:9:34 - error: \"ItemAdapter\" is not exported from module \"itemadapter\"\r\n  \u00a0\u00a0Import from \"itemadapter.adapter\" instead (reportPrivateImportUsage)\r\n/.../pyright_bug/pipelines.py\r\n  /.../pyright_bug/pipelines.py:8:25 - error: \"ItemAdapter\" is not exported from module \"itemadapter\"\r\n  \u00a0\u00a0Import from \"itemadapter.adapter\" instead (reportPrivateImportUsage)\r\n/.../pyright_bug/spiders/my_spider.py\r\n  /.../pyright_bug/spiders/my_spider.py:2:25 - error: \"Response\" is not exported from module \"scrapy.http\"\r\n  \u00a0\u00a0Import from \"scrapy.http.response\" instead (reportPrivateImportUsage)\r\n4 errors, 0 warnings, 0 informations \r\n```\r\n\r\n**Reproduces how often:** \r\n\r\n100%\r\n\r\n### Versions\r\n\r\n```\r\nScrapy       : 2.12.0\r\nlxml         : 5.3.0.0\r\nlibxml2      : 2.12.9\r\ncssselect    : 1.2.0\r\nparsel       : 1.9.1\r\nw3lib        : 2.2.1\r\nTwisted      : 24.11.0\r\nPython       : 3.12.4 | packaged by Anaconda, Inc. | (main, Jun 18 2024, 10:07:17) [Clang 14.0.6 ]\r\npyOpenSSL    : 24.3.0 (OpenSSL 3.4.0 22 Oct 2024)\r\ncryptography : 44.0.0\r\nPlatform     : macOS-15.1.1-arm64-arm-64bit\r\n```\r\n\r\nIn addition, the result of `pyright --version`\r\n\r\n```\r\npyright 1.1.390\r\n```\r\n\r\n### Additional context\r\n\r\nThis is the meaning of the error according to the [pyright documentation](https://microsoft.github.io/pyright/#/configuration?id=main-configuration-options):\r\n\r\n> reportPrivateImportUsage [boolean or string, optional]: Generate or suppress diagnostics for use of a symbol from a \"py.typed\" > module that is not meant to be exported from that module. The default value for this setting is \"error\".\r\n\r\nI'm not certain of this, but I think this means is that any import that is meant to be considered public should appear in an `__all__` statement. \r\n\n",
    "hints_text": "We don't currently plan to make the codebase error-free with type checkers other than `mypy`. In this specific case I also wonder how popular is disabling that diagnostic, as such things are controversial.\r\n\r\nAlso note that `itemadapter` is a separate project.\nThat's fair, although it's not possible with pyright to disable this rule for a specific import across an entire project: you can either disable the rule for a specific line, for a list of files/directories, or for the entire project. This forces any scrapy project that uses pyright to disable that rule for their *entire project*, or to litter their imports with `pyright: ignore` statements, or to use workarounds.\r\n\r\nOne workaround I could imagine is to bundle all scrapy imports into a single project file, ignore the diagnostic in that file, and then use that file as the scrapy import path across the project, rather than the actual scrapy import path.\r\n\nI think we should mark re-exports explicitly, I just don't like the options we have for that :)",
    "created_at": "2024-12-11T10:27:07Z",
    "version": "2.12",
    "PASS_TO_PASS": [],
    "FAIL_TO_PASS": [
      "tests/test_item.py::ItemTest::test_copy",
      "tests/test_item.py::ItemTest::test_custom_methods",
      "tests/test_item.py::ItemTest::test_deepcopy",
      "tests/test_item.py::ItemTest::test_init",
      "tests/test_item.py::ItemTest::test_invalid_field",
      "tests/test_item.py::ItemTest::test_metaclass",
      "tests/test_item.py::ItemTest::test_metaclass_inheritance",
      "tests/test_item.py::ItemTest::test_metaclass_multiple_inheritance_diamond",
      "tests/test_item.py::ItemTest::test_metaclass_multiple_inheritance_simple",
      "tests/test_item.py::ItemTest::test_metaclass_multiple_inheritance_without_metaclass",
      "tests/test_item.py::ItemTest::test_metaclass_with_fields_attribute",
      "tests/test_item.py::ItemTest::test_private_attr",
      "tests/test_item.py::ItemTest::test_raise_getattr",
      "tests/test_item.py::ItemTest::test_raise_setattr",
      "tests/test_item.py::ItemTest::test_repr",
      "tests/test_item.py::ItemTest::test_simple",
      "tests/test_item.py::ItemTest::test_to_dict",
      "tests/test_item.py::ItemMetaTest::test_new_method_propagates_classcell",
      "tests/test_item.py::ItemMetaClassCellRegression::test_item_meta_classcell_regression",
      "tests/test_utils_url.py::UrlUtilsTest::test_url_has_any_extension",
      "tests/test_utils_url.py::UrlUtilsTest::test_url_is_from_any_domain",
      "tests/test_utils_url.py::UrlUtilsTest::test_url_is_from_spider",
      "tests/test_utils_url.py::UrlUtilsTest::test_url_is_from_spider_class_attributes",
      "tests/test_utils_url.py::UrlUtilsTest::test_url_is_from_spider_with_allowed_domains",
      "tests/test_utils_url.py::UrlUtilsTest::test_url_is_from_spider_with_allowed_domains_class_attributes",
      "tests/test_utils_url.py::AddHttpIfNoScheme::test_add_scheme",
      "tests/test_utils_url.py::AddHttpIfNoScheme::test_complete_url",
      "tests/test_utils_url.py::AddHttpIfNoScheme::test_fragment",
      "tests/test_utils_url.py::AddHttpIfNoScheme::test_path",
      "tests/test_utils_url.py::AddHttpIfNoScheme::test_port",
      "tests/test_utils_url.py::AddHttpIfNoScheme::test_preserve_ftp",
      "tests/test_utils_url.py::AddHttpIfNoScheme::test_preserve_http",
      "tests/test_utils_url.py::AddHttpIfNoScheme::test_preserve_http_complete_url",
      "tests/test_utils_url.py::AddHttpIfNoScheme::test_preserve_http_fragment",
      "tests/test_utils_url.py::AddHttpIfNoScheme::test_preserve_http_path",
      "tests/test_utils_url.py::AddHttpIfNoScheme::test_preserve_http_port",
      "tests/test_utils_url.py::AddHttpIfNoScheme::test_preserve_http_query",
      "tests/test_utils_url.py::AddHttpIfNoScheme::test_preserve_http_username_password",
      "tests/test_utils_url.py::AddHttpIfNoScheme::test_preserve_http_without_subdomain",
      "tests/test_utils_url.py::AddHttpIfNoScheme::test_preserve_https",
      "tests/test_utils_url.py::AddHttpIfNoScheme::test_protocol_relative",
      "tests/test_utils_url.py::AddHttpIfNoScheme::test_protocol_relative_complete_url",
      "tests/test_utils_url.py::AddHttpIfNoScheme::test_protocol_relative_fragment",
      "tests/test_utils_url.py::AddHttpIfNoScheme::test_protocol_relative_path",
      "tests/test_utils_url.py::AddHttpIfNoScheme::test_protocol_relative_port",
      "tests/test_utils_url.py::AddHttpIfNoScheme::test_protocol_relative_query",
      "tests/test_utils_url.py::AddHttpIfNoScheme::test_protocol_relative_username_password",
      "tests/test_utils_url.py::AddHttpIfNoScheme::test_protocol_relative_without_subdomain",
      "tests/test_utils_url.py::AddHttpIfNoScheme::test_query",
      "tests/test_utils_url.py::AddHttpIfNoScheme::test_username_password",
      "tests/test_utils_url.py::AddHttpIfNoScheme::test_without_subdomain",
      "tests/test_utils_url.py::GuessSchemeTest::test_uri_001",
      "tests/test_utils_url.py::GuessSchemeTest::test_uri_002",
      "tests/test_utils_url.py::GuessSchemeTest::test_uri_003",
      "tests/test_utils_url.py::GuessSchemeTest::test_uri_004",
      "tests/test_utils_url.py::GuessSchemeTest::test_uri_005",
      "tests/test_utils_url.py::GuessSchemeTest::test_uri_006",
      "tests/test_utils_url.py::GuessSchemeTest::test_uri_007",
      "tests/test_utils_url.py::GuessSchemeTest::test_uri_008",
      "tests/test_utils_url.py::GuessSchemeTest::test_uri_009",
      "tests/test_utils_url.py::GuessSchemeTest::test_uri_010",
      "tests/test_utils_url.py::GuessSchemeTest::test_uri_011",
      "tests/test_utils_url.py::GuessSchemeTest::test_uri_012",
      "tests/test_utils_url.py::GuessSchemeTest::test_uri_013",
      "tests/test_utils_url.py::GuessSchemeTest::test_uri_014",
      "tests/test_utils_url.py::GuessSchemeTest::test_uri_015",
      "tests/test_utils_url.py::GuessSchemeTest::test_uri_016",
      "tests/test_utils_url.py::GuessSchemeTest::test_uri_017",
      "tests/test_utils_url.py::GuessSchemeTest::test_uri_018",
      "tests/test_utils_url.py::GuessSchemeTest::test_uri_019",
      "tests/test_utils_url.py::GuessSchemeTest::test_uri_020"
    ]
  },
  {
    "repo": "scrapy/scrapy",
    "pull_number": 6547,
    "instance_id": "scrapy__scrapy-6547",
    "issue_numbers": [
      "6514",
      "6520"
    ],
    "base_commit": "efb53aafdcaae058962c6189ddecb3dc62b02c31",
    "patch": "diff --git a/.bandit.yml b/.bandit.yml\ndeleted file mode 100644\nindex b7f1817e034..00000000000\n--- a/.bandit.yml\n+++ /dev/null\n@@ -1,7 +0,0 @@\n-skips:\n-- B101  # assert_used, needed for mypy\n-- B321  # ftplib, https://github.com/scrapy/scrapy/issues/4180\n-- B402  # import_ftplib, https://github.com/scrapy/scrapy/issues/4180\n-- B411  # import_xmlrpclib, https://github.com/PyCQA/bandit/issues/1082\n-- B503  # ssl_with_bad_defaults\n-exclude_dirs: ['tests']\ndiff --git a/.bumpversion.cfg b/.bumpversion.cfg\ndeleted file mode 100644\nindex f83e3e890bf..00000000000\n--- a/.bumpversion.cfg\n+++ /dev/null\n@@ -1,11 +0,0 @@\n-[bumpversion]\n-current_version = 2.12.0\n-commit = True\n-tag = True\n-tag_name = {new_version}\n-\n-[bumpversion:file:scrapy/VERSION]\n-\n-[bumpversion:file:SECURITY.md]\n-parse = (?P<major>\\d+)\\.(?P<minor>\\d+)\\.x\n-serialize = {major}.{minor}.x\ndiff --git a/.coveragerc b/.coveragerc\ndeleted file mode 100644\nindex f9ad353d54f..00000000000\n--- a/.coveragerc\n+++ /dev/null\n@@ -1,12 +0,0 @@\n-[run]\n-branch = true\n-include = scrapy/*\n-omit =\n-  tests/*\n-disable_warnings = include-ignored\n-\n-[report]\n-# https://github.com/nedbat/coveragepy/issues/831#issuecomment-517778185\n-exclude_lines =\n-    pragma: no cover\n-    if TYPE_CHECKING:\ndiff --git a/.isort.cfg b/.isort.cfg\ndeleted file mode 100644\nindex f238bf7ea13..00000000000\n--- a/.isort.cfg\n+++ /dev/null\n@@ -1,2 +0,0 @@\n-[settings]\n-profile = black\ndiff --git a/.pre-commit-config.yaml b/.pre-commit-config.yaml\nindex fbd710f6f92..b411f492710 100644\n--- a/.pre-commit-config.yaml\n+++ b/.pre-commit-config.yaml\n@@ -3,7 +3,8 @@ repos:\n   rev: 1.7.9\n   hooks:\n   - id: bandit\n-    args: [-r, -c, .bandit.yml]\n+    args: [\"-c\", \"pyproject.toml\"]\n+    additional_dependencies: [\"bandit[toml]\"]\n - repo: https://github.com/PyCQA/flake8\n   rev: 7.1.0\n   hooks:\ndiff --git a/MANIFEST.in b/MANIFEST.in\nindex 06971e39c80..7700ae7bd81 100644\n--- a/MANIFEST.in\n+++ b/MANIFEST.in\n@@ -10,7 +10,6 @@ include scrapy/py.typed\n \n include codecov.yml\n include conftest.py\n-include pytest.ini\n include tox.ini\n \n recursive-include scrapy/templates *\ndiff --git a/pylintrc b/pylintrc\ndeleted file mode 100644\nindex e927b903c14..00000000000\n--- a/pylintrc\n+++ /dev/null\n@@ -1,73 +0,0 @@\n-[MASTER]\n-persistent=no\n-jobs=1  # >1 hides results\n-\n-[MESSAGES CONTROL]\n-disable=abstract-method,\n-        arguments-differ,\n-        arguments-renamed,\n-        attribute-defined-outside-init,\n-        bad-classmethod-argument,\n-        bare-except,\n-        broad-except,\n-        broad-exception-raised,\n-        c-extension-no-member,\n-        consider-using-with,\n-        cyclic-import,\n-        dangerous-default-value,\n-        disallowed-name,\n-        duplicate-code,  # https://github.com/PyCQA/pylint/issues/214\n-        eval-used,\n-        fixme,\n-        function-redefined,\n-        global-statement,\n-        implicit-str-concat,\n-        import-error,\n-        import-outside-toplevel,\n-        inherit-non-class,\n-        invalid-name,\n-        invalid-overridden-method,\n-        isinstance-second-argument-not-valid-type,\n-        keyword-arg-before-vararg,\n-        line-too-long,\n-        logging-format-interpolation,\n-        logging-fstring-interpolation,\n-        logging-not-lazy,\n-        lost-exception,\n-        missing-docstring,\n-        no-member,\n-        no-method-argument,\n-        no-name-in-module,\n-        no-self-argument,\n-        no-value-for-parameter,  # https://github.com/pylint-dev/pylint/issues/3268\n-        not-callable,\n-        pointless-statement,\n-        pointless-string-statement,\n-        protected-access,\n-        raise-missing-from,\n-        redefined-builtin,\n-        redefined-outer-name,\n-        reimported,\n-        signature-differs,\n-        too-few-public-methods,\n-        too-many-ancestors,\n-        too-many-arguments,\n-        too-many-branches,\n-        too-many-format-args,\n-        too-many-function-args,\n-        too-many-instance-attributes,\n-        too-many-lines,\n-        too-many-locals,\n-        too-many-public-methods,\n-        too-many-return-statements,\n-        unbalanced-tuple-unpacking,\n-        unnecessary-dunder-call,\n-        unnecessary-pass,\n-        unreachable,\n-        unused-argument,\n-        unused-import,\n-        unused-variable,\n-        used-before-assignment,\n-        useless-return,\n-        wildcard-import,\n-        wrong-import-position\ndiff --git a/pyproject.toml b/pyproject.toml\nnew file mode 100644\nindex 00000000000..f25715e76f9\n--- /dev/null\n+++ b/pyproject.toml\n@@ -0,0 +1,235 @@\n+[build-system]\n+requires = [\"setuptools >= 61.0\"]\n+build-backend = \"setuptools.build_meta\"\n+\n+[project]\n+name = \"Scrapy\"\n+dynamic = [\"version\"]\n+description = \"A high-level Web Crawling and Web Scraping framework\"\n+dependencies = [\n+    \"Twisted>=21.7.0\",\n+    \"cryptography>=37.0.0\",\n+    \"cssselect>=0.9.1\",\n+    \"itemloaders>=1.0.1\",\n+    \"parsel>=1.5.0\",\n+    \"pyOpenSSL>=22.0.0\",\n+    \"queuelib>=1.4.2\",\n+    \"service_identity>=18.1.0\",\n+    \"w3lib>=1.17.0\",\n+    \"zope.interface>=5.1.0\",\n+    \"protego>=0.1.15\",\n+    \"itemadapter>=0.1.0\",\n+    \"packaging\",\n+    \"tldextract\",\n+    \"lxml>=4.6.0\",\n+    \"defusedxml>=0.7.1\",\n+    # Platform-specific dependencies\n+    'PyDispatcher>=2.0.5; platform_python_implementation == \"CPython\"',\n+    'PyPyDispatcher>=2.1.0; platform_python_implementation == \"PyPy\"',\n+]\n+classifiers = [\n+    \"Framework :: Scrapy\",\n+    \"Development Status :: 5 - Production/Stable\",\n+    \"Environment :: Console\",\n+    \"Intended Audience :: Developers\",\n+    \"License :: OSI Approved :: BSD License\",\n+    \"Operating System :: OS Independent\",\n+    \"Programming Language :: Python\",\n+    \"Programming Language :: Python :: 3\",\n+    \"Programming Language :: Python :: 3.9\",\n+    \"Programming Language :: Python :: 3.10\",\n+    \"Programming Language :: Python :: 3.11\",\n+    \"Programming Language :: Python :: 3.12\",\n+    \"Programming Language :: Python :: 3.13\",\n+    \"Programming Language :: Python :: Implementation :: CPython\",\n+    \"Programming Language :: Python :: Implementation :: PyPy\",\n+    \"Topic :: Internet :: WWW/HTTP\",\n+    \"Topic :: Software Development :: Libraries :: Application Frameworks\",\n+    \"Topic :: Software Development :: Libraries :: Python Modules\",\n+]\n+readme = \"README.rst\"\n+requires-python = \">=3.9\"\n+authors = [{ name = \"Scrapy developers\", email = \"pablo@pablohoffman.com\" }]\n+maintainers = [{ name = \"Pablo Hoffman\", email = \"pablo@pablohoffman.com\" }]\n+\n+[project.urls]\n+Homepage = \"https://scrapy.org/\"\n+Documentation = \"https://docs.scrapy.org/\"\n+Source = \"https://github.com/scrapy/scrapy\"\n+Tracker = \"https://github.com/scrapy/scrapy/issues\"\n+Changelog = \"https://github.com/scrapy/scrapy/commits/master/\"\n+releasenotes = \"https://docs.scrapy.org/en/latest/news.html\"\n+\n+[project.scripts]\n+scrapy = \"scrapy.cmdline:execute\"\n+\n+[tool.setuptools.packages.find]\n+where = [\".\"]\n+include = [\"scrapy\", \"scrapy.*\",]\n+\n+[tool.setuptools.dynamic]\n+version = {file = \"./scrapy/VERSION\"}\n+\n+[tool.mypy]\n+ignore_missing_imports = true\n+\n+# Interface classes are hard to support\n+\n+[[tool.mypy.overrides]]\n+module = \"twisted.internet.interfaces\"\n+follow_imports = \"skip\"\n+\n+[[tool.mypy.overrides]]\n+module = \"scrapy.interfaces\"\n+ignore_errors = true\n+\n+[[tool.mypy.overrides]]\n+module = \"twisted.internet.reactor\"\n+follow_imports = \"skip\"\n+\n+# FIXME: remove the following section once the issues are solved\n+[[tool.mypy.overrides]]\n+module = \"scrapy.settings.default_settings\"\n+ignore_errors = true\n+\n+[tool.bandit]\n+skips = [\n+    \"B101\", # assert_used, needed for mypy\n+    \"B321\", # ftplib, https://github.com/scrapy/scrapy/issues/4180\n+    \"B402\", # import_ftplib, https://github.com/scrapy/scrapy/issues/4180\n+    \"B411\", # import_xmlrpclib, https://github.com/PyCQA/bandit/issues/1082\n+    \"B503\", # ssl_with_bad_defaults\n+]\n+exclude_dirs = [\"tests\"]\n+\n+[tool.bumpversion]\n+current_version = \"2.12.0\"\n+commit = true\n+tag = true\n+tag_name = \"{new_version}\"\n+\n+[[tool.bumpversion.files]]\n+filename = \"scrapy/VERSION\"\n+\n+[[tool.bumpversion.files]]\n+filename = \"SECURITY.md\"\n+parse = \"\"\"(?P<major>0|[1-9]\\\\d*)\\\\.(?P<minor>0|[1-9]\\\\d*)\"\"\"\n+serialize = [\"{major}.{minor}\"]\n+\n+[tool.coverage.run]\n+branch = true\n+include = [\"scrapy/*\"]\n+omit = [\"tests/*\"]\n+disable_warnings = [\"include-ignored\"]\n+\n+[tool.coverage.report]\n+# https://github.com/nedbat/coveragepy/issues/831#issuecomment-517778185\n+exclude_lines = [\"pragma: no cover\", \"if TYPE_CHECKING:\"]\n+\n+[tool.isort]\n+profile = \"black\"\n+\n+[tool.pylint.MASTER]\n+persistent = \"no\"\n+jobs = 1          # >1 hides results\n+\n+[tool.pylint.\"MESSAGES CONTROL\"]\n+disable = [\n+    \"abstract-method\",\n+    \"arguments-differ\",\n+    \"arguments-renamed\",\n+    \"attribute-defined-outside-init\",\n+    \"bad-classmethod-argument\",\n+    \"bare-except\",\n+    \"broad-except\",\n+    \"broad-exception-raised\",\n+    \"c-extension-no-member\",\n+    \"consider-using-with\",\n+    \"cyclic-import\",\n+    \"dangerous-default-value\",\n+    \"disallowed-name\",\n+    \"duplicate-code\",                            # https://github.com/PyCQA/pylint/issues/214\n+    \"eval-used\",\n+    \"fixme\",\n+    \"function-redefined\",\n+    \"global-statement\",\n+    \"implicit-str-concat\",\n+    \"import-error\",\n+    \"import-outside-toplevel\",\n+    \"inherit-non-class\",\n+    \"invalid-name\",\n+    \"invalid-overridden-method\",\n+    \"isinstance-second-argument-not-valid-type\",\n+    \"keyword-arg-before-vararg\",\n+    \"line-too-long\",\n+    \"logging-format-interpolation\",\n+    \"logging-fstring-interpolation\",\n+    \"logging-not-lazy\",\n+    \"lost-exception\",\n+    \"missing-docstring\",\n+    \"no-member\",\n+    \"no-method-argument\",\n+    \"no-name-in-module\",\n+    \"no-self-argument\",\n+    \"no-value-for-parameter\",                    # https://github.com/pylint-dev/pylint/issues/3268\n+    \"not-callable\",\n+    \"pointless-statement\",\n+    \"pointless-string-statement\",\n+    \"protected-access\",\n+    \"raise-missing-from\",\n+    \"redefined-builtin\",\n+    \"redefined-outer-name\",\n+    \"reimported\",\n+    \"signature-differs\",\n+    \"too-few-public-methods\",\n+    \"too-many-ancestors\",\n+    \"too-many-arguments\",\n+    \"too-many-branches\",\n+    \"too-many-format-args\",\n+    \"too-many-function-args\",\n+    \"too-many-instance-attributes\",\n+    \"too-many-lines\",\n+    \"too-many-locals\",\n+    \"too-many-public-methods\",\n+    \"too-many-return-statements\",\n+    \"unbalanced-tuple-unpacking\",\n+    \"unnecessary-dunder-call\",\n+    \"unnecessary-pass\",\n+    \"unreachable\",\n+    \"unused-argument\",\n+    \"unused-import\",\n+    \"unused-variable\",\n+    \"used-before-assignment\",\n+    \"useless-return\",\n+    \"wildcard-import\",\n+    \"wrong-import-position\",\n+]\n+\n+[tool.pytest.ini_options]\n+xfail_strict = true\n+usefixtures = \"chdir\"\n+python_files = [\"test_*.py\", \"__init__.py\"]\n+python_classes = []\n+addopts = [\n+    \"--assert=plain\",\n+    \"--ignore=docs/_ext\",\n+    \"--ignore=docs/conf.py\",\n+    \"--ignore=docs/news.rst\",\n+    \"--ignore=docs/topics/dynamic-content.rst\",\n+    \"--ignore=docs/topics/items.rst\",\n+    \"--ignore=docs/topics/leaks.rst\",\n+    \"--ignore=docs/topics/loaders.rst\",\n+    \"--ignore=docs/topics/selectors.rst\",\n+    \"--ignore=docs/topics/shell.rst\",\n+    \"--ignore=docs/topics/stats.rst\",\n+    \"--ignore=docs/topics/telnetconsole.rst\",\n+    \"--ignore=docs/utils\",\n+]\n+markers = [\n+    \"only_asyncio: marks tests as only enabled when --reactor=asyncio is passed\",\n+    \"only_not_asyncio: marks tests as only enabled when --reactor=asyncio is not passed\",\n+    \"requires_uvloop: marks tests as only enabled when uvloop is known to be working\",\n+    \"requires_botocore: marks tests that need botocore (but not boto3)\",\n+    \"requires_boto3: marks tests that need botocore and boto3\",\n+]\n+filterwarnings = []\n\\ No newline at end of file\ndiff --git a/setup.cfg b/setup.cfg\ndeleted file mode 100644\nindex 151e784c661..00000000000\n--- a/setup.cfg\n+++ /dev/null\n@@ -1,24 +0,0 @@\n-[bdist_rpm]\n-doc_files = docs AUTHORS INSTALL LICENSE README.rst\n-\n-[bdist_wheel]\n-universal=1\n-\n-[mypy]\n-ignore_missing_imports = true\n-\n-# Interface classes are hard to support\n-\n-[mypy-twisted.internet.interfaces]\n-follow_imports = skip\n-\n-[mypy-scrapy.interfaces]\n-ignore_errors = True\n-\n-[mypy-twisted.internet.reactor]\n-follow_imports = skip\n-\n-# FIXME: remove the following sections once the issues are solved\n-\n-[mypy-scrapy.settings.default_settings]\n-ignore_errors = True\ndiff --git a/setup.py b/setup.py\ndeleted file mode 100644\nindex 6cc1150a568..00000000000\n--- a/setup.py\n+++ /dev/null\n@@ -1,75 +0,0 @@\n-from pathlib import Path\n-\n-from setuptools import find_packages, setup\n-\n-version = (Path(__file__).parent / \"scrapy/VERSION\").read_text(\"ascii\").strip()\n-\n-\n-install_requires = [\n-    \"Twisted>=21.7.0\",\n-    \"cryptography>=37.0.0\",\n-    \"cssselect>=0.9.1\",\n-    \"itemloaders>=1.0.1\",\n-    \"parsel>=1.5.0\",\n-    \"pyOpenSSL>=22.0.0\",\n-    \"queuelib>=1.4.2\",\n-    \"service_identity>=18.1.0\",\n-    \"w3lib>=1.17.0\",\n-    \"zope.interface>=5.1.0\",\n-    \"protego>=0.1.15\",\n-    \"itemadapter>=0.1.0\",\n-    \"packaging\",\n-    \"tldextract\",\n-    \"lxml>=4.6.0\",\n-    \"defusedxml>=0.7.1\",\n-]\n-extras_require = {\n-    ':platform_python_implementation == \"CPython\"': [\"PyDispatcher>=2.0.5\"],\n-    ':platform_python_implementation == \"PyPy\"': [\"PyPyDispatcher>=2.1.0\"],\n-}\n-\n-\n-setup(\n-    name=\"Scrapy\",\n-    version=version,\n-    url=\"https://scrapy.org\",\n-    project_urls={\n-        \"Documentation\": \"https://docs.scrapy.org/\",\n-        \"Source\": \"https://github.com/scrapy/scrapy\",\n-        \"Tracker\": \"https://github.com/scrapy/scrapy/issues\",\n-    },\n-    description=\"A high-level Web Crawling and Web Scraping framework\",\n-    long_description=open(\"README.rst\", encoding=\"utf-8\").read(),\n-    author=\"Scrapy developers\",\n-    author_email=\"pablo@pablohoffman.com\",\n-    maintainer=\"Pablo Hoffman\",\n-    maintainer_email=\"pablo@pablohoffman.com\",\n-    license=\"BSD\",\n-    packages=find_packages(exclude=(\"tests\", \"tests.*\")),\n-    include_package_data=True,\n-    zip_safe=False,\n-    entry_points={\"console_scripts\": [\"scrapy = scrapy.cmdline:execute\"]},\n-    classifiers=[\n-        \"Framework :: Scrapy\",\n-        \"Development Status :: 5 - Production/Stable\",\n-        \"Environment :: Console\",\n-        \"Intended Audience :: Developers\",\n-        \"License :: OSI Approved :: BSD License\",\n-        \"Operating System :: OS Independent\",\n-        \"Programming Language :: Python\",\n-        \"Programming Language :: Python :: 3\",\n-        \"Programming Language :: Python :: 3.9\",\n-        \"Programming Language :: Python :: 3.10\",\n-        \"Programming Language :: Python :: 3.11\",\n-        \"Programming Language :: Python :: 3.12\",\n-        \"Programming Language :: Python :: 3.13\",\n-        \"Programming Language :: Python :: Implementation :: CPython\",\n-        \"Programming Language :: Python :: Implementation :: PyPy\",\n-        \"Topic :: Internet :: WWW/HTTP\",\n-        \"Topic :: Software Development :: Libraries :: Application Frameworks\",\n-        \"Topic :: Software Development :: Libraries :: Python Modules\",\n-    ],\n-    python_requires=\">=3.9\",\n-    install_requires=install_requires,\n-    extras_require=extras_require,\n-)\ndiff --git a/tox.ini b/tox.ini\nindex 5783a0e6172..4e1a99473f5 100644\n--- a/tox.ini\n+++ b/tox.ini\n@@ -79,7 +79,7 @@ deps =\n     {[testenv:extra-deps]deps}\n     pylint==3.2.5\n commands =\n-    pylint conftest.py docs extras scrapy setup.py tests\n+    pylint conftest.py docs extras scrapy tests\n \n [testenv:twinecheck]\n basepython = python3\n",
    "test_patch": "diff --git a/pytest.ini b/pytest.ini\ndeleted file mode 100644\nindex 824c0e9e91b..00000000000\n--- a/pytest.ini\n+++ /dev/null\n@@ -1,26 +0,0 @@\n-[pytest]\n-xfail_strict = true\n-usefixtures = chdir\n-python_files=test_*.py __init__.py\n-python_classes=\n-addopts =\n-    --assert=plain\n-    --ignore=docs/_ext\n-    --ignore=docs/conf.py\n-    --ignore=docs/news.rst\n-    --ignore=docs/topics/dynamic-content.rst\n-    --ignore=docs/topics/items.rst\n-    --ignore=docs/topics/leaks.rst\n-    --ignore=docs/topics/loaders.rst\n-    --ignore=docs/topics/selectors.rst\n-    --ignore=docs/topics/shell.rst\n-    --ignore=docs/topics/stats.rst\n-    --ignore=docs/topics/telnetconsole.rst\n-    --ignore=docs/utils\n-markers =\n-    only_asyncio: marks tests as only enabled when --reactor=asyncio is passed\n-    only_not_asyncio: marks tests as only enabled when --reactor=asyncio is not passed\n-    requires_uvloop: marks tests as only enabled when uvloop is known to be working\n-    requires_botocore: marks tests that need botocore (but not boto3)\n-    requires_boto3: marks tests that need botocore and boto3\n-filterwarnings =\ndiff --git a/tests/test_crawler.py b/tests/test_crawler.py\nindex 853acf2ded3..a77531f6216 100644\n--- a/tests/test_crawler.py\n+++ b/tests/test_crawler.py\n@@ -899,7 +899,7 @@ def test_shutdown_forced(self):\n         p.expect_exact(\"shutting down gracefully\")\n         # sending the second signal too fast often causes problems\n         d = Deferred()\n-        reactor.callLater(0.1, d.callback, None)\n+        reactor.callLater(0.01, d.callback, None)\n         yield d\n         p.kill(sig)\n         p.expect_exact(\"forcing unclean shutdown\")\ndiff --git a/tests/test_spiderloader/__init__.py b/tests/test_spiderloader/__init__.py\nindex d2ff9ba488f..9b53b9b9631 100644\n--- a/tests/test_spiderloader/__init__.py\n+++ b/tests/test_spiderloader/__init__.py\n@@ -144,9 +144,10 @@ def test_syntax_error_exception(self):\n             self.assertRaises(SyntaxError, SpiderLoader.from_settings, settings)\n \n     def test_syntax_error_warning(self):\n-        with warnings.catch_warnings(record=True) as w, mock.patch.object(\n-            SpiderLoader, \"_load_spiders\"\n-        ) as m:\n+        with (\n+            warnings.catch_warnings(record=True) as w,\n+            mock.patch.object(SpiderLoader, \"_load_spiders\") as m,\n+        ):\n             m.side_effect = SyntaxError\n             module = \"tests.test_spiderloader.test_spiders.spider1\"\n             settings = Settings(\n",
    "problem_statement": "Migrate from setup.py to pyproject.toml\nWe should migrate to the modern declarative setuptools metadata approach as discussed in https://setuptools.pypa.io/en/latest/userguide/quickstart.html and https://setuptools.pypa.io/en/latest/userguide/pyproject_config.html, but only after the 2.12 release.\nMigrate to pyproject.toml\nCloses #6514\n",
    "hints_text": "\n",
    "created_at": "2024-11-16T20:48:39Z",
    "version": "2.12",
    "PASS_TO_PASS": [],
    "FAIL_TO_PASS": [
      "tests/test_crawler.py",
      "tests/test_spiderloader/__init__.py"
    ]
  },
  {
    "repo": "scrapy/scrapy",
    "pull_number": 6542,
    "instance_id": "scrapy__scrapy-6542",
    "issue_numbers": [
      "6505"
    ],
    "base_commit": "ab5cb7c7d9e268b501009d991d97ca19b6f7fe96",
    "patch": "diff --git a/scrapy/contracts/__init__.py b/scrapy/contracts/__init__.py\nindex 9071395e3d9..3b4f932a014 100644\n--- a/scrapy/contracts/__init__.py\n+++ b/scrapy/contracts/__init__.py\n@@ -38,9 +38,7 @@ def add_pre_hook(self, request: Request, results: TestResult) -> Request:\n             assert cb is not None\n \n             @wraps(cb)\n-            def wrapper(  # pylint: disable=inconsistent-return-statements\n-                response: Response, **cb_kwargs: Any\n-            ) -> list[Any]:\n+            def wrapper(response: Response, **cb_kwargs: Any) -> list[Any]:\n                 try:\n                     results.startTest(self.testcase_pre)\n                     self.pre_process(response)\n@@ -51,13 +49,10 @@ def wrapper(  # pylint: disable=inconsistent-return-statements\n                     results.addError(self.testcase_pre, sys.exc_info())\n                 else:\n                     results.addSuccess(self.testcase_pre)\n-                finally:\n-                    cb_result = cb(response, **cb_kwargs)\n-                    if isinstance(cb_result, (AsyncGenerator, CoroutineType)):\n-                        raise TypeError(\"Contracts don't support async callbacks\")\n-                    return list(  # pylint: disable=return-in-finally\n-                        cast(Iterable[Any], iterate_spider_output(cb_result))\n-                    )\n+                cb_result = cb(response, **cb_kwargs)\n+                if isinstance(cb_result, (AsyncGenerator, CoroutineType)):\n+                    raise TypeError(\"Contracts don't support async callbacks\")\n+                return list(cast(Iterable[Any], iterate_spider_output(cb_result)))\n \n             request.callback = wrapper\n \n@@ -69,9 +64,7 @@ def add_post_hook(self, request: Request, results: TestResult) -> Request:\n             assert cb is not None\n \n             @wraps(cb)\n-            def wrapper(  # pylint: disable=inconsistent-return-statements\n-                response: Response, **cb_kwargs: Any\n-            ) -> list[Any]:\n+            def wrapper(response: Response, **cb_kwargs: Any) -> list[Any]:\n                 cb_result = cb(response, **cb_kwargs)\n                 if isinstance(cb_result, (AsyncGenerator, CoroutineType)):\n                     raise TypeError(\"Contracts don't support async callbacks\")\n@@ -86,8 +79,7 @@ def wrapper(  # pylint: disable=inconsistent-return-statements\n                     results.addError(self.testcase_post, sys.exc_info())\n                 else:\n                     results.addSuccess(self.testcase_post)\n-                finally:\n-                    return output  # pylint: disable=return-in-finally\n+                return output\n \n             request.callback = wrapper\n \n",
    "test_patch": "diff --git a/tests/test_contracts.py b/tests/test_contracts.py\nindex d578b3af450..b0cb92d12d9 100644\n--- a/tests/test_contracts.py\n+++ b/tests/test_contracts.py\n@@ -556,3 +556,61 @@ def test_inherited_contracts(self):\n \n         requests = self.conman.from_spider(spider, self.results)\n         self.assertTrue(requests)\n+\n+\n+class CustomFailContractPreProcess(Contract):\n+    name = \"test_contract\"\n+\n+    def pre_process(self, response):\n+        raise KeyboardInterrupt(\"Pre-process exception\")\n+\n+\n+class CustomFailContractPostProcess(Contract):\n+    name = \"test_contract\"\n+\n+    def post_process(self, response):\n+        raise KeyboardInterrupt(\"Post-process exception\")\n+\n+\n+class CustomContractPrePostProcess(unittest.TestCase):\n+\n+    def setUp(self):\n+        self.results = TextTestResult(stream=None, descriptions=False, verbosity=0)\n+\n+    def test_pre_hook_keyboard_interrupt(self):\n+        spider = TestSpider()\n+        response = ResponseMock()\n+        contract = CustomFailContractPreProcess(spider.returns_request)\n+        conman = ContractsManager([contract])\n+\n+        try:\n+            request = conman.from_method(spider.returns_request, self.results)\n+            contract.add_pre_hook(request, self.results)\n+            # Expect this to raise a KeyboardInterrupt\n+            request.callback(response, **request.cb_kwargs)\n+        except KeyboardInterrupt as e:\n+            self.assertEqual(str(e), \"Pre-process exception\")\n+        else:\n+            self.fail(\"KeyboardInterrupt not raised\")\n+\n+        self.assertFalse(self.results.failures)\n+        self.assertFalse(self.results.errors)\n+\n+    def test_post_hook_keyboard_interrupt(self):\n+        spider = TestSpider()\n+        response = ResponseMock()\n+        contract = CustomFailContractPostProcess(spider.returns_request)\n+        conman = ContractsManager([contract])\n+\n+        try:\n+            request = conman.from_method(spider.returns_request, self.results)\n+            contract.add_post_hook(request, self.results)\n+            # Expect this to raise a KeyboardInterrupt\n+            request.callback(response, **request.cb_kwargs)\n+        except KeyboardInterrupt as e:\n+            self.assertEqual(str(e), \"Post-process exception\")\n+        else:\n+            self.fail(\"KeyboardInterrupt not raised\")\n+\n+        self.assertFalse(self.results.failures)\n+        self.assertFalse(self.results.errors)\n",
    "problem_statement": "return in finally can swallow exception\n### Description\r\n\r\nThere are two places in `scrapy/contracts/__init__.py` where a `finally:` body contains a `return` statement, which would swallow any in-flight exception. \r\n\r\nThis means that if a `BaseException` (such as `KeyboardInterrupt`) is raised from the body, or any exception is raised from one of the `except:` clause, it will not propagate on as expected. \r\n\r\nThe pylint warning about this was suppressed in [this commit](https://github.com/scrapy/scrapy/commit/991121fa91aee4d428ae09e75427d4e91970a41b) but it doesn't seem like there was justification for that.\r\n\r\nThese are the two locations:\r\nhttps://github.com/scrapy/scrapy/blame/b4bad97eae6bcce790a626d244c63589f4134408/scrapy/contracts/__init__.py#L56\r\n\r\nhttps://github.com/scrapy/scrapy/blame/b4bad97eae6bcce790a626d244c63589f4134408/scrapy/contracts/__init__.py#L86\r\n\r\nSee also https://docs.python.org/3/tutorial/errors.html#defining-clean-up-actions.\n",
    "hints_text": "> If the finally clause executes a [break](https://docs.python.org/3/reference/simple_stmts.html#break), [continue](https://docs.python.org/3/reference/simple_stmts.html#continue) or [return](https://docs.python.org/3/reference/simple_stmts.html#return) statement, exceptions are not re-raised.\r\n\r\nTIL\nHey,  \r\nWhat about re-raising the issue with a general raise statement in every except block along with putting the return statement outside the finally block?  \r\nIf this solution seems promising, I'd like to contribute to the same. I would appreciate your insights.\nI don't remember why I silenced them :-/\nIs this issue still open?\n@divyranjan17 it is.\r\n\r\n@AdityaS8804 I don't think that makes sense to me.\nFrom a quick look, it seems to me that simply taking the return part out of finally, and put it after the try-except blocks, could be all that\u2019s needed. But we should also first write tests that detect the issues before we address those issues.\n> From a quick look, it seems to me that simply taking the return part out of finally, and put it after the try-except blocks, could be all that\u2019s needed.\r\n\r\nThis matches my first impression.\nIs there a way to reproduce a this failure?\n> Is there a way to reproduce a this failure?\r\n\r\nFor the first issue, for example, it seems like raising `KeyboardInterrupt` from an implementation of https://docs.scrapy.org/en/2.11/topics/contracts.html#scrapy.contracts.Contract.pre_process should see that exception raise, but will instead silence it.\n\r\nI can think of 3 ways to tackle this issue\r\n\r\n#### Solution 1: Using a Temporary Variable for Return Value\r\nWe can capture the callback result in a variable outside the `finally` block and then return it at the end of the function. By avoiding `return` inside `finally`, exceptions propagate naturally, allowing errors to be handled as expected.\r\n\r\n**Simply:**\r\n\r\n- Store the callback output in a variable (e.g., `cb_result`).\r\n- Avoid using `return` in the `finally` block.\r\n- Return `cb_result` at the end of the function, outside of any `try/finally` structure.\r\n\r\n```python\r\ncb_result = None\r\ntry:\r\n    cb_result = cb(response, **cb_kwargs)\r\nfinally:\r\n    pass  # Any final cleanup here\r\nreturn list(iterate_spider_output(cb_result))\r\n```\r\n\r\n#### Solution 2: Separating Error Logging and Result Processing\r\n- Create a helper function, `log_results()`, to handle logging outcomes.\r\n- Call `log_results()` within `try/except` blocks to process success or errors.\r\n- Return `cb_result` outside of the `try` block without `finally`.\r\n\r\n```python\r\ndef log_results(testcase, exception_info=None):\r\n    if exception_info:\r\n        # Log failure\r\n```\r\n\r\n#### Solution 3: Wrapping Return Values with Exception Handling\r\n- Define `process_result` to manage callback outputs while capturing exceptions.\r\n- Invoke `process_result` instead of a direct return in the callback.\r\n- Ensure all exception info is handled without using a return in `finally`.\r\n\r\n```python\r\ndef process_result(cb, response, **cb_kwargs):\r\n    try:\r\n        cb_result = cb(response, **cb_kwargs)\r\n    except Exception as exc:\r\n        log_error(exc)\r\n    return list(iterate_spider_output(cb_result))\r\n```\r\n\nIs this AI-generated?\nyes Sol.1 and Sol.3 were suggested by github copilot \nThat's unfortunate, especially as the way forward was already suggested earlier.\n[Here](https://github.com/scrapy/scrapy/issues/6505#issuecomment-2434584223), specifically.\n> [Here](https://github.com/scrapy/scrapy/issues/6505#issuecomment-2434584223), specifically.\r\n\r\nUnderstood",
    "created_at": "2024-11-14T03:19:30Z",
    "version": "2.11",
    "PASS_TO_PASS": [],
    "FAIL_TO_PASS": [
      "tests/test_contracts.py"
    ]
  },
  {
    "repo": "scrapy/scrapy",
    "pull_number": 6475,
    "instance_id": "scrapy__scrapy-6475",
    "issue_numbers": [
      "6433"
    ],
    "base_commit": "67ab8d4650c1e9212c9508803c7b5265e166cbaa",
    "patch": "diff --git a/scrapy/core/engine.py b/scrapy/core/engine.py\nindex 63d84339dcd..fd9a5f7817e 100644\n--- a/scrapy/core/engine.py\n+++ b/scrapy/core/engine.py\n@@ -39,7 +39,6 @@\n from scrapy.signalmanager import SignalManager\n from scrapy.utils.log import failure_to_exc_info, logformatter_adapter\n from scrapy.utils.misc import build_from_crawler, load_object\n-from scrapy.utils.python import global_object_name\n from scrapy.utils.reactor import CallLaterOnce\n \n if TYPE_CHECKING:\n@@ -325,10 +324,6 @@ def _schedule_request(self, request: Request, spider: Spider) -> None:\n         )\n         for handler, result in request_scheduled_result:\n             if isinstance(result, Failure) and isinstance(result.value, IgnoreRequest):\n-                logger.debug(\n-                    f\"Signal handler {global_object_name(handler)} dropped \"\n-                    f\"request {request} before it reached the scheduler.\"\n-                )\n                 return\n         if not self.slot.scheduler.enqueue_request(request):  # type: ignore[union-attr]\n             self.signals.send_catch_log(\n",
    "test_patch": "diff --git a/tests/test_engine.py b/tests/test_engine.py\nindex 86526420f83..2ebc0b5e449 100644\n--- a/tests/test_engine.py\n+++ b/tests/test_engine.py\n@@ -499,7 +499,6 @@ def signal_handler(request: Request, spider: Spider) -> None:\n     assert scheduler.enqueued == [\n         keep_request\n     ], f\"{scheduler.enqueued!r} != [{keep_request!r}]\"\n-    assert \"dropped request <GET https://drop.example>\" in caplog.text\n     crawler.signals.disconnect(signal_handler, request_scheduled)\n \n \n",
    "problem_statement": "core.engine/Signal handler polluting log\n### Description\r\n\r\nThe `OffsiteMiddleware` logs a single message for each domain filtered. Great!\r\nBut then the `core.engine` logs a message for every single url filtered by the OffsiteMiddleware.\r\n(LOG_LEVEL: DEBUG)\r\n\r\nThe websites I am scraping have like 10 external links to twitter/youtube/etc in each page. For hundreds pages scrapped, the only thing I can see in the logs is `Signal handler scrapy.downloadermiddlewares.offsite.OffsiteMiddleware.request_scheduled dropped request`. \r\n\r\nI don't know if this is intended behavior. If so, it is obviously not a bug.\r\nBut nonetheless, it is very different behavior compared to previous 1.x Scrapy versions. (I don't know when it has changed and I couldn't find anything in the release notes about that.)\r\n\r\nIf not a bug, maybe we could discuss the possibility of changing this behavior so we can have logs less polluted when debugging.\r\n\r\n### Steps to Reproduce\r\n\r\n#### Just run the following spider.\r\n(url taken from another issue).\r\n\r\n```python\r\nimport scrapy\r\n\r\nclass TestSpider(scrapy.spiders.CrawlSpider):\r\n    name = 'test'\r\n    allowed_domains = ['capybala.com']\r\n    start_urls = ['https://capybala.com/']\r\n    custom_settings = {\r\n        'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',\r\n        'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',\r\n        'LOG_LEVEL': 'DEBUG'\r\n    }\r\n\r\n    rules = (scrapy.spiders.Rule(scrapy.linkextractors.LinkExtractor(), callback='parse', follow=True),)\r\n    \r\n    def parse(self, response):\r\n        print('noop')\r\n```\r\n\r\n#### Output: \r\n```txt\r\n2024-07-08 16:34:43 [scrapy.utils.log] INFO: Scrapy 2.11.2 started (bot: scrapybot)\r\n2024-07-08 16:34:43 [scrapy.utils.log] INFO: Versions: lxml 5.2.2.0, libxml2 2.12.6, cssselect 1.2.0, parsel 1.9.1, w3lib 2.2.1, Twisted 24.3.0, Python 3.12.4 (main, Jul  3 2024, 16:55:58) [GCC 11.2.0], pyOpenSSL 24.1.0 (OpenSSL 3.2.2 4 Jun 2024), cryptography 42.0.8, Platform Linux-5.15.145-x86_64-AMD_Ryzen_9_5980HX_with_Radeon_Graphics-with-glibc2.33\r\n2024-07-08 16:34:43 [scrapy.addons] INFO: Enabled addons:\r\n[]\r\n2024-07-08 16:34:43 [asyncio] DEBUG: Using selector: EpollSelector\r\n2024-07-08 16:34:43 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\r\n2024-07-08 16:34:43 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop\r\n2024-07-08 16:34:43 [scrapy.extensions.telnet] INFO: Telnet Password: d2c4cce2938fba32\r\n2024-07-08 16:34:43 [scrapy.middleware] INFO: Enabled extensions:\r\n['scrapy.extensions.corestats.CoreStats',\r\n 'scrapy.extensions.telnet.TelnetConsole',\r\n 'scrapy.extensions.memusage.MemoryUsage',\r\n 'scrapy.extensions.logstats.LogStats']\r\n2024-07-08 16:34:43 [scrapy.crawler] INFO: Overridden settings:\r\n{'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',\r\n 'SPIDER_LOADER_WARN_ONLY': True,\r\n 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}\r\n2024-07-08 16:34:43 [scrapy.middleware] INFO: Enabled downloader middlewares:\r\n['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',\r\n 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\r\n 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\r\n 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\r\n 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\r\n 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\r\n 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\r\n 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\r\n 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\r\n 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\r\n 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\r\n 'scrapy.downloadermiddlewares.stats.DownloaderStats']\r\n2024-07-08 16:34:43 [scrapy.middleware] INFO: Enabled spider middlewares:\r\n['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\r\n 'scrapy.spidermiddlewares.referer.RefererMiddleware',\r\n 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\r\n 'scrapy.spidermiddlewares.depth.DepthMiddleware']\r\n2024-07-08 16:34:43 [scrapy.middleware] INFO: Enabled item pipelines:\r\n[]\r\n2024-07-08 16:34:43 [scrapy.core.engine] INFO: Spider opened\r\n2024-07-08 16:34:43 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\r\n2024-07-08 16:34:43 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\r\n2024-07-08 16:34:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://capybala.com/> (referer: None)\r\n2024-07-08 16:34:44 [scrapy.downloadermiddlewares.offsite] DEBUG: Filtered offsite request to 'bokuran.com': <GET https://bokuran.com/>\r\n2024-07-08 16:34:44 [scrapy.core.engine] DEBUG: Signal handler scrapy.downloadermiddlewares.offsite.OffsiteMiddleware.request_scheduled dropped request <GET https://bokuran.com/> before it reached the scheduler.\r\n2024-07-08 16:34:44 [scrapy.downloadermiddlewares.offsite] DEBUG: Filtered offsite request to 'webooker.info': <GET http://webooker.info/2013/10/ebook1-release/>\r\n2024-07-08 16:34:44 [scrapy.core.engine] DEBUG: Signal handler scrapy.downloadermiddlewares.offsite.OffsiteMiddleware.request_scheduled dropped request <GET http://webooker.info/2013/10/ebook1-release/> before it reached the scheduler.\r\n2024-07-08 16:34:44 [scrapy.downloadermiddlewares.offsite] DEBUG: Filtered offsite request to 'ebook-1.com': <GET https://ebook-1.com/>\r\n2024-07-08 16:34:44 [scrapy.core.engine] DEBUG: Signal handler scrapy.downloadermiddlewares.offsite.OffsiteMiddleware.request_scheduled dropped request <GET https://ebook-1.com/> before it reached the scheduler.\r\n2024-07-08 16:34:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://capybala.com/> (referer: https://capybala.com/)\r\n2024-07-08 16:34:44 [scrapy.downloadermiddlewares.offsite] DEBUG: Filtered offsite request to 'chrome.google.com': <GET https://chrome.google.com/webstore/detail/find-ebook-edition/jhhpocdmfelpmobcnmjfppdpnbepkono>\r\n2024-07-08 16:34:44 [scrapy.core.engine] DEBUG: Signal handler scrapy.downloadermiddlewares.offsite.OffsiteMiddleware.request_scheduled dropped request <GET https://chrome.google.com/webstore/detail/find-ebook-edition/jhhpocdmfelpmobcnmjfppdpnbepkono> before it reached the scheduler.\r\n2024-07-08 16:34:44 [scrapy.downloadermiddlewares.offsite] DEBUG: Filtered offsite request to 'twitter.com': <GET https://twitter.com/orangain>\r\n2024-07-08 16:34:44 [scrapy.core.engine] DEBUG: Signal handler scrapy.downloadermiddlewares.offsite.OffsiteMiddleware.request_scheduled dropped request <GET https://twitter.com/orangain> before it reached the scheduler.\r\n2024-07-08 16:34:44 [scrapy.core.engine] DEBUG: Signal handler scrapy.downloadermiddlewares.offsite.OffsiteMiddleware.request_scheduled dropped request <GET https://twitter.com/webooker_log> before it reached the scheduler.\r\n2024-07-08 16:34:44 [scrapy.core.engine] DEBUG: Signal handler scrapy.downloadermiddlewares.offsite.OffsiteMiddleware.request_scheduled dropped request <GET http://webooker.info/> before it reached the scheduler.\r\n2024-07-08 16:34:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://capybala.com/find-kindle-edition/> (referer: https://capybala.com/)\r\n2024-07-08 16:34:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://capybala.com/bokuran/> (referer: https://capybala.com/)\r\n2024-07-08 16:34:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://capybala.com/ebook-1/> (referer: https://capybala.com/)\r\n2024-07-08 16:34:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://capybala.com/dendrogram/> (referer: https://capybala.com/)\r\nnoop\r\n2024-07-08 16:34:44 [scrapy.dupefilters] DEBUG: Filtered duplicate request: <GET https://capybala.com/> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)\r\n2024-07-08 16:34:44 [scrapy.core.engine] DEBUG: Signal handler scrapy.downloadermiddlewares.offsite.OffsiteMiddleware.request_scheduled dropped request <GET https://bokuran.com/> before it reached the scheduler.\r\n2024-07-08 16:34:44 [scrapy.core.engine] DEBUG: Signal handler scrapy.downloadermiddlewares.offsite.OffsiteMiddleware.request_scheduled dropped request <GET http://webooker.info/2013/10/ebook1-release/> before it reached the scheduler.\r\n2024-07-08 16:34:44 [scrapy.core.engine] DEBUG: Signal handler scrapy.downloadermiddlewares.offsite.OffsiteMiddleware.request_scheduled dropped request <GET https://ebook-1.com/> before it reached the scheduler.\r\n2024-07-08 16:34:44 [scrapy.core.engine] DEBUG: Signal handler scrapy.downloadermiddlewares.offsite.OffsiteMiddleware.request_scheduled dropped request <GET https://chrome.google.com/webstore/detail/find-ebook-edition/jhhpocdmfelpmobcnmjfppdpnbepkono> before it reached the scheduler.\r\n2024-07-08 16:34:44 [scrapy.core.engine] DEBUG: Signal handler scrapy.downloadermiddlewares.offsite.OffsiteMiddleware.request_scheduled dropped request <GET https://twitter.com/orangain> before it reached the scheduler.\r\n2024-07-08 16:34:44 [scrapy.core.engine] DEBUG: Signal handler scrapy.downloadermiddlewares.offsite.OffsiteMiddleware.request_scheduled dropped request <GET https://twitter.com/webooker_log> before it reached the scheduler.\r\n2024-07-08 16:34:44 [scrapy.core.engine] DEBUG: Signal handler scrapy.downloadermiddlewares.offsite.OffsiteMiddleware.request_scheduled dropped request <GET http://webooker.info/> before it reached the scheduler.\r\nnoop\r\n2024-07-08 16:34:44 [scrapy.core.engine] DEBUG: Signal handler scrapy.downloadermiddlewares.offsite.OffsiteMiddleware.request_scheduled dropped request <GET https://chrome.google.com/webstore/detail/find-ebook-edition/jhhpocdmfelpmobcnmjfppdpnbepkono> before it reached the scheduler.\r\nnoop\r\n2024-07-08 16:34:44 [scrapy.core.engine] DEBUG: Signal handler scrapy.downloadermiddlewares.offsite.OffsiteMiddleware.request_scheduled dropped request <GET https://bokuran.com/> before it reached the scheduler.\r\nnoop\r\nnoop\r\n2024-07-08 16:34:44 [scrapy.core.engine] DEBUG: Signal handler scrapy.downloadermiddlewares.offsite.OffsiteMiddleware.request_scheduled dropped request <GET http://webooker.info/2013/10/ebook1-release/> before it reached the scheduler.\r\n2024-07-08 16:34:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://tree.capybala.com/> (referer: https://capybala.com/)\r\nnoop\r\n2024-07-08 16:34:45 [scrapy.core.engine] INFO: Closing spider (finished)\r\n2024-07-08 16:34:45 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\r\n{'downloader/request_bytes': 1735,\r\n 'downloader/request_count': 7,\r\n 'downloader/request_method_count/GET': 7,\r\n 'downloader/response_bytes': 17486,\r\n 'downloader/response_count': 7,\r\n 'downloader/response_status_count/200': 7,\r\n 'dupefilter/filtered': 16,\r\n 'elapsed_time_seconds': 1.950522,\r\n 'finish_reason': 'finished',\r\n 'finish_time': datetime.datetime(2024, 7, 8, 19, 34, 45, 376469, tzinfo=datetime.timezone.utc),\r\n 'httpcompression/response_bytes': 29892,\r\n 'httpcompression/response_count': 7,\r\n 'log_count/DEBUG': 33,\r\n 'log_count/INFO': 10,\r\n 'memusage/max': 70103040,\r\n 'memusage/startup': 70103040,\r\n 'offsite/domains': 5,\r\n 'offsite/filtered': 17,\r\n 'request_depth_max': 2,\r\n 'response_received_count': 7,\r\n 'scheduler/dequeued': 7,\r\n 'scheduler/dequeued/memory': 7,\r\n 'scheduler/enqueued': 7,\r\n 'scheduler/enqueued/memory': 7,\r\n 'start_time': datetime.datetime(2024, 7, 8, 19, 34, 43, 425947, tzinfo=datetime.timezone.utc)}\r\n2024-07-08 16:34:45 [scrapy.core.engine] INFO: Spider closed (finished)\r\n\r\n```\r\n\r\n**Expected behavior:**\r\n\r\nI was not expecting to see so many `[scrapy.core.engine] DEBUG: Signal handler scrapy.downloadermiddlewares.offsite.OffsiteMiddleware.request_scheduled dropped request <GET [...]> before it reached the scheduler.` messages. I believe just the messages given by the OffsiteMiddleware are enough.\r\n\r\n**Actual behavior:**\r\n\r\nThere are **a lot** of  \"dropped request\" messages.\r\nFurthermore the same message is replicated several times if the same url is found more than one time. (e.g. https://twitter.com/orangain or https://twitter.com/webooker_log in the previous log)\r\n\r\n**Reproduces how often:** always\r\n\r\n### Versions\r\n\r\n$  scrapy version --verbose\r\nScrapy       : 2.11.2\r\nlxml         : 5.2.2.0\r\nlibxml2      : 2.12.6\r\ncssselect    : 1.2.0\r\nparsel       : 1.9.1\r\nw3lib        : 2.2.1\r\nTwisted      : 24.3.0\r\nPython       : 3.12.4 (main, Jul  3 2024, 16:55:58) [GCC 11.2.0]\r\npyOpenSSL    : 24.1.0 (OpenSSL 3.2.2 4 Jun 2024)\r\ncryptography : 42.0.8\r\nPlatform     : Linux-5.15.145-x86_64-AMD_Ryzen_9_5980HX_with_Radeon_Graphics-with-glibc2.33\r\n\r\n### Additional context\r\n\r\nI believe this has nothing to do with the `CrawlSpider`, but that is what I am using.\n",
    "hints_text": "In fact Scrapy version 2.11.1 has the previous and expected behavior.  The current behavior was introduced in Scrapy 2.11.2 by the commit f149ea4b804. Could we revert to the previous behavior?\nI\u2019m OK with removing the log message or adding a setting to silence it and silencing it by default.\nHi @Gallaecio can you assign me to this issue please? Both are working so which option do you prefer?\r\n\r\n1: simply remove the log message\r\n\r\n2: add a setting to`scrapy/settings/default_settings.py` enabling users to silence the message (silenced by default) \nWe rarely assign tickets, but feel free to give it a try!\r\n\r\n@wRAR @kmike Any preference about the approach to follow?\nI would prefer removing it.",
    "created_at": "2024-09-09T19:44:42Z",
    "version": "2.11",
    "PASS_TO_PASS": [],
    "FAIL_TO_PASS": [
      "tests/test_engine.py::EngineTest::test_close_downloader",
      "tests/test_engine.py::EngineTest::test_crawler",
      "tests/test_engine.py::EngineTest::test_crawler_change_close_reason_on_idle",
      "tests/test_engine.py::EngineTest::test_crawler_dupefilter",
      "tests/test_engine.py::EngineTest::test_crawler_itemerror",
      "tests/test_engine.py::EngineTest::test_short_timeout",
      "tests/test_engine.py::EngineTest::test_start_already_running_exception",
      "tests/test_engine.py::test_request_scheduled_signal"
    ]
  },
  {
    "repo": "scrapy/scrapy",
    "pull_number": 6540,
    "instance_id": "scrapy__scrapy-6540",
    "issue_numbers": [
      "6534"
    ],
    "base_commit": "b042ad255db139adc740cd97047b6607889f9f1c",
    "patch": "diff --git a/docs/topics/email.rst b/docs/topics/email.rst\nindex d6a7ad354cb..8f7a2357a5a 100644\n--- a/docs/topics/email.rst\n+++ b/docs/topics/email.rst\n@@ -27,13 +27,13 @@ the standard ``__init__`` method:\n \n     mailer = MailSender()\n \n-Or you can instantiate it passing a Scrapy settings object, which will respect\n-the :ref:`settings <topics-email-settings>`:\n+Or you can instantiate it passing a :class:`scrapy.Crawler` instance, which\n+will respect the :ref:`settings <topics-email-settings>`:\n \n .. skip: start\n .. code-block:: python\n \n-    mailer = MailSender.from_settings(settings)\n+    mailer = MailSender.from_crawler(crawler)\n \n And here is how to use it to send an e-mail (without attachments):\n \n@@ -81,13 +81,13 @@ rest of the framework.\n     :param smtpssl: enforce using a secure SSL connection\n     :type smtpssl: bool\n \n-    .. classmethod:: from_settings(settings)\n+    .. classmethod:: from_crawler(crawler)\n \n-        Instantiate using a Scrapy settings object, which will respect\n-        :ref:`these Scrapy settings <topics-email-settings>`.\n+        Instantiate using a :class:`scrapy.Crawler` instance, which will\n+        respect :ref:`these Scrapy settings <topics-email-settings>`.\n \n-        :param settings: the e-mail recipients\n-        :type settings: :class:`scrapy.settings.Settings` object\n+        :param crawler: the crawler\n+        :type settings: :class:`scrapy.Crawler` object\n \n     .. method:: send(to, subject, body, cc=None, attachs=(), mimetype='text/plain', charset=None)\n \ndiff --git a/docs/topics/request-response.rst b/docs/topics/request-response.rst\nindex 7c15b67e8f3..710e2e1314e 100644\n--- a/docs/topics/request-response.rst\n+++ b/docs/topics/request-response.rst\n@@ -488,7 +488,7 @@ A request fingerprinter is a class that must implement the following method:\n    :param request: request to fingerprint\n    :type request: scrapy.http.Request\n \n-Additionally, it may also implement the following methods:\n+Additionally, it may also implement the following method:\n \n .. classmethod:: from_crawler(cls, crawler)\n    :noindex:\n@@ -504,13 +504,6 @@ Additionally, it may also implement the following methods:\n    :param crawler: crawler that uses this request fingerprinter\n    :type crawler: :class:`~scrapy.crawler.Crawler` object\n \n-.. classmethod:: from_settings(cls, settings)\n-\n-   If present, and ``from_crawler`` is not defined, this class method is called\n-   to create a request fingerprinter instance from a\n-   :class:`~scrapy.settings.Settings` object. It must return a new instance of\n-   the request fingerprinter.\n-\n .. currentmodule:: scrapy.http\n \n The :meth:`fingerprint` method of the default request fingerprinter,\ndiff --git a/scrapy/core/downloader/contextfactory.py b/scrapy/core/downloader/contextfactory.py\nindex f80f832a706..8e17eab9aa7 100644\n--- a/scrapy/core/downloader/contextfactory.py\n+++ b/scrapy/core/downloader/contextfactory.py\n@@ -21,6 +21,7 @@\n     ScrapyClientTLSOptions,\n     openssl_methods,\n )\n+from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.utils.misc import build_from_crawler, load_object\n \n if TYPE_CHECKING:\n@@ -69,6 +70,31 @@ def from_settings(\n         method: int = SSL.SSLv23_METHOD,\n         *args: Any,\n         **kwargs: Any,\n+    ) -> Self:\n+        warnings.warn(\n+            f\"{cls.__name__}.from_settings() is deprecated, use from_crawler() instead.\",\n+            category=ScrapyDeprecationWarning,\n+            stacklevel=2,\n+        )\n+        return cls._from_settings(settings, method, *args, **kwargs)\n+\n+    @classmethod\n+    def from_crawler(\n+        cls,\n+        crawler: Crawler,\n+        method: int = SSL.SSLv23_METHOD,\n+        *args: Any,\n+        **kwargs: Any,\n+    ) -> Self:\n+        return cls._from_settings(crawler.settings, method, *args, **kwargs)\n+\n+    @classmethod\n+    def _from_settings(\n+        cls,\n+        settings: BaseSettings,\n+        method: int = SSL.SSLv23_METHOD,\n+        *args: Any,\n+        **kwargs: Any,\n     ) -> Self:\n         tls_verbose_logging: bool = settings.getbool(\n             \"DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING\"\ndiff --git a/scrapy/dupefilters.py b/scrapy/dupefilters.py\nindex d37d2741a48..7b8eea135e7 100644\n--- a/scrapy/dupefilters.py\n+++ b/scrapy/dupefilters.py\n@@ -1,9 +1,11 @@\n from __future__ import annotations\n \n import logging\n+import warnings\n from pathlib import Path\n from typing import TYPE_CHECKING\n \n+from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.utils.job import job_dir\n from scrapy.utils.request import (\n     RequestFingerprinter,\n@@ -26,6 +28,15 @@\n class BaseDupeFilter:\n     @classmethod\n     def from_settings(cls, settings: BaseSettings) -> Self:\n+        warnings.warn(\n+            f\"{cls.__name__}.from_settings() is deprecated, use from_crawler() instead.\",\n+            category=ScrapyDeprecationWarning,\n+            stacklevel=2,\n+        )\n+        return cls()\n+\n+    @classmethod\n+    def from_crawler(cls, crawler: Crawler) -> Self:\n         return cls()\n \n     def request_seen(self, request: Request) -> bool:\n@@ -72,17 +83,31 @@ def from_settings(\n         *,\n         fingerprinter: RequestFingerprinterProtocol | None = None,\n     ) -> Self:\n-        debug = settings.getbool(\"DUPEFILTER_DEBUG\")\n-        return cls(job_dir(settings), debug, fingerprinter=fingerprinter)\n+        warnings.warn(\n+            f\"{cls.__name__}.from_settings() is deprecated, use from_crawler() instead.\",\n+            category=ScrapyDeprecationWarning,\n+            stacklevel=2,\n+        )\n+        return cls._from_settings(settings, fingerprinter=fingerprinter)\n \n     @classmethod\n     def from_crawler(cls, crawler: Crawler) -> Self:\n         assert crawler.request_fingerprinter\n-        return cls.from_settings(\n+        return cls._from_settings(\n             crawler.settings,\n             fingerprinter=crawler.request_fingerprinter,\n         )\n \n+    @classmethod\n+    def _from_settings(\n+        cls,\n+        settings: BaseSettings,\n+        *,\n+        fingerprinter: RequestFingerprinterProtocol | None = None,\n+    ) -> Self:\n+        debug = settings.getbool(\"DUPEFILTER_DEBUG\")\n+        return cls(job_dir(settings), debug, fingerprinter=fingerprinter)\n+\n     def request_seen(self, request: Request) -> bool:\n         fp = self.request_fingerprint(request)\n         if fp in self.fingerprints:\ndiff --git a/scrapy/extensions/feedexport.py b/scrapy/extensions/feedexport.py\nindex 6ab88dbb467..27f0b79ae01 100644\n--- a/scrapy/extensions/feedexport.py\n+++ b/scrapy/extensions/feedexport.py\n@@ -62,6 +62,11 @@ def build_storage(\n     preargs: Iterable[Any] = (),\n     **kwargs: Any,\n ) -> _StorageT:\n+    warnings.warn(\n+        \"scrapy.extensions.feedexport.build_storage() is deprecated, call the builder directly.\",\n+        category=ScrapyDeprecationWarning,\n+        stacklevel=2,\n+    )\n     kwargs[\"feed_options\"] = feed_options\n     return builder(*preargs, uri, *args, **kwargs)\n \n@@ -248,8 +253,7 @@ def from_crawler(\n         *,\n         feed_options: dict[str, Any] | None = None,\n     ) -> Self:\n-        return build_storage(\n-            cls,\n+        return cls(\n             uri,\n             access_key=crawler.settings[\"AWS_ACCESS_KEY_ID\"],\n             secret_key=crawler.settings[\"AWS_SECRET_ACCESS_KEY\"],\n@@ -323,10 +327,9 @@ def from_crawler(\n         *,\n         feed_options: dict[str, Any] | None = None,\n     ) -> Self:\n-        return build_storage(\n-            cls,\n+        return cls(\n             uri,\n-            crawler.settings.getbool(\"FEED_STORAGE_FTP_ACTIVE\"),\n+            use_active_mode=crawler.settings.getbool(\"FEED_STORAGE_FTP_ACTIVE\"),\n             feed_options=feed_options,\n         )\n \n@@ -407,15 +410,12 @@ def start_exporting(self) -> None:\n             self.exporter.start_exporting()\n             self._exporting = True\n \n-    def _get_instance(\n-        self, objcls: type[BaseItemExporter], *args: Any, **kwargs: Any\n-    ) -> BaseItemExporter:\n-        return build_from_crawler(objcls, self.crawler, *args, **kwargs)\n-\n     def _get_exporter(\n         self, file: IO[bytes], format: str, *args: Any, **kwargs: Any\n     ) -> BaseItemExporter:\n-        return self._get_instance(self.exporters[format], file, *args, **kwargs)\n+        return build_from_crawler(\n+            self.exporters[format], self.crawler, file, *args, **kwargs\n+        )\n \n     def finish_exporting(self) -> None:\n         if self._exporting:\n@@ -692,34 +692,8 @@ def _storage_supported(self, uri: str, feed_options: dict[str, Any]) -> bool:\n     def _get_storage(\n         self, uri: str, feed_options: dict[str, Any]\n     ) -> FeedStorageProtocol:\n-        \"\"\"Fork of create_instance specific to feed storage classes\n-\n-        It supports not passing the *feed_options* parameters to classes that\n-        do not support it, and issuing a deprecation warning instead.\n-        \"\"\"\n         feedcls = self.storages.get(urlparse(uri).scheme, self.storages[\"file\"])\n-        crawler = getattr(self, \"crawler\", None)\n-\n-        def build_instance(\n-            builder: type[FeedStorageProtocol], *preargs: Any\n-        ) -> FeedStorageProtocol:\n-            return build_storage(\n-                builder, uri, feed_options=feed_options, preargs=preargs\n-            )\n-\n-        instance: FeedStorageProtocol\n-        if crawler and hasattr(feedcls, \"from_crawler\"):\n-            instance = build_instance(feedcls.from_crawler, crawler)\n-            method_name = \"from_crawler\"\n-        elif hasattr(feedcls, \"from_settings\"):\n-            instance = build_instance(feedcls.from_settings, self.settings)\n-            method_name = \"from_settings\"\n-        else:\n-            instance = build_instance(feedcls)\n-            method_name = \"__new__\"\n-        if instance is None:\n-            raise TypeError(f\"{feedcls.__qualname__}.{method_name} returned None\")\n-        return instance\n+        return build_from_crawler(feedcls, self.crawler, uri, feed_options=feed_options)\n \n     def _get_uri_params(\n         self,\ndiff --git a/scrapy/extensions/memusage.py b/scrapy/extensions/memusage.py\nindex 73d864d5dc1..d7f810107bd 100644\n--- a/scrapy/extensions/memusage.py\n+++ b/scrapy/extensions/memusage.py\n@@ -48,7 +48,7 @@ def __init__(self, crawler: Crawler):\n         self.check_interval: float = crawler.settings.getfloat(\n             \"MEMUSAGE_CHECK_INTERVAL_SECONDS\"\n         )\n-        self.mail: MailSender = MailSender.from_settings(crawler.settings)\n+        self.mail: MailSender = MailSender.from_crawler(crawler)\n         crawler.signals.connect(self.engine_started, signal=signals.engine_started)\n         crawler.signals.connect(self.engine_stopped, signal=signals.engine_stopped)\n \ndiff --git a/scrapy/extensions/statsmailer.py b/scrapy/extensions/statsmailer.py\nindex 600eebcf2de..22162864205 100644\n--- a/scrapy/extensions/statsmailer.py\n+++ b/scrapy/extensions/statsmailer.py\n@@ -33,7 +33,7 @@ def from_crawler(cls, crawler: Crawler) -> Self:\n         recipients: list[str] = crawler.settings.getlist(\"STATSMAILER_RCPTS\")\n         if not recipients:\n             raise NotConfigured\n-        mail: MailSender = MailSender.from_settings(crawler.settings)\n+        mail: MailSender = MailSender.from_crawler(crawler)\n         assert crawler.stats\n         o = cls(crawler.stats, recipients, mail)\n         crawler.signals.connect(o.spider_closed, signal=signals.spider_closed)\ndiff --git a/scrapy/mail.py b/scrapy/mail.py\nindex ce7beb77307..3c40fea34c6 100644\n--- a/scrapy/mail.py\n+++ b/scrapy/mail.py\n@@ -7,6 +7,7 @@\n from __future__ import annotations\n \n import logging\n+import warnings\n from email import encoders as Encoders\n from email.mime.base import MIMEBase\n from email.mime.multipart import MIMEMultipart\n@@ -19,6 +20,7 @@\n from twisted.internet import ssl\n from twisted.internet.defer import Deferred\n \n+from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.utils.misc import arg_to_iter\n from scrapy.utils.python import to_bytes\n \n@@ -32,6 +34,7 @@\n     # typing.Self requires Python 3.11\n     from typing_extensions import Self\n \n+    from scrapy.crawler import Crawler\n     from scrapy.settings import BaseSettings\n \n \n@@ -72,6 +75,19 @@ def __init__(\n \n     @classmethod\n     def from_settings(cls, settings: BaseSettings) -> Self:\n+        warnings.warn(\n+            f\"{cls.__name__}.from_settings() is deprecated, use from_crawler() instead.\",\n+            category=ScrapyDeprecationWarning,\n+            stacklevel=2,\n+        )\n+        return cls._from_settings(settings)\n+\n+    @classmethod\n+    def from_crawler(cls, crawler: Crawler) -> Self:\n+        return cls._from_settings(crawler.settings)\n+\n+    @classmethod\n+    def _from_settings(cls, settings: BaseSettings) -> Self:\n         return cls(\n             smtphost=settings[\"MAIL_HOST\"],\n             mailfrom=settings[\"MAIL_FROM\"],\ndiff --git a/scrapy/middleware.py b/scrapy/middleware.py\nindex b6a4278952b..2b67dcd21a1 100644\n--- a/scrapy/middleware.py\n+++ b/scrapy/middleware.py\n@@ -2,12 +2,13 @@\n \n import logging\n import pprint\n+import warnings\n from collections import defaultdict, deque\n from typing import TYPE_CHECKING, Any, TypeVar, cast\n \n-from scrapy.exceptions import NotConfigured\n+from scrapy.exceptions import NotConfigured, ScrapyDeprecationWarning\n from scrapy.utils.defer import process_chain, process_parallel\n-from scrapy.utils.misc import build_from_crawler, build_from_settings, load_object\n+from scrapy.utils.misc import build_from_crawler, load_object\n \n if TYPE_CHECKING:\n     from collections.abc import Callable, Iterable\n@@ -20,7 +21,7 @@\n \n     from scrapy import Spider\n     from scrapy.crawler import Crawler\n-    from scrapy.settings import Settings\n+    from scrapy.settings import BaseSettings, Settings\n \n     _P = ParamSpec(\"_P\")\n \n@@ -50,8 +51,33 @@ def __init__(self, *middlewares: Any) -> None:\n     def _get_mwlist_from_settings(cls, settings: Settings) -> list[Any]:\n         raise NotImplementedError\n \n+    @staticmethod\n+    def _build_from_settings(objcls: type[_T], settings: BaseSettings) -> _T:\n+        if hasattr(objcls, \"from_settings\"):\n+            instance = objcls.from_settings(settings)  # type: ignore[attr-defined]\n+            method_name = \"from_settings\"\n+        else:\n+            instance = objcls()\n+            method_name = \"__new__\"\n+        if instance is None:\n+            raise TypeError(f\"{objcls.__qualname__}.{method_name} returned None\")\n+        return cast(_T, instance)\n+\n     @classmethod\n     def from_settings(cls, settings: Settings, crawler: Crawler | None = None) -> Self:\n+        warnings.warn(\n+            f\"{cls.__name__}.from_settings() is deprecated, use from_crawler() instead.\",\n+            category=ScrapyDeprecationWarning,\n+            stacklevel=2,\n+        )\n+        return cls._from_settings(settings, crawler)\n+\n+    @classmethod\n+    def from_crawler(cls, crawler: Crawler) -> Self:\n+        return cls._from_settings(crawler.settings, crawler)\n+\n+    @classmethod\n+    def _from_settings(cls, settings: Settings, crawler: Crawler | None = None) -> Self:\n         mwlist = cls._get_mwlist_from_settings(settings)\n         middlewares = []\n         enabled = []\n@@ -61,7 +87,7 @@ def from_settings(cls, settings: Settings, crawler: Crawler | None = None) -> Se\n                 if crawler is not None:\n                     mw = build_from_crawler(mwcls, crawler)\n                 else:\n-                    mw = build_from_settings(mwcls, settings)\n+                    mw = MiddlewareManager._build_from_settings(mwcls, settings)\n                 middlewares.append(mw)\n                 enabled.append(clspath)\n             except NotConfigured as e:\n@@ -82,10 +108,6 @@ def from_settings(cls, settings: Settings, crawler: Crawler | None = None) -> Se\n         )\n         return cls(*middlewares)\n \n-    @classmethod\n-    def from_crawler(cls, crawler: Crawler) -> Self:\n-        return cls.from_settings(crawler.settings, crawler)\n-\n     def _add_middleware(self, mw: Any) -> None:\n         if hasattr(mw, \"open_spider\"):\n             self.methods[\"open_spider\"].append(mw.open_spider)\ndiff --git a/scrapy/pipelines/files.py b/scrapy/pipelines/files.py\nindex 4a8639c220b..196b54acb7f 100644\n--- a/scrapy/pipelines/files.py\n+++ b/scrapy/pipelines/files.py\n@@ -12,6 +12,7 @@\n import logging\n import mimetypes\n import time\n+import warnings\n from collections import defaultdict\n from contextlib import suppress\n from ftplib import FTP\n@@ -24,16 +25,17 @@\n from twisted.internet.defer import Deferred, maybeDeferred\n from twisted.internet.threads import deferToThread\n \n-from scrapy.exceptions import IgnoreRequest, NotConfigured\n+from scrapy.exceptions import IgnoreRequest, NotConfigured, ScrapyDeprecationWarning\n from scrapy.http import Request, Response\n from scrapy.http.request import NO_CALLBACK\n from scrapy.pipelines.media import FileInfo, FileInfoOrError, MediaPipeline\n-from scrapy.settings import Settings\n+from scrapy.settings import BaseSettings, Settings\n from scrapy.utils.boto import is_botocore_available\n from scrapy.utils.datatypes import CaseInsensitiveDict\n+from scrapy.utils.deprecate import method_is_overridden\n from scrapy.utils.ftp import ftp_store_file\n from scrapy.utils.log import failure_to_exc_info\n-from scrapy.utils.python import to_bytes\n+from scrapy.utils.python import get_func_args, global_object_name, to_bytes\n from scrapy.utils.request import referer_str\n \n if TYPE_CHECKING:\n@@ -46,6 +48,7 @@\n     from typing_extensions import Self\n \n     from scrapy import Spider\n+    from scrapy.crawler import Crawler\n \n \n logger = logging.getLogger(__name__)\n@@ -443,12 +446,24 @@ def __init__(\n         store_uri: str | PathLike[str],\n         download_func: Callable[[Request, Spider], Response] | None = None,\n         settings: Settings | dict[str, Any] | None = None,\n+        *,\n+        crawler: Crawler | None = None,\n     ):\n         store_uri = _to_string(store_uri)\n         if not store_uri:\n             raise NotConfigured\n \n-        if isinstance(settings, dict) or settings is None:\n+        if crawler is not None:\n+            if settings is not None:\n+                warnings.warn(\n+                    f\"FilesPipeline.__init__() was called with a crawler instance and a settings instance\"\n+                    f\" when creating {global_object_name(self.__class__)}. The settings instance will be ignored\"\n+                    f\" and crawler.settings will be used. The settings argument will be removed in a future Scrapy version.\",\n+                    category=ScrapyDeprecationWarning,\n+                    stacklevel=2,\n+                )\n+            settings = crawler.settings\n+        elif isinstance(settings, dict) or settings is None:\n             settings = Settings(settings)\n         cls_name = \"FilesPipeline\"\n         self.store: FilesStoreProtocol = self._get_store(store_uri)\n@@ -467,10 +482,54 @@ def __init__(\n             resolve(\"FILES_RESULT_FIELD\"), self.FILES_RESULT_FIELD\n         )\n \n-        super().__init__(download_func=download_func, settings=settings)\n+        super().__init__(\n+            download_func=download_func,\n+            settings=settings if not crawler else None,\n+            crawler=crawler,\n+        )\n \n     @classmethod\n     def from_settings(cls, settings: Settings) -> Self:\n+        warnings.warn(\n+            f\"{cls.__name__}.from_settings() is deprecated, use from_crawler() instead.\",\n+            category=ScrapyDeprecationWarning,\n+            stacklevel=2,\n+        )\n+        return cls._from_settings(settings, None)\n+\n+    @classmethod\n+    def from_crawler(cls, crawler: Crawler) -> Self:\n+        if method_is_overridden(cls, FilesPipeline, \"from_settings\"):\n+            warnings.warn(\n+                f\"{global_object_name(cls)} overrides FilesPipeline.from_settings().\"\n+                f\" This method is deprecated and won't be called in future Scrapy versions,\"\n+                f\" please update your code so that it overrides from_crawler() instead.\",\n+                category=ScrapyDeprecationWarning,\n+            )\n+            o = cls.from_settings(crawler.settings)\n+            o._finish_init(crawler)\n+            return o\n+        return cls._from_settings(crawler.settings, crawler)\n+\n+    @classmethod\n+    def _from_settings(cls, settings: Settings, crawler: Crawler | None) -> Self:\n+        cls._update_stores(settings)\n+        store_uri = settings[\"FILES_STORE\"]\n+        if \"crawler\" in get_func_args(cls.__init__):\n+            o = cls(store_uri, crawler=crawler)\n+        else:\n+            o = cls(store_uri, settings=settings)\n+            if crawler:\n+                o._finish_init(crawler)\n+            warnings.warn(\n+                f\"{global_object_name(cls)}.__init__() doesn't take a crawler argument.\"\n+                \" This is deprecated and the argument will be required in future Scrapy versions.\",\n+                category=ScrapyDeprecationWarning,\n+            )\n+        return o\n+\n+    @classmethod\n+    def _update_stores(cls, settings: BaseSettings) -> None:\n         s3store: type[S3FilesStore] = cast(type[S3FilesStore], cls.STORE_SCHEMES[\"s3\"])\n         s3store.AWS_ACCESS_KEY_ID = settings[\"AWS_ACCESS_KEY_ID\"]\n         s3store.AWS_SECRET_ACCESS_KEY = settings[\"AWS_SECRET_ACCESS_KEY\"]\n@@ -494,9 +553,6 @@ def from_settings(cls, settings: Settings) -> Self:\n         ftp_store.FTP_PASSWORD = settings[\"FTP_PASSWORD\"]\n         ftp_store.USE_ACTIVE_MODE = settings.getbool(\"FEED_STORAGE_FTP_ACTIVE\")\n \n-        store_uri = settings[\"FILES_STORE\"]\n-        return cls(store_uri, settings=settings)\n-\n     def _get_store(self, uri: str) -> FilesStoreProtocol:\n         if Path(uri).is_absolute():  # to support win32 paths like: C:\\\\some\\dir\n             scheme = \"file\"\ndiff --git a/scrapy/pipelines/images.py b/scrapy/pipelines/images.py\nindex 2c4c9376e49..e86e7c4930e 100644\n--- a/scrapy/pipelines/images.py\n+++ b/scrapy/pipelines/images.py\n@@ -8,25 +8,19 @@\n \n import functools\n import hashlib\n+import warnings\n from contextlib import suppress\n from io import BytesIO\n-from typing import TYPE_CHECKING, Any, cast\n+from typing import TYPE_CHECKING, Any\n \n from itemadapter import ItemAdapter\n \n-from scrapy.exceptions import NotConfigured\n+from scrapy.exceptions import NotConfigured, ScrapyDeprecationWarning\n from scrapy.http import Request, Response\n from scrapy.http.request import NO_CALLBACK\n-from scrapy.pipelines.files import (\n-    FileException,\n-    FilesPipeline,\n-    FTPFilesStore,\n-    GCSFilesStore,\n-    S3FilesStore,\n-    _md5sum,\n-)\n+from scrapy.pipelines.files import FileException, FilesPipeline, _md5sum\n from scrapy.settings import Settings\n-from scrapy.utils.python import to_bytes\n+from scrapy.utils.python import get_func_args, global_object_name, to_bytes\n \n if TYPE_CHECKING:\n     from collections.abc import Callable, Iterable\n@@ -38,6 +32,7 @@\n     from typing_extensions import Self\n \n     from scrapy import Spider\n+    from scrapy.crawler import Crawler\n     from scrapy.pipelines.media import FileInfoOrError, MediaPipeline\n \n \n@@ -64,6 +59,8 @@ def __init__(\n         store_uri: str | PathLike[str],\n         download_func: Callable[[Request, Spider], Response] | None = None,\n         settings: Settings | dict[str, Any] | None = None,\n+        *,\n+        crawler: Crawler | None = None,\n     ):\n         try:\n             from PIL import Image\n@@ -74,9 +71,24 @@ def __init__(\n                 \"ImagesPipeline requires installing Pillow 4.0.0 or later\"\n             )\n \n-        super().__init__(store_uri, settings=settings, download_func=download_func)\n+        super().__init__(\n+            store_uri,\n+            settings=settings if not crawler else None,\n+            download_func=download_func,\n+            crawler=crawler,\n+        )\n \n-        if isinstance(settings, dict) or settings is None:\n+        if crawler is not None:\n+            if settings is not None:\n+                warnings.warn(\n+                    f\"ImagesPipeline.__init__() was called with a crawler instance and a settings instance\"\n+                    f\" when creating {global_object_name(self.__class__)}. The settings instance will be ignored\"\n+                    f\" and crawler.settings will be used. The settings argument will be removed in a future Scrapy version.\",\n+                    category=ScrapyDeprecationWarning,\n+                    stacklevel=2,\n+                )\n+            settings = crawler.settings\n+        elif isinstance(settings, dict) or settings is None:\n             settings = Settings(settings)\n \n         resolve = functools.partial(\n@@ -108,32 +120,21 @@ def __init__(\n         )\n \n     @classmethod\n-    def from_settings(cls, settings: Settings) -> Self:\n-        s3store: type[S3FilesStore] = cast(type[S3FilesStore], cls.STORE_SCHEMES[\"s3\"])\n-        s3store.AWS_ACCESS_KEY_ID = settings[\"AWS_ACCESS_KEY_ID\"]\n-        s3store.AWS_SECRET_ACCESS_KEY = settings[\"AWS_SECRET_ACCESS_KEY\"]\n-        s3store.AWS_SESSION_TOKEN = settings[\"AWS_SESSION_TOKEN\"]\n-        s3store.AWS_ENDPOINT_URL = settings[\"AWS_ENDPOINT_URL\"]\n-        s3store.AWS_REGION_NAME = settings[\"AWS_REGION_NAME\"]\n-        s3store.AWS_USE_SSL = settings[\"AWS_USE_SSL\"]\n-        s3store.AWS_VERIFY = settings[\"AWS_VERIFY\"]\n-        s3store.POLICY = settings[\"IMAGES_STORE_S3_ACL\"]\n-\n-        gcs_store: type[GCSFilesStore] = cast(\n-            type[GCSFilesStore], cls.STORE_SCHEMES[\"gs\"]\n-        )\n-        gcs_store.GCS_PROJECT_ID = settings[\"GCS_PROJECT_ID\"]\n-        gcs_store.POLICY = settings[\"IMAGES_STORE_GCS_ACL\"] or None\n-\n-        ftp_store: type[FTPFilesStore] = cast(\n-            type[FTPFilesStore], cls.STORE_SCHEMES[\"ftp\"]\n-        )\n-        ftp_store.FTP_USERNAME = settings[\"FTP_USER\"]\n-        ftp_store.FTP_PASSWORD = settings[\"FTP_PASSWORD\"]\n-        ftp_store.USE_ACTIVE_MODE = settings.getbool(\"FEED_STORAGE_FTP_ACTIVE\")\n-\n+    def _from_settings(cls, settings: Settings, crawler: Crawler | None) -> Self:\n+        cls._update_stores(settings)\n         store_uri = settings[\"IMAGES_STORE\"]\n-        return cls(store_uri, settings=settings)\n+        if \"crawler\" in get_func_args(cls.__init__):\n+            o = cls(store_uri, crawler=crawler)\n+        else:\n+            o = cls(store_uri, settings=settings)\n+            if crawler:\n+                o._finish_init(crawler)\n+            warnings.warn(\n+                f\"{global_object_name(cls)}.__init__() doesn't take a crawler argument.\"\n+                \" This is deprecated and the argument will be required in future Scrapy versions.\",\n+                category=ScrapyDeprecationWarning,\n+            )\n+        return o\n \n     def file_downloaded(\n         self,\ndiff --git a/scrapy/pipelines/media.py b/scrapy/pipelines/media.py\nindex b10ec147b34..6d7808c31b4 100644\n--- a/scrapy/pipelines/media.py\n+++ b/scrapy/pipelines/media.py\n@@ -2,6 +2,7 @@\n \n import functools\n import logging\n+import warnings\n from abc import ABC, abstractmethod\n from collections import defaultdict\n from typing import (\n@@ -20,12 +21,14 @@\n from twisted.python.failure import Failure\n from twisted.python.versions import Version\n \n+from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.http.request import NO_CALLBACK, Request\n from scrapy.settings import Settings\n from scrapy.utils.datatypes import SequenceExclude\n from scrapy.utils.defer import defer_result, mustbe_deferred\n from scrapy.utils.log import failure_to_exc_info\n from scrapy.utils.misc import arg_to_iter\n+from scrapy.utils.python import get_func_args, global_object_name\n \n if TYPE_CHECKING:\n     from collections.abc import Callable\n@@ -38,7 +41,6 @@\n     from scrapy.http import Response\n     from scrapy.utils.request import RequestFingerprinter\n \n-\n _T = TypeVar(\"_T\")\n \n \n@@ -51,13 +53,13 @@ class FileInfo(TypedDict):\n \n FileInfoOrError = Union[tuple[Literal[True], FileInfo], tuple[Literal[False], Failure]]\n \n-\n logger = logging.getLogger(__name__)\n \n \n class MediaPipeline(ABC):\n     crawler: Crawler\n     _fingerprinter: RequestFingerprinter\n+    _modern_init = False\n \n     LOG_FAILED_RESULTS: bool = True\n \n@@ -74,10 +76,22 @@ def __init__(\n         self,\n         download_func: Callable[[Request, Spider], Response] | None = None,\n         settings: Settings | dict[str, Any] | None = None,\n+        *,\n+        crawler: Crawler | None = None,\n     ):\n         self.download_func = download_func\n \n-        if isinstance(settings, dict) or settings is None:\n+        if crawler is not None:\n+            if settings is not None:\n+                warnings.warn(\n+                    f\"MediaPipeline.__init__() was called with a crawler instance and a settings instance\"\n+                    f\" when creating {global_object_name(self.__class__)}. The settings instance will be ignored\"\n+                    f\" and crawler.settings will be used. The settings argument will be removed in a future Scrapy version.\",\n+                    category=ScrapyDeprecationWarning,\n+                    stacklevel=2,\n+                )\n+            settings = crawler.settings\n+        elif isinstance(settings, dict) or settings is None:\n             settings = Settings(settings)\n         resolve = functools.partial(\n             self._key_for_pipe, base_class_name=\"MediaPipeline\", settings=settings\n@@ -87,6 +101,27 @@ def __init__(\n         )\n         self._handle_statuses(self.allow_redirects)\n \n+        if crawler:\n+            self._finish_init(crawler)\n+            self._modern_init = True\n+        else:\n+            warnings.warn(\n+                f\"MediaPipeline.__init__() was called without the crawler argument\"\n+                f\" when creating {global_object_name(self.__class__)}.\"\n+                f\" This is deprecated and the argument will be required in future Scrapy versions.\",\n+                category=ScrapyDeprecationWarning,\n+                stacklevel=2,\n+            )\n+\n+    def _finish_init(self, crawler: Crawler) -> None:\n+        # This was done in from_crawler() before 2.12, now it's done in __init__()\n+        # if the crawler was passed to it and may be needed to be called in other\n+        # deprecated code paths explicitly too. After the crawler argument of __init__()\n+        # becomes mandatory this should be inlined there.\n+        self.crawler = crawler\n+        assert crawler.request_fingerprinter\n+        self._fingerprinter = crawler.request_fingerprinter\n+\n     def _handle_statuses(self, allow_redirects: bool) -> None:\n         self.handle_httpstatus_list = None\n         if allow_redirects:\n@@ -112,13 +147,19 @@ def _key_for_pipe(\n     @classmethod\n     def from_crawler(cls, crawler: Crawler) -> Self:\n         pipe: Self\n-        try:\n+        if hasattr(cls, \"from_settings\"):\n             pipe = cls.from_settings(crawler.settings)  # type: ignore[attr-defined]\n-        except AttributeError:\n+        elif \"crawler\" in get_func_args(cls.__init__):\n+            pipe = cls(crawler=crawler)\n+        else:\n             pipe = cls()\n-        pipe.crawler = crawler\n-        assert crawler.request_fingerprinter\n-        pipe._fingerprinter = crawler.request_fingerprinter\n+            warnings.warn(\n+                f\"{global_object_name(cls)}.__init__() doesn't take a crawler argument.\"\n+                \" This is deprecated and the argument will be required in future Scrapy versions.\",\n+                category=ScrapyDeprecationWarning,\n+            )\n+        if not pipe._modern_init:\n+            pipe._finish_init(crawler)\n         return pipe\n \n     def open_spider(self, spider: Spider) -> None:\ndiff --git a/scrapy/spidermiddlewares/urllength.py b/scrapy/spidermiddlewares/urllength.py\nindex 191adb6cd32..a1cd1bb7cfa 100644\n--- a/scrapy/spidermiddlewares/urllength.py\n+++ b/scrapy/spidermiddlewares/urllength.py\n@@ -7,9 +7,10 @@\n from __future__ import annotations\n \n import logging\n+import warnings\n from typing import TYPE_CHECKING, Any\n \n-from scrapy.exceptions import NotConfigured\n+from scrapy.exceptions import NotConfigured, ScrapyDeprecationWarning\n from scrapy.http import Request, Response\n \n if TYPE_CHECKING:\n@@ -19,6 +20,7 @@\n     from typing_extensions import Self\n \n     from scrapy import Spider\n+    from scrapy.crawler import Crawler\n     from scrapy.settings import BaseSettings\n \n \n@@ -31,6 +33,19 @@ def __init__(self, maxlength: int):\n \n     @classmethod\n     def from_settings(cls, settings: BaseSettings) -> Self:\n+        warnings.warn(\n+            f\"{cls.__name__}.from_settings() is deprecated, use from_crawler() instead.\",\n+            category=ScrapyDeprecationWarning,\n+            stacklevel=2,\n+        )\n+        return cls._from_settings(settings)\n+\n+    @classmethod\n+    def from_crawler(cls, crawler: Crawler) -> Self:\n+        return cls._from_settings(crawler.settings)\n+\n+    @classmethod\n+    def _from_settings(cls, settings: BaseSettings) -> Self:\n         maxlength = settings.getint(\"URLLENGTH_LIMIT\")\n         if not maxlength:\n             raise NotConfigured\ndiff --git a/scrapy/utils/misc.py b/scrapy/utils/misc.py\nindex 12c09839f0f..a408a205dda 100644\n--- a/scrapy/utils/misc.py\n+++ b/scrapy/utils/misc.py\n@@ -26,7 +26,6 @@\n \n     from scrapy import Spider\n     from scrapy.crawler import Crawler\n-    from scrapy.settings import BaseSettings\n \n \n _ITERABLE_SINGLE_VALUES = dict, Item, str, bytes\n@@ -150,7 +149,7 @@ def create_instance(objcls, settings, crawler, *args, **kwargs):\n     \"\"\"\n     warnings.warn(\n         \"The create_instance() function is deprecated. \"\n-        \"Please use build_from_crawler() or build_from_settings() instead.\",\n+        \"Please use build_from_crawler() instead.\",\n         category=ScrapyDeprecationWarning,\n         stacklevel=2,\n     )\n@@ -176,7 +175,7 @@ def create_instance(objcls, settings, crawler, *args, **kwargs):\n def build_from_crawler(\n     objcls: type[T], crawler: Crawler, /, *args: Any, **kwargs: Any\n ) -> T:\n-    \"\"\"Construct a class instance using its ``from_crawler`` constructor.\n+    \"\"\"Construct a class instance using its ``from_crawler`` or ``from_settings`` constructor.\n \n     ``*args`` and ``**kwargs`` are forwarded to the constructor.\n \n@@ -186,6 +185,14 @@ def build_from_crawler(\n         instance = objcls.from_crawler(crawler, *args, **kwargs)  # type: ignore[attr-defined]\n         method_name = \"from_crawler\"\n     elif hasattr(objcls, \"from_settings\"):\n+        warnings.warn(\n+            f\"{objcls.__qualname__} has from_settings() but not from_crawler().\"\n+            \" This is deprecated and calling from_settings() will be removed in a future\"\n+            \" Scrapy version. You can implement a simple from_crawler() that calls\"\n+            \" from_settings() with crawler.settings.\",\n+            category=ScrapyDeprecationWarning,\n+            stacklevel=2,\n+        )\n         instance = objcls.from_settings(crawler.settings, *args, **kwargs)  # type: ignore[attr-defined]\n         method_name = \"from_settings\"\n     else:\n@@ -196,26 +203,6 @@ def build_from_crawler(\n     return cast(T, instance)\n \n \n-def build_from_settings(\n-    objcls: type[T], settings: BaseSettings, /, *args: Any, **kwargs: Any\n-) -> T:\n-    \"\"\"Construct a class instance using its ``from_settings`` constructor.\n-\n-    ``*args`` and ``**kwargs`` are forwarded to the constructor.\n-\n-    Raises ``TypeError`` if the resulting instance is ``None``.\n-    \"\"\"\n-    if hasattr(objcls, \"from_settings\"):\n-        instance = objcls.from_settings(settings, *args, **kwargs)  # type: ignore[attr-defined]\n-        method_name = \"from_settings\"\n-    else:\n-        instance = objcls(*args, **kwargs)\n-        method_name = \"__new__\"\n-    if instance is None:\n-        raise TypeError(f\"{objcls.__qualname__}.{method_name} returned None\")\n-    return cast(T, instance)\n-\n-\n @contextmanager\n def set_environ(**kwargs: str) -> Iterator[None]:\n     \"\"\"Temporarily set environment variables inside the context manager and\n",
    "test_patch": "diff --git a/tests/test_dupefilters.py b/tests/test_dupefilters.py\nindex 9ba8bd64f40..4fd648f4834 100644\n--- a/tests/test_dupefilters.py\n+++ b/tests/test_dupefilters.py\n@@ -33,14 +33,6 @@ def from_crawler(cls, crawler):\n         return df\n \n \n-class FromSettingsRFPDupeFilter(RFPDupeFilter):\n-    @classmethod\n-    def from_settings(cls, settings, *, fingerprinter=None):\n-        df = super().from_settings(settings, fingerprinter=fingerprinter)\n-        df.method = \"from_settings\"\n-        return df\n-\n-\n class DirectDupeFilter:\n     method = \"n/a\"\n \n@@ -56,16 +48,6 @@ def test_df_from_crawler_scheduler(self):\n         self.assertTrue(scheduler.df.debug)\n         self.assertEqual(scheduler.df.method, \"from_crawler\")\n \n-    def test_df_from_settings_scheduler(self):\n-        settings = {\n-            \"DUPEFILTER_DEBUG\": True,\n-            \"DUPEFILTER_CLASS\": FromSettingsRFPDupeFilter,\n-        }\n-        crawler = get_crawler(settings_dict=settings)\n-        scheduler = Scheduler.from_crawler(crawler)\n-        self.assertTrue(scheduler.df.debug)\n-        self.assertEqual(scheduler.df.method, \"from_settings\")\n-\n     def test_df_direct_scheduler(self):\n         settings = {\n             \"DUPEFILTER_CLASS\": DirectDupeFilter,\ndiff --git a/tests/test_middleware.py b/tests/test_middleware.py\nindex a42c7b3d1e2..3a1cf19ad30 100644\n--- a/tests/test_middleware.py\n+++ b/tests/test_middleware.py\n@@ -2,7 +2,7 @@\n \n from scrapy.exceptions import NotConfigured\n from scrapy.middleware import MiddlewareManager\n-from scrapy.settings import Settings\n+from scrapy.utils.test import get_crawler\n \n \n class M1:\n@@ -23,8 +23,6 @@ def open_spider(self, spider):\n     def close_spider(self, spider):\n         pass\n \n-    pass\n-\n \n class M3:\n     def process(self, response, request, spider):\n@@ -83,7 +81,7 @@ def test_enabled(self):\n         self.assertEqual(mwman.middlewares, (m1, m2, m3))\n \n     def test_enabled_from_settings(self):\n-        settings = Settings()\n-        mwman = TestMiddlewareManager.from_settings(settings)\n+        crawler = get_crawler()\n+        mwman = TestMiddlewareManager.from_crawler(crawler)\n         classes = [x.__class__ for x in mwman.middlewares]\n         self.assertEqual(classes, [M1, M3])\ndiff --git a/tests/test_pipeline_files.py b/tests/test_pipeline_files.py\nindex 47840caaa16..9dcb3e4d18d 100644\n--- a/tests/test_pipeline_files.py\n+++ b/tests/test_pipeline_files.py\n@@ -2,6 +2,7 @@\n import os\n import random\n import time\n+import warnings\n from datetime import datetime\n from io import BytesIO\n from pathlib import Path\n@@ -25,7 +26,6 @@\n     GCSFilesStore,\n     S3FilesStore,\n )\n-from scrapy.settings import Settings\n from scrapy.utils.test import (\n     assert_gcs_environ,\n     get_crawler,\n@@ -217,8 +217,8 @@ class CustomFilesPipeline(FilesPipeline):\n             def file_path(self, request, response=None, info=None, item=None):\n                 return f'full/{item.get(\"path\")}'\n \n-        file_path = CustomFilesPipeline.from_settings(\n-            Settings({\"FILES_STORE\": self.tempdir})\n+        file_path = CustomFilesPipeline.from_crawler(\n+            get_crawler(None, {\"FILES_STORE\": self.tempdir})\n         ).file_path\n         item = {\"path\": \"path-to-store-file\"}\n         request = Request(\"http://example.com\")\n@@ -235,7 +235,9 @@ def tearDown(self):\n     def test_item_fields_default(self):\n         url = \"http://www.example.com/files/1.txt\"\n         item = self.item_class(name=\"item1\", file_urls=[url])\n-        pipeline = FilesPipeline.from_settings(Settings({\"FILES_STORE\": self.tempdir}))\n+        pipeline = FilesPipeline.from_crawler(\n+            get_crawler(None, {\"FILES_STORE\": self.tempdir})\n+        )\n         requests = list(pipeline.get_media_requests(item, None))\n         self.assertEqual(requests[0].url, url)\n         results = [(True, {\"url\": url})]\n@@ -247,13 +249,14 @@ def test_item_fields_default(self):\n     def test_item_fields_override_settings(self):\n         url = \"http://www.example.com/files/1.txt\"\n         item = self.item_class(name=\"item1\", custom_file_urls=[url])\n-        pipeline = FilesPipeline.from_settings(\n-            Settings(\n+        pipeline = FilesPipeline.from_crawler(\n+            get_crawler(\n+                None,\n                 {\n                     \"FILES_STORE\": self.tempdir,\n                     \"FILES_URLS_FIELD\": \"custom_file_urls\",\n                     \"FILES_RESULT_FIELD\": \"custom_files\",\n-                }\n+                },\n             )\n         )\n         requests = list(pipeline.get_media_requests(item, None))\n@@ -371,8 +374,10 @@ def test_different_settings_for_different_instances(self):\n         different settings.\n         \"\"\"\n         custom_settings = self._generate_fake_settings()\n-        another_pipeline = FilesPipeline.from_settings(Settings(custom_settings))\n-        one_pipeline = FilesPipeline(self.tempdir)\n+        another_pipeline = FilesPipeline.from_crawler(\n+            get_crawler(None, custom_settings)\n+        )\n+        one_pipeline = FilesPipeline(self.tempdir, crawler=get_crawler(None))\n         for pipe_attr, settings_attr, pipe_ins_attr in self.file_cls_attr_settings_map:\n             default_value = self.default_cls_settings[pipe_attr]\n             self.assertEqual(getattr(one_pipeline, pipe_attr), default_value)\n@@ -385,7 +390,7 @@ def test_subclass_attributes_preserved_if_no_settings(self):\n         If subclasses override class attributes and there are no special settings those values should be kept.\n         \"\"\"\n         pipe_cls = self._generate_fake_pipeline()\n-        pipe = pipe_cls.from_settings(Settings({\"FILES_STORE\": self.tempdir}))\n+        pipe = pipe_cls.from_crawler(get_crawler(None, {\"FILES_STORE\": self.tempdir}))\n         for pipe_attr, settings_attr, pipe_ins_attr in self.file_cls_attr_settings_map:\n             custom_value = getattr(pipe, pipe_ins_attr)\n             self.assertNotEqual(custom_value, self.default_cls_settings[pipe_attr])\n@@ -398,7 +403,7 @@ def test_subclass_attrs_preserved_custom_settings(self):\n         \"\"\"\n         pipeline_cls = self._generate_fake_pipeline()\n         settings = self._generate_fake_settings()\n-        pipeline = pipeline_cls.from_settings(Settings(settings))\n+        pipeline = pipeline_cls.from_crawler(get_crawler(None, settings))\n         for pipe_attr, settings_attr, pipe_ins_attr in self.file_cls_attr_settings_map:\n             value = getattr(pipeline, pipe_ins_attr)\n             setting_value = settings.get(settings_attr)\n@@ -414,8 +419,8 @@ def test_no_custom_settings_for_subclasses(self):\n         class UserDefinedFilesPipeline(FilesPipeline):\n             pass\n \n-        user_pipeline = UserDefinedFilesPipeline.from_settings(\n-            Settings({\"FILES_STORE\": self.tempdir})\n+        user_pipeline = UserDefinedFilesPipeline.from_crawler(\n+            get_crawler(None, {\"FILES_STORE\": self.tempdir})\n         )\n         for pipe_attr, settings_attr, pipe_ins_attr in self.file_cls_attr_settings_map:\n             # Values from settings for custom pipeline should be set on pipeline instance.\n@@ -433,7 +438,9 @@ class UserDefinedFilesPipeline(FilesPipeline):\n \n         prefix = UserDefinedFilesPipeline.__name__.upper()\n         settings = self._generate_fake_settings(prefix=prefix)\n-        user_pipeline = UserDefinedFilesPipeline.from_settings(Settings(settings))\n+        user_pipeline = UserDefinedFilesPipeline.from_crawler(\n+            get_crawler(None, settings)\n+        )\n         for pipe_attr, settings_attr, pipe_inst_attr in self.file_cls_attr_settings_map:\n             # Values from settings for custom pipeline should be set on pipeline instance.\n             custom_value = settings.get(prefix + \"_\" + settings_attr)\n@@ -448,7 +455,7 @@ def test_custom_settings_and_class_attrs_for_subclasses(self):\n         pipeline_cls = self._generate_fake_pipeline()\n         prefix = pipeline_cls.__name__.upper()\n         settings = self._generate_fake_settings(prefix=prefix)\n-        user_pipeline = pipeline_cls.from_settings(Settings(settings))\n+        user_pipeline = pipeline_cls.from_crawler(get_crawler(None, settings))\n         for (\n             pipe_cls_attr,\n             settings_attr,\n@@ -463,8 +470,8 @@ class UserDefinedFilesPipeline(FilesPipeline):\n             DEFAULT_FILES_RESULT_FIELD = \"this\"\n             DEFAULT_FILES_URLS_FIELD = \"that\"\n \n-        pipeline = UserDefinedFilesPipeline.from_settings(\n-            Settings({\"FILES_STORE\": self.tempdir})\n+        pipeline = UserDefinedFilesPipeline.from_crawler(\n+            get_crawler(None, {\"FILES_STORE\": self.tempdir})\n         )\n         self.assertEqual(\n             pipeline.files_result_field,\n@@ -484,7 +491,7 @@ def test_user_defined_subclass_default_key_names(self):\n         class UserPipe(FilesPipeline):\n             pass\n \n-        pipeline_cls = UserPipe.from_settings(Settings(settings))\n+        pipeline_cls = UserPipe.from_crawler(get_crawler(None, settings))\n \n         for pipe_attr, settings_attr, pipe_inst_attr in self.file_cls_attr_settings_map:\n             expected_value = settings.get(settings_attr)\n@@ -495,8 +502,8 @@ class CustomFilesPipelineWithPathLikeDir(FilesPipeline):\n             def file_path(self, request, response=None, info=None, *, item=None):\n                 return Path(\"subdir\") / Path(request.url).name\n \n-        pipeline = CustomFilesPipelineWithPathLikeDir.from_settings(\n-            Settings({\"FILES_STORE\": Path(\"./Temp\")})\n+        pipeline = CustomFilesPipelineWithPathLikeDir.from_crawler(\n+            get_crawler(None, {\"FILES_STORE\": Path(\"./Temp\")})\n         )\n         request = Request(\"http://example.com/image01.jpg\")\n         self.assertEqual(pipeline.file_path(request), Path(\"subdir/image01.jpg\"))\n@@ -687,3 +694,75 @@ def _prepare_request_object(item_url, flags=None):\n         item_url,\n         meta={\"response\": Response(item_url, status=200, body=b\"data\", flags=flags)},\n     )\n+\n+\n+# this is separate from the one in test_pipeline_media.py to specifically test FilesPipeline subclasses\n+class BuildFromCrawlerTestCase(unittest.TestCase):\n+    def setUp(self):\n+        self.tempdir = mkdtemp()\n+        self.crawler = get_crawler(None, {\"FILES_STORE\": self.tempdir})\n+\n+    def tearDown(self):\n+        rmtree(self.tempdir)\n+\n+    def test_simple(self):\n+        class Pipeline(FilesPipeline):\n+            pass\n+\n+        with warnings.catch_warnings(record=True) as w:\n+            pipe = Pipeline.from_crawler(self.crawler)\n+            assert pipe.crawler == self.crawler\n+            assert pipe._fingerprinter\n+            self.assertEqual(len(w), 0)\n+            assert pipe.store\n+\n+    def test_has_old_init(self):\n+        class Pipeline(FilesPipeline):\n+            def __init__(self, store_uri, download_func=None, settings=None):\n+                super().__init__(store_uri, download_func, settings)\n+                self._init_called = True\n+\n+        with warnings.catch_warnings(record=True) as w:\n+            pipe = Pipeline.from_crawler(self.crawler)\n+            assert pipe.crawler == self.crawler\n+            assert pipe._fingerprinter\n+            self.assertEqual(len(w), 2)\n+            assert pipe._init_called\n+\n+    def test_has_from_settings(self):\n+        class Pipeline(FilesPipeline):\n+            _from_settings_called = False\n+\n+            @classmethod\n+            def from_settings(cls, settings):\n+                o = super().from_settings(settings)\n+                o._from_settings_called = True\n+                return o\n+\n+        with warnings.catch_warnings(record=True) as w:\n+            pipe = Pipeline.from_crawler(self.crawler)\n+            assert pipe.crawler == self.crawler\n+            assert pipe._fingerprinter\n+            self.assertEqual(len(w), 3)\n+            assert pipe.store\n+            assert pipe._from_settings_called\n+\n+    def test_has_from_crawler_and_init(self):\n+        class Pipeline(FilesPipeline):\n+            _from_crawler_called = False\n+\n+            @classmethod\n+            def from_crawler(cls, crawler):\n+                settings = crawler.settings\n+                store_uri = settings[\"FILES_STORE\"]\n+                o = cls(store_uri, crawler=crawler)\n+                o._from_crawler_called = True\n+                return o\n+\n+        with warnings.catch_warnings(record=True) as w:\n+            pipe = Pipeline.from_crawler(self.crawler)\n+            assert pipe.crawler == self.crawler\n+            assert pipe._fingerprinter\n+            self.assertEqual(len(w), 0)\n+            assert pipe.store\n+            assert pipe._from_crawler_called\ndiff --git a/tests/test_pipeline_images.py b/tests/test_pipeline_images.py\nindex dfeead999d5..3ffef410249 100644\n--- a/tests/test_pipeline_images.py\n+++ b/tests/test_pipeline_images.py\n@@ -13,7 +13,7 @@\n from scrapy.http import Request, Response\n from scrapy.item import Field, Item\n from scrapy.pipelines.images import ImageException, ImagesPipeline\n-from scrapy.settings import Settings\n+from scrapy.utils.test import get_crawler\n \n skip_pillow: str | None\n try:\n@@ -33,7 +33,8 @@ class ImagesPipelineTestCase(unittest.TestCase):\n \n     def setUp(self):\n         self.tempdir = mkdtemp()\n-        self.pipeline = ImagesPipeline(self.tempdir)\n+        crawler = get_crawler()\n+        self.pipeline = ImagesPipeline(self.tempdir, crawler=crawler)\n \n     def tearDown(self):\n         rmtree(self.tempdir)\n@@ -123,8 +124,8 @@ def thumb_path(\n             ):\n                 return f\"thumb/{thumb_id}/{item.get('path')}\"\n \n-        thumb_path = CustomImagesPipeline.from_settings(\n-            Settings({\"IMAGES_STORE\": self.tempdir})\n+        thumb_path = CustomImagesPipeline.from_crawler(\n+            get_crawler(None, {\"IMAGES_STORE\": self.tempdir})\n         ).thumb_path\n         item = {\"path\": \"path-to-store-file\"}\n         request = Request(\"http://example.com\")\n@@ -218,8 +219,8 @@ class ImagesPipelineTestCaseFieldsMixin:\n     def test_item_fields_default(self):\n         url = \"http://www.example.com/images/1.jpg\"\n         item = self.item_class(name=\"item1\", image_urls=[url])\n-        pipeline = ImagesPipeline.from_settings(\n-            Settings({\"IMAGES_STORE\": \"s3://example/images/\"})\n+        pipeline = ImagesPipeline.from_crawler(\n+            get_crawler(None, {\"IMAGES_STORE\": \"s3://example/images/\"})\n         )\n         requests = list(pipeline.get_media_requests(item, None))\n         self.assertEqual(requests[0].url, url)\n@@ -232,13 +233,14 @@ def test_item_fields_default(self):\n     def test_item_fields_override_settings(self):\n         url = \"http://www.example.com/images/1.jpg\"\n         item = self.item_class(name=\"item1\", custom_image_urls=[url])\n-        pipeline = ImagesPipeline.from_settings(\n-            Settings(\n+        pipeline = ImagesPipeline.from_crawler(\n+            get_crawler(\n+                None,\n                 {\n                     \"IMAGES_STORE\": \"s3://example/images/\",\n                     \"IMAGES_URLS_FIELD\": \"custom_image_urls\",\n                     \"IMAGES_RESULT_FIELD\": \"custom_images\",\n-                }\n+                },\n             )\n         )\n         requests = list(pipeline.get_media_requests(item, None))\n@@ -389,9 +391,8 @@ def test_different_settings_for_different_instances(self):\n         have different settings.\n         \"\"\"\n         custom_settings = self._generate_fake_settings()\n-        default_settings = Settings()\n-        default_sts_pipe = ImagesPipeline(self.tempdir, settings=default_settings)\n-        user_sts_pipe = ImagesPipeline.from_settings(Settings(custom_settings))\n+        default_sts_pipe = ImagesPipeline(self.tempdir, crawler=get_crawler(None))\n+        user_sts_pipe = ImagesPipeline.from_crawler(get_crawler(None, custom_settings))\n         for pipe_attr, settings_attr in self.img_cls_attribute_names:\n             expected_default_value = self.default_pipeline_settings.get(pipe_attr)\n             custom_value = custom_settings.get(settings_attr)\n@@ -407,7 +408,9 @@ def test_subclass_attrs_preserved_default_settings(self):\n         from class attributes.\n         \"\"\"\n         pipeline_cls = self._generate_fake_pipeline_subclass()\n-        pipeline = pipeline_cls.from_settings(Settings({\"IMAGES_STORE\": self.tempdir}))\n+        pipeline = pipeline_cls.from_crawler(\n+            get_crawler(None, {\"IMAGES_STORE\": self.tempdir})\n+        )\n         for pipe_attr, settings_attr in self.img_cls_attribute_names:\n             # Instance attribute (lowercase) must be equal to class attribute (uppercase).\n             attr_value = getattr(pipeline, pipe_attr.lower())\n@@ -421,7 +424,7 @@ def test_subclass_attrs_preserved_custom_settings(self):\n         \"\"\"\n         pipeline_cls = self._generate_fake_pipeline_subclass()\n         settings = self._generate_fake_settings()\n-        pipeline = pipeline_cls.from_settings(Settings(settings))\n+        pipeline = pipeline_cls.from_crawler(get_crawler(None, settings))\n         for pipe_attr, settings_attr in self.img_cls_attribute_names:\n             # Instance attribute (lowercase) must be equal to\n             # value defined in settings.\n@@ -439,8 +442,8 @@ def test_no_custom_settings_for_subclasses(self):\n         class UserDefinedImagePipeline(ImagesPipeline):\n             pass\n \n-        user_pipeline = UserDefinedImagePipeline.from_settings(\n-            Settings({\"IMAGES_STORE\": self.tempdir})\n+        user_pipeline = UserDefinedImagePipeline.from_crawler(\n+            get_crawler(None, {\"IMAGES_STORE\": self.tempdir})\n         )\n         for pipe_attr, settings_attr in self.img_cls_attribute_names:\n             # Values from settings for custom pipeline should be set on pipeline instance.\n@@ -458,7 +461,9 @@ class UserDefinedImagePipeline(ImagesPipeline):\n \n         prefix = UserDefinedImagePipeline.__name__.upper()\n         settings = self._generate_fake_settings(prefix=prefix)\n-        user_pipeline = UserDefinedImagePipeline.from_settings(Settings(settings))\n+        user_pipeline = UserDefinedImagePipeline.from_crawler(\n+            get_crawler(None, settings)\n+        )\n         for pipe_attr, settings_attr in self.img_cls_attribute_names:\n             # Values from settings for custom pipeline should be set on pipeline instance.\n             custom_value = settings.get(prefix + \"_\" + settings_attr)\n@@ -473,7 +478,7 @@ def test_custom_settings_and_class_attrs_for_subclasses(self):\n         pipeline_cls = self._generate_fake_pipeline_subclass()\n         prefix = pipeline_cls.__name__.upper()\n         settings = self._generate_fake_settings(prefix=prefix)\n-        user_pipeline = pipeline_cls.from_settings(Settings(settings))\n+        user_pipeline = pipeline_cls.from_crawler(get_crawler(None, settings))\n         for pipe_attr, settings_attr in self.img_cls_attribute_names:\n             custom_value = settings.get(prefix + \"_\" + settings_attr)\n             self.assertNotEqual(custom_value, self.default_pipeline_settings[pipe_attr])\n@@ -484,8 +489,8 @@ class UserDefinedImagePipeline(ImagesPipeline):\n             DEFAULT_IMAGES_URLS_FIELD = \"something\"\n             DEFAULT_IMAGES_RESULT_FIELD = \"something_else\"\n \n-        pipeline = UserDefinedImagePipeline.from_settings(\n-            Settings({\"IMAGES_STORE\": self.tempdir})\n+        pipeline = UserDefinedImagePipeline.from_crawler(\n+            get_crawler(None, {\"IMAGES_STORE\": self.tempdir})\n         )\n         self.assertEqual(\n             pipeline.images_result_field,\n@@ -506,7 +511,7 @@ def test_user_defined_subclass_default_key_names(self):\n         class UserPipe(ImagesPipeline):\n             pass\n \n-        pipeline_cls = UserPipe.from_settings(Settings(settings))\n+        pipeline_cls = UserPipe.from_crawler(get_crawler(None, settings))\n \n         for pipe_attr, settings_attr in self.img_cls_attribute_names:\n             expected_value = settings.get(settings_attr)\ndiff --git a/tests/test_pipeline_media.py b/tests/test_pipeline_media.py\nindex c979e45d70a..58a2d367825 100644\n--- a/tests/test_pipeline_media.py\n+++ b/tests/test_pipeline_media.py\n@@ -1,5 +1,7 @@\n from __future__ import annotations\n \n+import warnings\n+\n from testfixtures import LogCapture\n from twisted.internet import reactor\n from twisted.internet.defer import Deferred, inlineCallbacks\n@@ -11,7 +13,6 @@\n from scrapy.http.request import NO_CALLBACK\n from scrapy.pipelines.files import FileException\n from scrapy.pipelines.media import MediaPipeline\n-from scrapy.settings import Settings\n from scrapy.spiders import Spider\n from scrapy.utils.log import failure_to_exc_info\n from scrapy.utils.signal import disconnect_all\n@@ -175,8 +176,8 @@ def test_default_process_item(self):\n \n \n class MockedMediaPipeline(UserDefinedPipeline):\n-    def __init__(self, *args, **kwargs):\n-        super().__init__(*args, **kwargs)\n+    def __init__(self, *args, crawler=None, **kwargs):\n+        super().__init__(*args, crawler=crawler, **kwargs)\n         self._mockcalled = []\n \n     def download(self, request, info):\n@@ -377,7 +378,7 @@ def test_key_for_pipe(self):\n class MediaPipelineAllowRedirectSettingsTestCase(unittest.TestCase):\n \n     def _assert_request_no3xx(self, pipeline_class, settings):\n-        pipe = pipeline_class(settings=Settings(settings))\n+        pipe = pipeline_class(crawler=get_crawler(None, settings))\n         request = Request(\"http://url\")\n         pipe._modify_media_request(request)\n \n@@ -410,3 +411,115 @@ def test_subclass_specific_setting(self):\n         self._assert_request_no3xx(\n             UserDefinedPipeline, {\"USERDEFINEDPIPELINE_MEDIA_ALLOW_REDIRECTS\": True}\n         )\n+\n+\n+class BuildFromCrawlerTestCase(unittest.TestCase):\n+    def setUp(self):\n+        self.crawler = get_crawler(None, {\"FILES_STORE\": \"/foo\"})\n+\n+    def test_simple(self):\n+        class Pipeline(UserDefinedPipeline):\n+            pass\n+\n+        with warnings.catch_warnings(record=True) as w:\n+            pipe = Pipeline.from_crawler(self.crawler)\n+            assert pipe.crawler == self.crawler\n+            assert pipe._fingerprinter\n+            self.assertEqual(len(w), 0)\n+\n+    def test_has_old_init(self):\n+        class Pipeline(UserDefinedPipeline):\n+            def __init__(self):\n+                super().__init__()\n+                self._init_called = True\n+\n+        with warnings.catch_warnings(record=True) as w:\n+            pipe = Pipeline.from_crawler(self.crawler)\n+            assert pipe.crawler == self.crawler\n+            assert pipe._fingerprinter\n+            self.assertEqual(len(w), 2)\n+            assert pipe._init_called\n+\n+    def test_has_from_settings(self):\n+        class Pipeline(UserDefinedPipeline):\n+            _from_settings_called = False\n+\n+            @classmethod\n+            def from_settings(cls, settings):\n+                o = cls()\n+                o._from_settings_called = True\n+                return o\n+\n+        with warnings.catch_warnings(record=True) as w:\n+            pipe = Pipeline.from_crawler(self.crawler)\n+            assert pipe.crawler == self.crawler\n+            assert pipe._fingerprinter\n+            self.assertEqual(len(w), 1)\n+            assert pipe._from_settings_called\n+\n+    def test_has_from_settings_and_init(self):\n+        class Pipeline(UserDefinedPipeline):\n+            _from_settings_called = False\n+\n+            def __init__(self, store_uri, settings):\n+                super().__init__()\n+                self._init_called = True\n+\n+            @classmethod\n+            def from_settings(cls, settings):\n+                store_uri = settings[\"FILES_STORE\"]\n+                o = cls(store_uri, settings=settings)\n+                o._from_settings_called = True\n+                return o\n+\n+        with warnings.catch_warnings(record=True) as w:\n+            pipe = Pipeline.from_crawler(self.crawler)\n+            assert pipe.crawler == self.crawler\n+            assert pipe._fingerprinter\n+            self.assertEqual(len(w), 1)\n+            assert pipe._from_settings_called\n+            assert pipe._init_called\n+\n+    def test_has_from_crawler_and_init(self):\n+        class Pipeline(UserDefinedPipeline):\n+            _from_crawler_called = False\n+\n+            def __init__(self, store_uri, settings, *, crawler):\n+                super().__init__(crawler=crawler)\n+                self._init_called = True\n+\n+            @classmethod\n+            def from_crawler(cls, crawler):\n+                settings = crawler.settings\n+                store_uri = settings[\"FILES_STORE\"]\n+                o = cls(store_uri, settings=settings, crawler=crawler)\n+                o._from_crawler_called = True\n+                return o\n+\n+        with warnings.catch_warnings(record=True) as w:\n+            pipe = Pipeline.from_crawler(self.crawler)\n+            assert pipe.crawler == self.crawler\n+            assert pipe._fingerprinter\n+            self.assertEqual(len(w), 0)\n+            assert pipe._from_crawler_called\n+            assert pipe._init_called\n+\n+    def test_has_from_crawler(self):\n+        class Pipeline(UserDefinedPipeline):\n+            _from_crawler_called = False\n+\n+            @classmethod\n+            def from_crawler(cls, crawler):\n+                settings = crawler.settings\n+                o = super().from_crawler(crawler)\n+                o._from_crawler_called = True\n+                o.store_uri = settings[\"FILES_STORE\"]\n+                return o\n+\n+        with warnings.catch_warnings(record=True) as w:\n+            pipe = Pipeline.from_crawler(self.crawler)\n+            # this and the next assert will fail as MediaPipeline.from_crawler() wasn't called\n+            assert pipe.crawler == self.crawler\n+            assert pipe._fingerprinter\n+            self.assertEqual(len(w), 0)\n+            assert pipe._from_crawler_called\ndiff --git a/tests/test_spidermiddleware_urllength.py b/tests/test_spidermiddleware_urllength.py\nindex 9111e4c82ab..1a0f2e223c4 100644\n--- a/tests/test_spidermiddleware_urllength.py\n+++ b/tests/test_spidermiddleware_urllength.py\n@@ -3,7 +3,6 @@\n from testfixtures import LogCapture\n \n from scrapy.http import Request, Response\n-from scrapy.settings import Settings\n from scrapy.spidermiddlewares.urllength import UrlLengthMiddleware\n from scrapy.spiders import Spider\n from scrapy.utils.test import get_crawler\n@@ -12,12 +11,10 @@\n class TestUrlLengthMiddleware(TestCase):\n     def setUp(self):\n         self.maxlength = 25\n-        settings = Settings({\"URLLENGTH_LIMIT\": self.maxlength})\n-\n-        crawler = get_crawler(Spider)\n+        crawler = get_crawler(Spider, {\"URLLENGTH_LIMIT\": self.maxlength})\n         self.spider = crawler._create_spider(\"foo\")\n         self.stats = crawler.stats\n-        self.mw = UrlLengthMiddleware.from_settings(settings)\n+        self.mw = UrlLengthMiddleware.from_crawler(crawler)\n \n         self.response = Response(\"http://scrapytest.org\")\n         self.short_url_req = Request(\"http://scrapytest.org/\")\ndiff --git a/tests/test_utils_misc/__init__.py b/tests/test_utils_misc/__init__.py\nindex 4d8e715210d..f71b2b034a9 100644\n--- a/tests/test_utils_misc/__init__.py\n+++ b/tests/test_utils_misc/__init__.py\n@@ -10,7 +10,6 @@\n from scrapy.utils.misc import (\n     arg_to_iter,\n     build_from_crawler,\n-    build_from_settings,\n     create_instance,\n     load_object,\n     rel_has_nofollow,\n@@ -197,39 +196,6 @@ def _test_with_crawler(mock, settings, crawler):\n         with self.assertRaises(TypeError):\n             build_from_crawler(m, crawler, *args, **kwargs)\n \n-    def test_build_from_settings(self):\n-        settings = mock.MagicMock()\n-        args = (True, 100.0)\n-        kwargs = {\"key\": \"val\"}\n-\n-        def _test_with_settings(mock, settings):\n-            build_from_settings(mock, settings, *args, **kwargs)\n-            if hasattr(mock, \"from_settings\"):\n-                mock.from_settings.assert_called_once_with(settings, *args, **kwargs)\n-                self.assertEqual(mock.call_count, 0)\n-            else:\n-                mock.assert_called_once_with(*args, **kwargs)\n-\n-        # Check usage of correct constructor using three mocks:\n-        #   1. with no alternative constructors\n-        #   2. with from_settings() constructor\n-        #   3. with from_settings() and from_crawler() constructor\n-        spec_sets = (\n-            [\"__qualname__\"],\n-            [\"__qualname__\", \"from_settings\"],\n-            [\"__qualname__\", \"from_settings\", \"from_crawler\"],\n-        )\n-        for specs in spec_sets:\n-            m = mock.MagicMock(spec_set=specs)\n-            _test_with_settings(m, settings)\n-            m.reset_mock()\n-\n-        # Check adoption of crawler settings\n-        m = mock.MagicMock(spec_set=[\"__qualname__\", \"from_settings\"])\n-        m.from_settings.return_value = None\n-        with self.assertRaises(TypeError):\n-            build_from_settings(m, settings, *args, **kwargs)\n-\n     def test_set_environ(self):\n         assert os.environ.get(\"some_test_environ\") is None\n         with set_environ(some_test_environ=\"test_value\"):\ndiff --git a/tests/test_utils_request.py b/tests/test_utils_request.py\nindex 965d050a4da..0a3e3b00be5 100644\n--- a/tests/test_utils_request.py\n+++ b/tests/test_utils_request.py\n@@ -8,6 +8,7 @@\n \n import pytest\n \n+from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.http import Request\n from scrapy.utils.python import to_bytes\n from scrapy.utils.request import (\n@@ -384,7 +385,9 @@ def fingerprint(self, request):\n             \"REQUEST_FINGERPRINTER_CLASS\": RequestFingerprinter,\n             \"FINGERPRINT\": b\"fingerprint\",\n         }\n-        crawler = get_crawler(settings_dict=settings)\n+        with warnings.catch_warnings():\n+            warnings.simplefilter(\"ignore\", ScrapyDeprecationWarning)\n+            crawler = get_crawler(settings_dict=settings)\n \n         request = Request(\"http://www.example.com\")\n         fingerprint = crawler.request_fingerprinter.fingerprint(request)\ndiff --git a/tests/test_webclient.py b/tests/test_webclient.py\nindex cce119001ac..1797d5e1fcb 100644\n--- a/tests/test_webclient.py\n+++ b/tests/test_webclient.py\n@@ -9,25 +9,18 @@\n \n import OpenSSL.SSL\n from twisted.internet import defer, reactor\n-from twisted.trial import unittest\n-from twisted.web import resource, server, static, util\n-\n-try:\n-    from twisted.internet.testing import StringTransport\n-except ImportError:\n-    # deprecated in Twisted 19.7.0\n-    # (remove once we bump our requirement past that version)\n-    from twisted.test.proto_helpers import StringTransport\n-\n from twisted.internet.defer import inlineCallbacks\n+from twisted.internet.testing import StringTransport\n from twisted.protocols.policies import WrappingFactory\n+from twisted.trial import unittest\n+from twisted.web import resource, server, static, util\n \n from scrapy.core.downloader import webclient as client\n from scrapy.core.downloader.contextfactory import ScrapyClientContextFactory\n from scrapy.http import Headers, Request\n-from scrapy.settings import Settings\n-from scrapy.utils.misc import build_from_settings\n+from scrapy.utils.misc import build_from_crawler\n from scrapy.utils.python import to_bytes, to_unicode\n+from scrapy.utils.test import get_crawler\n from tests.mockserver import (\n     BrokenDownloadResource,\n     ErrorResource,\n@@ -469,22 +462,22 @@ class WebClientCustomCiphersSSLTestCase(WebClientSSLTestCase):\n \n     def testPayload(self):\n         s = \"0123456789\" * 10\n-        settings = Settings({\"DOWNLOADER_CLIENT_TLS_CIPHERS\": self.custom_ciphers})\n-        client_context_factory = build_from_settings(\n-            ScrapyClientContextFactory, settings\n+        crawler = get_crawler(\n+            settings_dict={\"DOWNLOADER_CLIENT_TLS_CIPHERS\": self.custom_ciphers}\n         )\n+        client_context_factory = build_from_crawler(ScrapyClientContextFactory, crawler)\n         return getPage(\n             self.getURL(\"payload\"), body=s, contextFactory=client_context_factory\n         ).addCallback(self.assertEqual, to_bytes(s))\n \n     def testPayloadDisabledCipher(self):\n         s = \"0123456789\" * 10\n-        settings = Settings(\n-            {\"DOWNLOADER_CLIENT_TLS_CIPHERS\": \"ECDHE-RSA-AES256-GCM-SHA384\"}\n-        )\n-        client_context_factory = build_from_settings(\n-            ScrapyClientContextFactory, settings\n+        crawler = get_crawler(\n+            settings_dict={\n+                \"DOWNLOADER_CLIENT_TLS_CIPHERS\": \"ECDHE-RSA-AES256-GCM-SHA384\"\n+            }\n         )\n+        client_context_factory = build_from_crawler(ScrapyClientContextFactory, crawler)\n         d = getPage(\n             self.getURL(\"payload\"), body=s, contextFactory=client_context_factory\n         )\n",
    "problem_statement": "Don't ship `build_from_settings()`\nWe discussed this with @kmike and decided that we want to ship `build_from_crawler()` but not `build_from_settings()` as a last-minute follow-up to #5523/#6169. This, to my knowledge, has two consequences:\r\n\r\n1. We need to change `scrapy.middleware.MiddlewareManager` to require a `Crawler` instance for building.\r\n2. Users that use `create_instance()` and pass `settings` but not `crawler` will need to change the logic when migrating to `build_from_crawler()`, but we think they should normally have a Crawler instance there.\r\n\r\n`build_from_crawler()` also has a wrong docstring as it doesn't mention `from_settings()`.\n",
    "hints_text": "`build_from_settings()` is also used in a test where we create a Settings instance to create a component instance with it and don't need a Crawler - this should be easy to fix.\r\n\r\nRegarding `scrapy.middleware.MiddlewareManager`: it currently has a `from_crawler()` that just calls `from_settings()` and `from_settings()` that can optionally take a Crawler instance. If the Crawler instance is passed, it is also passed to created middlewares via `build_from_crawler()` and if it isn't passed, the middlewares are created with `build_from_settings()`. So the ideal state is simple: it has a `from_crawler()` but not `from_settings()` and always passed the Crawler instance to created middleware. But as a temporary backwards-compatible state we want to keep both methods (with appropriate deprecation warnings) and be able to create middlewares without a Crawler instance, for which we should either keep (inlined?) a `build_from_settings()` function or use `create_instance()` there (which will print additional warnings). Thoughts?\r\n\r\nIn Scrapy itself `MiddlewareManager` is not used directly but only as a base class for: `scrapy.extension.ExtensionManager`, `scrapy.core.spidermw.SpiderMiddlewareManager`, `scrapy.core.downloader.middleware.DownloaderMiddlewareManager`, `scrapy.pipelines.ItemPipelineManager`. None of these override either `from_crawler()` or `from_settings()`. Instances of all of these are created via `from_crawler()`. Only `ItemPipelineManager` can be replaced (via `ITEM_PROCESSOR`) but it still needs to implement `from_crawler()` due to the previous statement. So we can safely drop the code path that doesn't take and pass a Crawler instance.\n(Additionally, if we don't have `build_from_settings()` we will be able deprecate the whole `from_settings()` interface later, though I don't know if we should)\n> we should either keep (inlined?) a `build_from_settings()` function or use `create_instance()` there (which will print additional warnings). Thoughts?\r\n\r\nI think it\u2019s best to avoid the `create_instance()` warning, be it by silencing it or by using an in-lined `build_from_settings()`, no opinion on that part. The `from_settings()` class method should emit its own, specific deprecation warning.\r\n\r\n> Additionally, if we don't have `build_from_settings()` we will be able deprecate the whole `from_settings()` interface later, though I don't know if we should\r\n\r\nUnless there is a good reason not to, it sounds consistent with deprecating `create_instance()` without adding `build_from_settings()`, so we should consider doing it for 2.12 already.",
    "created_at": "2024-11-12T16:34:43Z",
    "version": "2.11",
    "PASS_TO_PASS": [],
    "FAIL_TO_PASS": [
      "tests/test_dupefilters.py",
      "tests/test_middleware.py",
      "tests/test_pipeline_files.py",
      "tests/test_pipeline_images.py",
      "tests/test_pipeline_media.py",
      "tests/test_spidermiddleware_urllength.py",
      "tests/test_utils_misc/__init__.py",
      "tests/test_utils_request.py",
      "tests/test_webclient.py"
    ]
  },
  {
    "repo": "scrapy/scrapy",
    "pull_number": 6388,
    "instance_id": "scrapy__scrapy-6388",
    "issue_numbers": [
      "6383"
    ],
    "base_commit": "2b9e32f1ca491340148e6a1918d1df70443823e6",
    "patch": "diff --git a/scrapy/contracts/__init__.py b/scrapy/contracts/__init__.py\nindex b300b8457fc..27bc2fcbaf9 100644\n--- a/scrapy/contracts/__init__.py\n+++ b/scrapy/contracts/__init__.py\n@@ -120,7 +120,8 @@ def extract_contracts(self, method: Callable) -> List[Contract]:\n \n             if line.startswith(\"@\"):\n                 m = re.match(r\"@(\\w+)\\s*(.*)\", line)\n-                assert m is not None\n+                if m is None:\n+                    continue\n                 name, args = m.groups()\n                 args = re.split(r\"\\s+\", args)\n \n",
    "test_patch": "diff --git a/tests/test_contracts.py b/tests/test_contracts.py\nindex 1459e0b5fd5..c9c12f0d804 100644\n--- a/tests/test_contracts.py\n+++ b/tests/test_contracts.py\n@@ -182,6 +182,19 @@ def custom_form(self, response):\n         \"\"\"\n         pass\n \n+    def invalid_regex(self, response):\n+        \"\"\"method with invalid regex\n+        @ Scrapy is awsome\n+        \"\"\"\n+        pass\n+\n+    def invalid_regex_with_valid_contract(self, response):\n+        \"\"\"method with invalid regex\n+        @ scrapy is awsome\n+        @url http://scrapy.org\n+        \"\"\"\n+        pass\n+\n \n class CustomContractSuccessSpider(Spider):\n     name = \"custom_contract_success_spider\"\n@@ -385,6 +398,21 @@ def test_scrapes(self):\n         message = \"ContractFail: Missing fields: name, url\"\n         assert message in self.results.failures[-1][-1]\n \n+    def test_regex(self):\n+        spider = TestSpider()\n+        response = ResponseMock()\n+\n+        # invalid regex\n+        request = self.conman.from_method(spider.invalid_regex, self.results)\n+        self.should_succeed()\n+\n+        # invalid regex with valid contract\n+        request = self.conman.from_method(\n+            spider.invalid_regex_with_valid_contract, self.results\n+        )\n+        self.should_succeed()\n+        request.callback(response)\n+\n     def test_custom_contracts(self):\n         self.conman.from_spider(CustomContractSuccessSpider(), self.results)\n         self.should_succeed()\n",
    "problem_statement": "Error handling in contract parsing\nWe found that there is no proper handling for unmatched regexes in `\u200escrapy.contracts.ContractsManager.extract_contracts()`, so e.g. calling `from_method()` on a method with `@ foo` in the docstring produces an unhandled exception. I think we should just skip lines that don't match.\n",
    "hints_text": "@wRAR May I work on this issue?\nOf course",
    "created_at": "2024-06-03T12:47:28Z",
    "version": "2.11",
    "PASS_TO_PASS": [],
    "FAIL_TO_PASS": [
      "tests/test_contracts.py::CustomContractSuccessSpider::test_regex",
      "tests/test_contracts.py::TestSpider::invalid_regex",
      "tests/test_contracts.py::TestSpider::invalid_regex_with_valid_contract"
    ]
  },
  {
    "repo": "scrapy/scrapy",
    "pull_number": 6519,
    "instance_id": "scrapy__scrapy-6519",
    "issue_numbers": [
      "6517"
    ],
    "base_commit": "d2bdbad8c8cc5e5b4b9d3a79c94e2411a44e94be",
    "patch": "diff --git a/scrapy/utils/misc.py b/scrapy/utils/misc.py\nindex 51621834730..12c09839f0f 100644\n--- a/scrapy/utils/misc.py\n+++ b/scrapy/utils/misc.py\n@@ -111,7 +111,7 @@ def md5sum(file: IO[bytes]) -> str:\n     \"\"\"\n     warnings.warn(\n         (\n-            \"The scrapy.utils.misc.md5sum function is deprecated, and will be \"\n+            \"The scrapy.utils.misc.md5sum function is deprecated and will be \"\n             \"removed in a future version of Scrapy.\"\n         ),\n         ScrapyDeprecationWarning,\ndiff --git a/scrapy/utils/python.py b/scrapy/utils/python.py\nindex 6268af72888..d970f5da53f 100644\n--- a/scrapy/utils/python.py\n+++ b/scrapy/utils/python.py\n@@ -8,12 +8,14 @@\n import inspect\n import re\n import sys\n+import warnings\n import weakref\n from collections.abc import AsyncIterable, Iterable, Mapping\n from functools import partial, wraps\n from itertools import chain\n from typing import TYPE_CHECKING, Any, TypeVar, overload\n \n+from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.utils.asyncgen import as_async_generator\n \n if TYPE_CHECKING:\n@@ -47,6 +49,11 @@ def flatten(x: Iterable[Any]) -> list[Any]:\n     >>> flatten([\"foo\", [\"baz\", 42], \"bar\"])\n     ['foo', 'baz', 42, 'bar']\n     \"\"\"\n+    warnings.warn(\n+        \"The flatten function is deprecated and will be removed in a future version of Scrapy.\",\n+        category=ScrapyDeprecationWarning,\n+        stacklevel=2,\n+    )\n     return list(iflatten(x))\n \n \n@@ -54,6 +61,11 @@ def iflatten(x: Iterable[Any]) -> Iterable[Any]:\n     \"\"\"iflatten(sequence) -> iterator\n \n     Similar to ``.flatten()``, but returns iterator instead\"\"\"\n+    warnings.warn(\n+        \"The iflatten function is deprecated and will be removed in a future version of Scrapy.\",\n+        category=ScrapyDeprecationWarning,\n+        stacklevel=2,\n+    )\n     for el in x:\n         if is_listlike(el):\n             yield from iflatten(el)\n@@ -272,6 +284,11 @@ def equal_attributes(\n     obj1: Any, obj2: Any, attributes: list[str | Callable[[Any], Any]] | None\n ) -> bool:\n     \"\"\"Compare two objects attributes\"\"\"\n+    warnings.warn(\n+        \"The equal_attributes function is deprecated and will be removed in a future version of Scrapy.\",\n+        category=ScrapyDeprecationWarning,\n+        stacklevel=2,\n+    )\n     # not attributes given return False by default\n     if not attributes:\n         return False\ndiff --git a/scrapy/utils/request.py b/scrapy/utils/request.py\nindex 82bdcb0f94a..cd0d6de61fb 100644\n--- a/scrapy/utils/request.py\n+++ b/scrapy/utils/request.py\n@@ -140,7 +140,7 @@ def __init__(self, crawler: Crawler | None = None):\n         if implementation != \"SENTINEL\":\n             message = (\n                 \"'REQUEST_FINGERPRINTER_IMPLEMENTATION' is a deprecated setting.\\n\"\n-                \"And it will be removed in future version of Scrapy.\"\n+                \"It will be removed in a future version of Scrapy.\"\n             )\n             warnings.warn(message, category=ScrapyDeprecationWarning, stacklevel=2)\n         self._fingerprint = fingerprint\n@@ -157,6 +157,11 @@ def request_authenticate(\n     \"\"\"Authenticate the given request (in place) using the HTTP basic access\n     authentication mechanism (RFC 2617) and the given username and password\n     \"\"\"\n+    warnings.warn(\n+        \"The request_authenticate function is deprecated and will be removed in a future version of Scrapy.\",\n+        category=ScrapyDeprecationWarning,\n+        stacklevel=2,\n+    )\n     request.headers[\"Authorization\"] = basic_auth_header(username, password)\n \n \ndiff --git a/scrapy/utils/serialize.py b/scrapy/utils/serialize.py\nindex 3b4f67f000c..308e351c6fa 100644\n--- a/scrapy/utils/serialize.py\n+++ b/scrapy/utils/serialize.py\n@@ -1,11 +1,13 @@\n import datetime\n import decimal\n import json\n+import warnings\n from typing import Any\n \n from itemadapter import ItemAdapter, is_item\n from twisted.internet import defer\n \n+from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.http import Request, Response\n \n \n@@ -36,4 +38,10 @@ def default(self, o: Any) -> Any:\n \n \n class ScrapyJSONDecoder(json.JSONDecoder):\n-    pass\n+    def __init__(self, *args, **kwargs):\n+        warnings.warn(\n+            \"The ScrapyJSONDecoder class is deprecated and will be removed in a future version of Scrapy.\",\n+            category=ScrapyDeprecationWarning,\n+            stacklevel=2,\n+        )\n+        super().__init__(*args, **kwargs)\n",
    "test_patch": "diff --git a/scrapy/utils/test.py b/scrapy/utils/test.py\nindex d65f2a76d7d..92b73a91a1f 100644\n--- a/scrapy/utils/test.py\n+++ b/scrapy/utils/test.py\n@@ -6,6 +6,7 @@\n \n import asyncio\n import os\n+import warnings\n from importlib import import_module\n from pathlib import Path\n from posixpath import split\n@@ -16,6 +17,7 @@\n \n from scrapy import Spider\n from scrapy.crawler import Crawler\n+from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.utils.boto import is_botocore_available\n \n if TYPE_CHECKING:\n@@ -125,6 +127,11 @@ def assert_samelines(\n     \"\"\"Asserts text1 and text2 have the same lines, ignoring differences in\n     line endings between platforms\n     \"\"\"\n+    warnings.warn(\n+        \"The assert_samelines function is deprecated and will be removed in a future version of Scrapy.\",\n+        category=ScrapyDeprecationWarning,\n+        stacklevel=2,\n+    )\n     testcase.assertEqual(text1.splitlines(), text2.splitlines(), msg)\n \n \ndiff --git a/tests/test_utils_python.py b/tests/test_utils_python.py\nindex 5681ff9a4cc..f80f2517ac6 100644\n--- a/tests/test_utils_python.py\n+++ b/tests/test_utils_python.py\n@@ -3,6 +3,7 @@\n import platform\n import sys\n \n+import pytest\n from twisted.trial import unittest\n \n from scrapy.utils.asyncgen import as_async_generator, collect_asyncgen\n@@ -151,6 +152,7 @@ def test_real_binary_bytes(self):\n \n \n class UtilsPythonTestCase(unittest.TestCase):\n+    @pytest.mark.filterwarnings(\"ignore::scrapy.exceptions.ScrapyDeprecationWarning\")\n     def test_equal_attributes(self):\n         class Obj:\n             pass\ndiff --git a/tests/test_utils_request.py b/tests/test_utils_request.py\nindex 7156b13d0fc..965d050a4da 100644\n--- a/tests/test_utils_request.py\n+++ b/tests/test_utils_request.py\n@@ -6,6 +6,8 @@\n from hashlib import sha1\n from weakref import WeakKeyDictionary\n \n+import pytest\n+\n from scrapy.http import Request\n from scrapy.utils.python import to_bytes\n from scrapy.utils.request import (\n@@ -19,6 +21,7 @@\n \n \n class UtilsRequestTest(unittest.TestCase):\n+    @pytest.mark.filterwarnings(\"ignore::scrapy.exceptions.ScrapyDeprecationWarning\")\n     def test_request_authenticate(self):\n         r = Request(\"http://www.example.com\")\n         request_authenticate(r, \"someuser\", \"somepass\")\n",
    "problem_statement": "Deprecate unused scrapy.utils code\nAs far as I can see the following functions and classes from `scrapy.utils` are not used anywhere (usually because the code using them is gone):\r\n\r\n* `scrapy.utils.python.flatten()`\r\n* `scrapy.utils.python.iflatten()`\r\n* `scrapy.utils.python.equal_attributes()`\r\n* `scrapy.utils.request.request_authenticate()`\r\n* `scrapy.utils.serialize.ScrapyJSONDecoder`\r\n* `scrapy.utils.test.assert_samelines()`\r\n\r\n`flatten()` and `iflatten()` are also present in Parsel where they are used but I wouldn't recommend switching to those (and they are easy to reimplement anyway).\n",
    "hints_text": "I'd be happy to work on this issue. I understand that the goal is to identify and remove the unused functions and classes listed above\r\n\r\nI'll check for any remaining references across the repository to confirm that these are indeed unused and proceed with removing them.\r\n\r\nAlso, could you please let me know which specific tests I should run to verify everything is functioning correctly after these removals? \r\n\r\n\n> I understand that the goal is to identify and remove the unused functions and classes listed above\r\n\r\nThey are already identified and the goal is to deprecate them, not remove them.\nSorry for the confusion so i have to just implement warnings about the deprecation ",
    "created_at": "2024-10-30T21:14:53Z",
    "version": "2.11",
    "PASS_TO_PASS": [],
    "FAIL_TO_PASS": [
      "scrapy/utils/test.py",
      "tests/test_utils_python.py",
      "tests/test_utils_request.py"
    ]
  },
  {
    "repo": "scrapy/scrapy",
    "pull_number": 6324,
    "instance_id": "scrapy__scrapy-6324",
    "issue_numbers": [
      "6323"
    ],
    "base_commit": "a5da77d01dccbc91206d053396fb5b80e1a6b15b",
    "patch": "diff --git a/scrapy/spiders/__init__.py b/scrapy/spiders/__init__.py\nindex 72c2aaba7f5..0a653bd4155 100644\n--- a/scrapy/spiders/__init__.py\n+++ b/scrapy/spiders/__init__.py\n@@ -22,6 +22,7 @@\n \n     from scrapy.crawler import Crawler\n     from scrapy.settings import BaseSettings\n+    from scrapy.utils.log import SpiderLoggerAdapter\n \n \n class Spider(object_ref):\n@@ -42,9 +43,11 @@ def __init__(self, name: Optional[str] = None, **kwargs: Any):\n             self.start_urls: List[str] = []\n \n     @property\n-    def logger(self) -> logging.LoggerAdapter:\n+    def logger(self) -> SpiderLoggerAdapter:\n+        from scrapy.utils.log import SpiderLoggerAdapter\n+\n         logger = logging.getLogger(self.name)\n-        return logging.LoggerAdapter(logger, {\"spider\": self})\n+        return SpiderLoggerAdapter(logger, {\"spider\": self})\n \n     def log(self, message: Any, level: int = logging.DEBUG, **kw: Any) -> None:\n         \"\"\"Log the given message at the given log level\ndiff --git a/scrapy/utils/log.py b/scrapy/utils/log.py\nindex 2a38f151a16..430a91e9592 100644\n--- a/scrapy/utils/log.py\n+++ b/scrapy/utils/log.py\n@@ -4,7 +4,17 @@\n import sys\n from logging.config import dictConfig\n from types import TracebackType\n-from typing import TYPE_CHECKING, Any, List, Optional, Tuple, Type, Union, cast\n+from typing import (\n+    TYPE_CHECKING,\n+    Any,\n+    List,\n+    MutableMapping,\n+    Optional,\n+    Tuple,\n+    Type,\n+    Union,\n+    cast,\n+)\n \n from twisted.python import log as twisted_log\n from twisted.python.failure import Failure\n@@ -238,3 +248,16 @@ def logformatter_adapter(logkws: dict) -> Tuple[int, str, dict]:\n     args = logkws if not logkws.get(\"args\") else logkws[\"args\"]\n \n     return (level, message, args)\n+\n+\n+class SpiderLoggerAdapter(logging.LoggerAdapter):\n+    def process(\n+        self, msg: str, kwargs: MutableMapping[str, Any]\n+    ) -> Tuple[str, MutableMapping[str, Any]]:\n+        \"\"\"Method that augments logging with additional 'extra' data\"\"\"\n+        if isinstance(kwargs.get(\"extra\"), MutableMapping):\n+            kwargs[\"extra\"].update(self.extra)\n+        else:\n+            kwargs[\"extra\"] = self.extra\n+\n+        return msg, kwargs\n",
    "test_patch": "diff --git a/tests/spiders.py b/tests/spiders.py\nindex 94969db993d..ea419afbdac 100644\n--- a/tests/spiders.py\n+++ b/tests/spiders.py\n@@ -4,6 +4,7 @@\n \n import asyncio\n import time\n+from typing import Optional\n from urllib.parse import urlencode\n \n from twisted.internet import defer\n@@ -78,6 +79,28 @@ def errback(self, failure):\n         self.t2_err = time.time()\n \n \n+class LogSpider(MetaSpider):\n+    name = \"log_spider\"\n+\n+    def log_debug(self, message: str, extra: Optional[dict] = None):\n+        self.logger.debug(message, extra=extra)\n+\n+    def log_info(self, message: str, extra: Optional[dict] = None):\n+        self.logger.info(message, extra=extra)\n+\n+    def log_warning(self, message: str, extra: Optional[dict] = None):\n+        self.logger.warning(message, extra=extra)\n+\n+    def log_error(self, message: str, extra: Optional[dict] = None):\n+        self.logger.error(message, extra=extra)\n+\n+    def log_critical(self, message: str, extra: Optional[dict] = None):\n+        self.logger.critical(message, extra=extra)\n+\n+    def parse(self, response):\n+        pass\n+\n+\n class SlowSpider(DelaySpider):\n     name = \"slow\"\n \ndiff --git a/tests/test_utils_log.py b/tests/test_utils_log.py\nindex eae744df5e4..a8d0808222e 100644\n--- a/tests/test_utils_log.py\n+++ b/tests/test_utils_log.py\n@@ -1,18 +1,26 @@\n+import json\n import logging\n+import re\n import sys\n import unittest\n+from io import StringIO\n+from typing import Any, Dict, Mapping, MutableMapping\n+from unittest import TestCase\n \n+import pytest\n from testfixtures import LogCapture\n from twisted.python.failure import Failure\n \n from scrapy.extensions import telnet\n from scrapy.utils.log import (\n     LogCounterHandler,\n+    SpiderLoggerAdapter,\n     StreamLogger,\n     TopLevelFormatter,\n     failure_to_exc_info,\n )\n from scrapy.utils.test import get_crawler\n+from tests.spiders import LogSpider\n \n \n class FailureToExcInfoTest(unittest.TestCase):\n@@ -106,3 +114,180 @@ def test_redirect(self):\n         with LogCapture() as log:\n             print(\"test log msg\")\n         log.check((\"test\", \"ERROR\", \"test log msg\"))\n+\n+\n+@pytest.mark.parametrize(\n+    (\"base_extra\", \"log_extra\", \"expected_extra\"),\n+    (\n+        (\n+            {\"spider\": \"test\"},\n+            {\"extra\": {\"log_extra\": \"info\"}},\n+            {\"extra\": {\"log_extra\": \"info\", \"spider\": \"test\"}},\n+        ),\n+        (\n+            {\"spider\": \"test\"},\n+            {\"extra\": None},\n+            {\"extra\": {\"spider\": \"test\"}},\n+        ),\n+        (\n+            {\"spider\": \"test\"},\n+            {\"extra\": {\"spider\": \"test2\"}},\n+            {\"extra\": {\"spider\": \"test\"}},\n+        ),\n+    ),\n+)\n+def test_spider_logger_adapter_process(\n+    base_extra: Mapping[str, Any], log_extra: MutableMapping, expected_extra: Dict\n+):\n+    logger = logging.getLogger(\"test\")\n+    spider_logger_adapter = SpiderLoggerAdapter(logger, base_extra)\n+\n+    log_message = \"test_log_message\"\n+    result_message, result_kwargs = spider_logger_adapter.process(\n+        log_message, log_extra\n+    )\n+\n+    assert result_message == log_message\n+    assert result_kwargs == expected_extra\n+\n+\n+class LoggingTestCase(TestCase):\n+    def setUp(self):\n+        self.log_stream = StringIO()\n+        handler = logging.StreamHandler(self.log_stream)\n+        logger = logging.getLogger(\"log_spider\")\n+        logger.addHandler(handler)\n+        logger.setLevel(logging.DEBUG)\n+        self.handler = handler\n+        self.logger = logger\n+        self.spider = LogSpider()\n+\n+    def tearDown(self):\n+        self.logger.removeHandler(self.handler)\n+\n+    def test_debug_logging(self):\n+        log_message = \"Foo message\"\n+        self.spider.log_debug(log_message)\n+        log_contents = self.log_stream.getvalue()\n+\n+        assert log_contents == f\"{log_message}\\n\"\n+\n+    def test_info_logging(self):\n+        log_message = \"Bar message\"\n+        self.spider.log_info(log_message)\n+        log_contents = self.log_stream.getvalue()\n+\n+        assert log_contents == f\"{log_message}\\n\"\n+\n+    def test_warning_logging(self):\n+        log_message = \"Baz message\"\n+        self.spider.log_warning(log_message)\n+        log_contents = self.log_stream.getvalue()\n+\n+        assert log_contents == f\"{log_message}\\n\"\n+\n+    def test_error_logging(self):\n+        log_message = \"Foo bar message\"\n+        self.spider.log_error(log_message)\n+        log_contents = self.log_stream.getvalue()\n+\n+        assert log_contents == f\"{log_message}\\n\"\n+\n+    def test_critical_logging(self):\n+        log_message = \"Foo bar baz message\"\n+        self.spider.log_critical(log_message)\n+        log_contents = self.log_stream.getvalue()\n+\n+        assert log_contents == f\"{log_message}\\n\"\n+\n+\n+class LoggingWithExtraTestCase(TestCase):\n+    def setUp(self):\n+        self.log_stream = StringIO()\n+        handler = logging.StreamHandler(self.log_stream)\n+        formatter = logging.Formatter(\n+            '{\"levelname\": \"%(levelname)s\", \"message\": \"%(message)s\", \"spider\": \"%(spider)s\", \"important_info\": \"%(important_info)s\"}'\n+        )\n+        handler.setFormatter(formatter)\n+        logger = logging.getLogger(\"log_spider\")\n+        logger.addHandler(handler)\n+        logger.setLevel(logging.DEBUG)\n+        self.handler = handler\n+        self.logger = logger\n+        self.spider = LogSpider()\n+        self.regex_pattern = re.compile(r\"^<LogSpider\\s'log_spider'\\sat\\s[^>]+>$\")\n+\n+    def tearDown(self):\n+        self.logger.removeHandler(self.handler)\n+\n+    def test_debug_logging(self):\n+        log_message = \"Foo message\"\n+        extra = {\"important_info\": \"foo\"}\n+        self.spider.log_debug(log_message, extra)\n+        log_contents = self.log_stream.getvalue()\n+        log_contents = json.loads(log_contents)\n+\n+        assert log_contents[\"levelname\"] == \"DEBUG\"\n+        assert log_contents[\"message\"] == log_message\n+        assert self.regex_pattern.match(log_contents[\"spider\"])\n+        assert log_contents[\"important_info\"] == extra[\"important_info\"]\n+\n+    def test_info_logging(self):\n+        log_message = \"Bar message\"\n+        extra = {\"important_info\": \"bar\"}\n+        self.spider.log_info(log_message, extra)\n+        log_contents = self.log_stream.getvalue()\n+        log_contents = json.loads(log_contents)\n+\n+        assert log_contents[\"levelname\"] == \"INFO\"\n+        assert log_contents[\"message\"] == log_message\n+        assert self.regex_pattern.match(log_contents[\"spider\"])\n+        assert log_contents[\"important_info\"] == extra[\"important_info\"]\n+\n+    def test_warning_logging(self):\n+        log_message = \"Baz message\"\n+        extra = {\"important_info\": \"baz\"}\n+        self.spider.log_warning(log_message, extra)\n+        log_contents = self.log_stream.getvalue()\n+        log_contents = json.loads(log_contents)\n+\n+        assert log_contents[\"levelname\"] == \"WARNING\"\n+        assert log_contents[\"message\"] == log_message\n+        assert self.regex_pattern.match(log_contents[\"spider\"])\n+        assert log_contents[\"important_info\"] == extra[\"important_info\"]\n+\n+    def test_error_logging(self):\n+        log_message = \"Foo bar message\"\n+        extra = {\"important_info\": \"foo bar\"}\n+        self.spider.log_error(log_message, extra)\n+        log_contents = self.log_stream.getvalue()\n+        log_contents = json.loads(log_contents)\n+\n+        assert log_contents[\"levelname\"] == \"ERROR\"\n+        assert log_contents[\"message\"] == log_message\n+        assert self.regex_pattern.match(log_contents[\"spider\"])\n+        assert log_contents[\"important_info\"] == extra[\"important_info\"]\n+\n+    def test_critical_logging(self):\n+        log_message = \"Foo bar baz message\"\n+        extra = {\"important_info\": \"foo bar baz\"}\n+        self.spider.log_critical(log_message, extra)\n+        log_contents = self.log_stream.getvalue()\n+        log_contents = json.loads(log_contents)\n+\n+        assert log_contents[\"levelname\"] == \"CRITICAL\"\n+        assert log_contents[\"message\"] == log_message\n+        assert self.regex_pattern.match(log_contents[\"spider\"])\n+        assert log_contents[\"important_info\"] == extra[\"important_info\"]\n+\n+    def test_overwrite_spider_extra(self):\n+        log_message = \"Foo message\"\n+        extra = {\"important_info\": \"foo\", \"spider\": \"shouldn't change\"}\n+        self.spider.log_error(log_message, extra)\n+        log_contents = self.log_stream.getvalue()\n+        log_contents = json.loads(log_contents)\n+\n+        assert log_contents[\"levelname\"] == \"ERROR\"\n+        assert log_contents[\"message\"] == log_message\n+        assert self.regex_pattern.match(log_contents[\"spider\"])\n+        assert log_contents[\"important_info\"] == extra[\"important_info\"]\n",
    "problem_statement": "Spider.logger not logging custom extra information\nI noticed the implicit behavior of the Spider.logger: when logging with extra, extra ultimately do not end up in the log because they are overwritten by default `process` method of [LoggerAdapter](https://github.com/scrapy/scrapy/blob/master/scrapy/spiders/__init__.py#L47)\r\n\r\nCurrent logic:\r\n```py\r\n>>> self.logger.info(\"test log\", extra={\"test\": \"very important information\"})\r\n{\"message\": \"test log\", \"spider\": \"spider_name\"}\r\n```\r\n\r\n\r\n\r\nExpected logic:\r\n```py\r\n>>> self.logger.info(\"test log\", extra={\"test\": \"very important information\"})\r\n{\"message\": \"test log\", \"spider\": \"spider_name\", \"test\": \"very important information\"}\r\n```\r\n\r\n\n",
    "hints_text": "",
    "created_at": "2024-04-27T16:58:30Z",
    "version": "2.11",
    "PASS_TO_PASS": [],
    "FAIL_TO_PASS": [
      "tests/test_utils_log.py::test_spider_logger_adapter_process[base_extra0-log_extra0-expected_extra0]",
      "tests/test_utils_log.py::test_spider_logger_adapter_process[base_extra1-log_extra1-expected_extra1]",
      "tests/test_utils_log.py::test_spider_logger_adapter_process[base_extra2-log_extra2-expected_extra2]",
      "tests/test_utils_log.py::LoggingTestCase::test_critical_logging",
      "tests/test_utils_log.py::LoggingTestCase::test_debug_logging",
      "tests/test_utils_log.py::LoggingTestCase::test_error_logging",
      "tests/test_utils_log.py::LoggingTestCase::test_info_logging",
      "tests/test_utils_log.py::LoggingTestCase::test_warning_logging",
      "tests/test_utils_log.py::LoggingWithExtraTestCase::test_debug_logging",
      "tests/test_utils_log.py::LoggingWithExtraTestCase::test_info_logging",
      "tests/test_utils_log.py::LoggingWithExtraTestCase::test_warning_logging",
      "tests/test_utils_log.py::LoggingWithExtraTestCase::test_error_logging",
      "tests/test_utils_log.py::LoggingWithExtraTestCase::test_critical_logging",
      "tests/test_utils_log.py::LoggingWithExtraTestCase::test_overwrite_spider_extra"
    ]
  },
  {
    "repo": "scrapy/scrapy",
    "pull_number": 6475,
    "instance_id": "scrapy__scrapy-6475",
    "issue_numbers": [
      "6433"
    ],
    "base_commit": "67ab8d4650c1e9212c9508803c7b5265e166cbaa",
    "patch": "diff --git a/scrapy/core/engine.py b/scrapy/core/engine.py\nindex 63d84339dcd..fd9a5f7817e 100644\n--- a/scrapy/core/engine.py\n+++ b/scrapy/core/engine.py\n@@ -39,7 +39,6 @@\n from scrapy.signalmanager import SignalManager\n from scrapy.utils.log import failure_to_exc_info, logformatter_adapter\n from scrapy.utils.misc import build_from_crawler, load_object\n-from scrapy.utils.python import global_object_name\n from scrapy.utils.reactor import CallLaterOnce\n \n if TYPE_CHECKING:\n@@ -325,10 +324,6 @@ def _schedule_request(self, request: Request, spider: Spider) -> None:\n         )\n         for handler, result in request_scheduled_result:\n             if isinstance(result, Failure) and isinstance(result.value, IgnoreRequest):\n-                logger.debug(\n-                    f\"Signal handler {global_object_name(handler)} dropped \"\n-                    f\"request {request} before it reached the scheduler.\"\n-                )\n                 return\n         if not self.slot.scheduler.enqueue_request(request):  # type: ignore[union-attr]\n             self.signals.send_catch_log(\n",
    "test_patch": "diff --git a/tests/test_engine.py b/tests/test_engine.py\nindex 86526420f83..2ebc0b5e449 100644\n--- a/tests/test_engine.py\n+++ b/tests/test_engine.py\n@@ -499,7 +499,6 @@ def signal_handler(request: Request, spider: Spider) -> None:\n     assert scheduler.enqueued == [\n         keep_request\n     ], f\"{scheduler.enqueued!r} != [{keep_request!r}]\"\n-    assert \"dropped request <GET https://drop.example>\" in caplog.text\n     crawler.signals.disconnect(signal_handler, request_scheduled)\n \n \n",
    "problem_statement": "core.engine/Signal handler polluting log\n### Description\r\n\r\nThe `OffsiteMiddleware` logs a single message for each domain filtered. Great!\r\nBut then the `core.engine` logs a message for every single url filtered by the OffsiteMiddleware.\r\n(LOG_LEVEL: DEBUG)\r\n\r\nThe websites I am scraping have like 10 external links to twitter/youtube/etc in each page. For hundreds pages scrapped, the only thing I can see in the logs is `Signal handler scrapy.downloadermiddlewares.offsite.OffsiteMiddleware.request_scheduled dropped request`. \r\n\r\nI don't know if this is intended behavior. If so, it is obviously not a bug.\r\nBut nonetheless, it is very different behavior compared to previous 1.x Scrapy versions. (I don't know when it has changed and I couldn't find anything in the release notes about that.)\r\n\r\nIf not a bug, maybe we could discuss the possibility of changing this behavior so we can have logs less polluted when debugging.\r\n\r\n### Steps to Reproduce\r\n\r\n#### Just run the following spider.\r\n(url taken from another issue).\r\n\r\n```python\r\nimport scrapy\r\n\r\nclass TestSpider(scrapy.spiders.CrawlSpider):\r\n    name = 'test'\r\n    allowed_domains = ['capybala.com']\r\n    start_urls = ['https://capybala.com/']\r\n    custom_settings = {\r\n        'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',\r\n        'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',\r\n        'LOG_LEVEL': 'DEBUG'\r\n    }\r\n\r\n    rules = (scrapy.spiders.Rule(scrapy.linkextractors.LinkExtractor(), callback='parse', follow=True),)\r\n    \r\n    def parse(self, response):\r\n        print('noop')\r\n```\r\n\r\n#### Output: \r\n```txt\r\n2024-07-08 16:34:43 [scrapy.utils.log] INFO: Scrapy 2.11.2 started (bot: scrapybot)\r\n2024-07-08 16:34:43 [scrapy.utils.log] INFO: Versions: lxml 5.2.2.0, libxml2 2.12.6, cssselect 1.2.0, parsel 1.9.1, w3lib 2.2.1, Twisted 24.3.0, Python 3.12.4 (main, Jul  3 2024, 16:55:58) [GCC 11.2.0], pyOpenSSL 24.1.0 (OpenSSL 3.2.2 4 Jun 2024), cryptography 42.0.8, Platform Linux-5.15.145-x86_64-AMD_Ryzen_9_5980HX_with_Radeon_Graphics-with-glibc2.33\r\n2024-07-08 16:34:43 [scrapy.addons] INFO: Enabled addons:\r\n[]\r\n2024-07-08 16:34:43 [asyncio] DEBUG: Using selector: EpollSelector\r\n2024-07-08 16:34:43 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\r\n2024-07-08 16:34:43 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop\r\n2024-07-08 16:34:43 [scrapy.extensions.telnet] INFO: Telnet Password: d2c4cce2938fba32\r\n2024-07-08 16:34:43 [scrapy.middleware] INFO: Enabled extensions:\r\n['scrapy.extensions.corestats.CoreStats',\r\n 'scrapy.extensions.telnet.TelnetConsole',\r\n 'scrapy.extensions.memusage.MemoryUsage',\r\n 'scrapy.extensions.logstats.LogStats']\r\n2024-07-08 16:34:43 [scrapy.crawler] INFO: Overridden settings:\r\n{'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',\r\n 'SPIDER_LOADER_WARN_ONLY': True,\r\n 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}\r\n2024-07-08 16:34:43 [scrapy.middleware] INFO: Enabled downloader middlewares:\r\n['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',\r\n 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\r\n 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\r\n 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\r\n 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\r\n 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\r\n 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\r\n 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\r\n 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\r\n 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\r\n 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\r\n 'scrapy.downloadermiddlewares.stats.DownloaderStats']\r\n2024-07-08 16:34:43 [scrapy.middleware] INFO: Enabled spider middlewares:\r\n['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\r\n 'scrapy.spidermiddlewares.referer.RefererMiddleware',\r\n 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\r\n 'scrapy.spidermiddlewares.depth.DepthMiddleware']\r\n2024-07-08 16:34:43 [scrapy.middleware] INFO: Enabled item pipelines:\r\n[]\r\n2024-07-08 16:34:43 [scrapy.core.engine] INFO: Spider opened\r\n2024-07-08 16:34:43 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\r\n2024-07-08 16:34:43 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\r\n2024-07-08 16:34:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://capybala.com/> (referer: None)\r\n2024-07-08 16:34:44 [scrapy.downloadermiddlewares.offsite] DEBUG: Filtered offsite request to 'bokuran.com': <GET https://bokuran.com/>\r\n2024-07-08 16:34:44 [scrapy.core.engine] DEBUG: Signal handler scrapy.downloadermiddlewares.offsite.OffsiteMiddleware.request_scheduled dropped request <GET https://bokuran.com/> before it reached the scheduler.\r\n2024-07-08 16:34:44 [scrapy.downloadermiddlewares.offsite] DEBUG: Filtered offsite request to 'webooker.info': <GET http://webooker.info/2013/10/ebook1-release/>\r\n2024-07-08 16:34:44 [scrapy.core.engine] DEBUG: Signal handler scrapy.downloadermiddlewares.offsite.OffsiteMiddleware.request_scheduled dropped request <GET http://webooker.info/2013/10/ebook1-release/> before it reached the scheduler.\r\n2024-07-08 16:34:44 [scrapy.downloadermiddlewares.offsite] DEBUG: Filtered offsite request to 'ebook-1.com': <GET https://ebook-1.com/>\r\n2024-07-08 16:34:44 [scrapy.core.engine] DEBUG: Signal handler scrapy.downloadermiddlewares.offsite.OffsiteMiddleware.request_scheduled dropped request <GET https://ebook-1.com/> before it reached the scheduler.\r\n2024-07-08 16:34:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://capybala.com/> (referer: https://capybala.com/)\r\n2024-07-08 16:34:44 [scrapy.downloadermiddlewares.offsite] DEBUG: Filtered offsite request to 'chrome.google.com': <GET https://chrome.google.com/webstore/detail/find-ebook-edition/jhhpocdmfelpmobcnmjfppdpnbepkono>\r\n2024-07-08 16:34:44 [scrapy.core.engine] DEBUG: Signal handler scrapy.downloadermiddlewares.offsite.OffsiteMiddleware.request_scheduled dropped request <GET https://chrome.google.com/webstore/detail/find-ebook-edition/jhhpocdmfelpmobcnmjfppdpnbepkono> before it reached the scheduler.\r\n2024-07-08 16:34:44 [scrapy.downloadermiddlewares.offsite] DEBUG: Filtered offsite request to 'twitter.com': <GET https://twitter.com/orangain>\r\n2024-07-08 16:34:44 [scrapy.core.engine] DEBUG: Signal handler scrapy.downloadermiddlewares.offsite.OffsiteMiddleware.request_scheduled dropped request <GET https://twitter.com/orangain> before it reached the scheduler.\r\n2024-07-08 16:34:44 [scrapy.core.engine] DEBUG: Signal handler scrapy.downloadermiddlewares.offsite.OffsiteMiddleware.request_scheduled dropped request <GET https://twitter.com/webooker_log> before it reached the scheduler.\r\n2024-07-08 16:34:44 [scrapy.core.engine] DEBUG: Signal handler scrapy.downloadermiddlewares.offsite.OffsiteMiddleware.request_scheduled dropped request <GET http://webooker.info/> before it reached the scheduler.\r\n2024-07-08 16:34:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://capybala.com/find-kindle-edition/> (referer: https://capybala.com/)\r\n2024-07-08 16:34:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://capybala.com/bokuran/> (referer: https://capybala.com/)\r\n2024-07-08 16:34:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://capybala.com/ebook-1/> (referer: https://capybala.com/)\r\n2024-07-08 16:34:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://capybala.com/dendrogram/> (referer: https://capybala.com/)\r\nnoop\r\n2024-07-08 16:34:44 [scrapy.dupefilters] DEBUG: Filtered duplicate request: <GET https://capybala.com/> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)\r\n2024-07-08 16:34:44 [scrapy.core.engine] DEBUG: Signal handler scrapy.downloadermiddlewares.offsite.OffsiteMiddleware.request_scheduled dropped request <GET https://bokuran.com/> before it reached the scheduler.\r\n2024-07-08 16:34:44 [scrapy.core.engine] DEBUG: Signal handler scrapy.downloadermiddlewares.offsite.OffsiteMiddleware.request_scheduled dropped request <GET http://webooker.info/2013/10/ebook1-release/> before it reached the scheduler.\r\n2024-07-08 16:34:44 [scrapy.core.engine] DEBUG: Signal handler scrapy.downloadermiddlewares.offsite.OffsiteMiddleware.request_scheduled dropped request <GET https://ebook-1.com/> before it reached the scheduler.\r\n2024-07-08 16:34:44 [scrapy.core.engine] DEBUG: Signal handler scrapy.downloadermiddlewares.offsite.OffsiteMiddleware.request_scheduled dropped request <GET https://chrome.google.com/webstore/detail/find-ebook-edition/jhhpocdmfelpmobcnmjfppdpnbepkono> before it reached the scheduler.\r\n2024-07-08 16:34:44 [scrapy.core.engine] DEBUG: Signal handler scrapy.downloadermiddlewares.offsite.OffsiteMiddleware.request_scheduled dropped request <GET https://twitter.com/orangain> before it reached the scheduler.\r\n2024-07-08 16:34:44 [scrapy.core.engine] DEBUG: Signal handler scrapy.downloadermiddlewares.offsite.OffsiteMiddleware.request_scheduled dropped request <GET https://twitter.com/webooker_log> before it reached the scheduler.\r\n2024-07-08 16:34:44 [scrapy.core.engine] DEBUG: Signal handler scrapy.downloadermiddlewares.offsite.OffsiteMiddleware.request_scheduled dropped request <GET http://webooker.info/> before it reached the scheduler.\r\nnoop\r\n2024-07-08 16:34:44 [scrapy.core.engine] DEBUG: Signal handler scrapy.downloadermiddlewares.offsite.OffsiteMiddleware.request_scheduled dropped request <GET https://chrome.google.com/webstore/detail/find-ebook-edition/jhhpocdmfelpmobcnmjfppdpnbepkono> before it reached the scheduler.\r\nnoop\r\n2024-07-08 16:34:44 [scrapy.core.engine] DEBUG: Signal handler scrapy.downloadermiddlewares.offsite.OffsiteMiddleware.request_scheduled dropped request <GET https://bokuran.com/> before it reached the scheduler.\r\nnoop\r\nnoop\r\n2024-07-08 16:34:44 [scrapy.core.engine] DEBUG: Signal handler scrapy.downloadermiddlewares.offsite.OffsiteMiddleware.request_scheduled dropped request <GET http://webooker.info/2013/10/ebook1-release/> before it reached the scheduler.\r\n2024-07-08 16:34:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://tree.capybala.com/> (referer: https://capybala.com/)\r\nnoop\r\n2024-07-08 16:34:45 [scrapy.core.engine] INFO: Closing spider (finished)\r\n2024-07-08 16:34:45 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\r\n{'downloader/request_bytes': 1735,\r\n 'downloader/request_count': 7,\r\n 'downloader/request_method_count/GET': 7,\r\n 'downloader/response_bytes': 17486,\r\n 'downloader/response_count': 7,\r\n 'downloader/response_status_count/200': 7,\r\n 'dupefilter/filtered': 16,\r\n 'elapsed_time_seconds': 1.950522,\r\n 'finish_reason': 'finished',\r\n 'finish_time': datetime.datetime(2024, 7, 8, 19, 34, 45, 376469, tzinfo=datetime.timezone.utc),\r\n 'httpcompression/response_bytes': 29892,\r\n 'httpcompression/response_count': 7,\r\n 'log_count/DEBUG': 33,\r\n 'log_count/INFO': 10,\r\n 'memusage/max': 70103040,\r\n 'memusage/startup': 70103040,\r\n 'offsite/domains': 5,\r\n 'offsite/filtered': 17,\r\n 'request_depth_max': 2,\r\n 'response_received_count': 7,\r\n 'scheduler/dequeued': 7,\r\n 'scheduler/dequeued/memory': 7,\r\n 'scheduler/enqueued': 7,\r\n 'scheduler/enqueued/memory': 7,\r\n 'start_time': datetime.datetime(2024, 7, 8, 19, 34, 43, 425947, tzinfo=datetime.timezone.utc)}\r\n2024-07-08 16:34:45 [scrapy.core.engine] INFO: Spider closed (finished)\r\n\r\n```\r\n\r\n**Expected behavior:**\r\n\r\nI was not expecting to see so many `[scrapy.core.engine] DEBUG: Signal handler scrapy.downloadermiddlewares.offsite.OffsiteMiddleware.request_scheduled dropped request <GET [...]> before it reached the scheduler.` messages. I believe just the messages given by the OffsiteMiddleware are enough.\r\n\r\n**Actual behavior:**\r\n\r\nThere are **a lot** of  \"dropped request\" messages.\r\nFurthermore the same message is replicated several times if the same url is found more than one time. (e.g. https://twitter.com/orangain or https://twitter.com/webooker_log in the previous log)\r\n\r\n**Reproduces how often:** always\r\n\r\n### Versions\r\n\r\n$  scrapy version --verbose\r\nScrapy       : 2.11.2\r\nlxml         : 5.2.2.0\r\nlibxml2      : 2.12.6\r\ncssselect    : 1.2.0\r\nparsel       : 1.9.1\r\nw3lib        : 2.2.1\r\nTwisted      : 24.3.0\r\nPython       : 3.12.4 (main, Jul  3 2024, 16:55:58) [GCC 11.2.0]\r\npyOpenSSL    : 24.1.0 (OpenSSL 3.2.2 4 Jun 2024)\r\ncryptography : 42.0.8\r\nPlatform     : Linux-5.15.145-x86_64-AMD_Ryzen_9_5980HX_with_Radeon_Graphics-with-glibc2.33\r\n\r\n### Additional context\r\n\r\nI believe this has nothing to do with the `CrawlSpider`, but that is what I am using.\n",
    "hints_text": "In fact Scrapy version 2.11.1 has the previous and expected behavior.  The current behavior was introduced in Scrapy 2.11.2 by the commit f149ea4b804. Could we revert to the previous behavior?\nI\u2019m OK with removing the log message or adding a setting to silence it and silencing it by default.\nHi @Gallaecio can you assign me to this issue please? Both are working so which option do you prefer?\r\n\r\n1: simply remove the log message\r\n\r\n2: add a setting to`scrapy/settings/default_settings.py` enabling users to silence the message (silenced by default) \nWe rarely assign tickets, but feel free to give it a try!\r\n\r\n@wRAR @kmike Any preference about the approach to follow?\nI would prefer removing it.",
    "created_at": "2024-09-09T19:44:42Z",
    "version": "2.11",
    "PASS_TO_PASS": [],
    "FAIL_TO_PASS": [
      "tests/test_engine.py"
    ]
  },
  {
    "repo": "scrapy/scrapy",
    "pull_number": 5950,
    "instance_id": "scrapy__scrapy-5950",
    "issue_numbers": [
      "5992"
    ],
    "base_commit": "510574216d70ec84d75639ebcda360834a992e47",
    "patch": "diff --git a/docs/index.rst b/docs/index.rst\nindex 5404969e02e..8798aebd132 100644\n--- a/docs/index.rst\n+++ b/docs/index.rst\n@@ -222,6 +222,7 @@ Extending Scrapy\n    :hidden:\n \n    topics/architecture\n+   topics/addons\n    topics/downloader-middleware\n    topics/spider-middleware\n    topics/extensions\n@@ -235,6 +236,9 @@ Extending Scrapy\n :doc:`topics/architecture`\n     Understand the Scrapy architecture.\n \n+:doc:`topics/addons`\n+    Enable and configure third-party extensions.\n+\n :doc:`topics/downloader-middleware`\n     Customize how pages get requested and downloaded.\n \ndiff --git a/docs/topics/addons.rst b/docs/topics/addons.rst\nnew file mode 100644\nindex 00000000000..1bf2172bd40\n--- /dev/null\n+++ b/docs/topics/addons.rst\n@@ -0,0 +1,193 @@\n+.. _topics-addons:\n+\n+=======\n+Add-ons\n+=======\n+\n+Scrapy's add-on system is a framework which unifies managing and configuring\n+components that extend Scrapy's core functionality, such as middlewares,\n+extensions, or pipelines. It provides users with a plug-and-play experience in\n+Scrapy extension management, and grants extensive configuration control to\n+developers.\n+\n+\n+Activating and configuring add-ons\n+==================================\n+\n+During :class:`~scrapy.crawler.Crawler` initialization, the list of enabled\n+add-ons is read from your ``ADDONS`` setting.\n+\n+The ``ADDONS`` setting is a dict in which every key is an add-on class or its\n+import path and the value is its priority.\n+\n+This is an example where two add-ons are enabled in a project's\n+``settings.py``::\n+\n+    ADDONS = {\n+        'path.to.someaddon': 0,\n+        SomeAddonClass: 1,\n+    }\n+\n+\n+Writing your own add-ons\n+========================\n+\n+Add-ons are Python classes that include the following method:\n+\n+.. method:: update_settings(settings)\n+\n+    This method is called during the initialization of the\n+    :class:`~scrapy.crawler.Crawler`. Here, you should perform dependency checks\n+    (e.g. for external Python libraries) and update the\n+    :class:`~scrapy.settings.Settings` object as wished, e.g. enable components\n+    for this add-on or set required configuration of other extensions.\n+\n+    :param settings: The settings object storing Scrapy/component configuration\n+    :type settings: :class:`~scrapy.settings.Settings`\n+\n+They can also have the following method:\n+\n+.. classmethod:: from_crawler(cls, crawler)\n+   :noindex:\n+\n+   If present, this class method is called to create an add-on instance\n+   from a :class:`~scrapy.crawler.Crawler`. It must return a new instance\n+   of the add-on. The crawler object provides access to all Scrapy core\n+   components like settings and signals; it is a way for the add-on to access\n+   them and hook its functionality into Scrapy.\n+\n+   :param crawler: The crawler that uses this add-on\n+   :type crawler: :class:`~scrapy.crawler.Crawler`\n+\n+The settings set by the add-on should use the ``addon`` priority (see\n+:ref:`populating-settings` and :func:`scrapy.settings.BaseSettings.set`)::\n+\n+    class MyAddon:\n+        def update_settings(self, settings):\n+            settings.set(\"DNSCACHE_ENABLED\", True, \"addon\")\n+\n+This allows users to override these settings in the project or spider\n+configuration. This is not possible with settings that are mutable objects,\n+such as the dict that is a value of :setting:`ITEM_PIPELINES`. In these cases\n+you can provide an add-on-specific setting that governs whether the add-on will\n+modify :setting:`ITEM_PIPELINES`::\n+\n+    class MyAddon:\n+        def update_settings(self, settings):\n+            if settings.getbool(\"MYADDON_ENABLE_PIPELINE\"):\n+                settings[\"ITEM_PIPELINES\"][\"path.to.mypipeline\"] = 200\n+\n+If the ``update_settings`` method raises\n+:exc:`scrapy.exceptions.NotConfigured`, the add-on will be skipped. This makes\n+it easy to enable an add-on only when some conditions are met.\n+\n+Fallbacks\n+---------\n+\n+Some components provided by add-ons need to fall back to \"default\"\n+implementations, e.g. a custom download handler needs to send the request that\n+it doesn't handle via the default download handler, or a stats collector that\n+includes some additional processing but otherwise uses the default stats\n+collector. And it's possible that a project needs to use several custom\n+components of the same type, e.g. two custom download handlers that support\n+different kinds of custom requests and still need to use the default download\n+handler for other requests. To make such use cases easier to configure, we\n+recommend that such custom components should be written in the following way:\n+\n+1. The custom component (e.g. ``MyDownloadHandler``) shouldn't inherit from the\n+   default Scrapy one (e.g.\n+   ``scrapy.core.downloader.handlers.http.HTTPDownloadHandler``), but instead\n+   be able to load the class of the fallback component from a special setting\n+   (e.g. ``MY_FALLBACK_DOWNLOAD_HANDLER``), create an instance of it and use\n+   it.\n+2. The add-ons that include these components should read the current value of\n+   the default setting (e.g. ``DOWNLOAD_HANDLERS``) in their\n+   ``update_settings()`` methods, save that value into the fallback setting\n+   (``MY_FALLBACK_DOWNLOAD_HANDLER`` mentioned earlier) and set the default\n+   setting to the component provided by the add-on (e.g.\n+   ``MyDownloadHandler``). If the fallback setting is already set by the user,\n+   they shouldn't change it.\n+3. This way, if there are several add-ons that want to modify the same setting,\n+   all of them will fallback to the component from the previous one and then to\n+   the Scrapy default. The order of that depends on the priority order in the\n+   ``ADDONS`` setting.\n+\n+\n+Add-on examples\n+===============\n+\n+Set some basic configuration:\n+\n+.. code-block:: python\n+\n+    class MyAddon:\n+        def update_settings(self, settings):\n+            settings[\"ITEM_PIPELINES\"][\"path.to.mypipeline\"] = 200\n+            settings.set(\"DNSCACHE_ENABLED\", True, \"addon\")\n+\n+Check dependencies:\n+\n+.. code-block:: python\n+\n+    class MyAddon:\n+        def update_settings(self, settings):\n+            try:\n+                import boto\n+            except ImportError:\n+                raise NotConfigured(\"MyAddon requires the boto library\")\n+            ...\n+\n+Access the crawler instance:\n+\n+.. code-block:: python\n+\n+    class MyAddon:\n+        def __init__(self, crawler) -> None:\n+            super().__init__()\n+            self.crawler = crawler\n+\n+        @classmethod\n+        def from_crawler(cls, crawler):\n+            return cls(crawler)\n+\n+        def update_settings(self, settings):\n+            ...\n+\n+Use a fallback component:\n+\n+.. code-block:: python\n+\n+    from scrapy.core.downloader.handlers.http import HTTPDownloadHandler\n+\n+\n+    FALLBACK_SETTING = \"MY_FALLBACK_DOWNLOAD_HANDLER\"\n+\n+\n+    class MyHandler:\n+        lazy = False\n+\n+        def __init__(self, settings, crawler):\n+            dhcls = load_object(settings.get(FALLBACK_SETTING))\n+            self._fallback_handler = create_instance(\n+                dhcls,\n+                settings=None,\n+                crawler=crawler,\n+            )\n+\n+        def download_request(self, request, spider):\n+            if request.meta.get(\"my_params\"):\n+                # handle the request\n+                ...\n+            else:\n+                return self._fallback_handler.download_request(request, spider)\n+\n+\n+    class MyAddon:\n+        def update_settings(self, settings):\n+            if not settings.get(FALLBACK_SETTING):\n+                settings.set(\n+                    FALLBACK_SETTING,\n+                    settings.getwithbase(\"DOWNLOAD_HANDLERS\")[\"https\"],\n+                    \"addon\",\n+                )\n+            settings[\"DOWNLOAD_HANDLERS\"][\"https\"] = MyHandler\ndiff --git a/docs/topics/api.rst b/docs/topics/api.rst\nindex 26834487998..16c28405cfb 100644\n--- a/docs/topics/api.rst\n+++ b/docs/topics/api.rst\n@@ -137,6 +137,7 @@ Settings API\n         SETTINGS_PRIORITIES = {\n             \"default\": 0,\n             \"command\": 10,\n+            \"addon\": 15,\n             \"project\": 20,\n             \"spider\": 30,\n             \"cmdline\": 40,\ndiff --git a/docs/topics/settings.rst b/docs/topics/settings.rst\nindex 3e06d84f90b..602ab587d7e 100644\n--- a/docs/topics/settings.rst\n+++ b/docs/topics/settings.rst\n@@ -40,8 +40,9 @@ precedence:\n  1. Command line options (most precedence)\n  2. Settings per-spider\n  3. Project settings module\n- 4. Default settings per-command\n- 5. Default global settings (less precedence)\n+ 4. Settings set by add-ons\n+ 5. Default settings per-command\n+ 6. Default global settings (less precedence)\n \n The population of these settings sources is taken care of internally, but a\n manual handling is possible using API calls. See the\n@@ -89,7 +90,13 @@ project, it's where most of your custom settings will be populated. For a\n standard Scrapy project, this means you'll be adding or changing the settings\n in the ``settings.py`` file created for your project.\n \n-4. Default settings per-command\n+4. Settings set by add-ons\n+--------------------------\n+\n+:ref:`Add-ons <topics-addons>` can modify settings. They should do this with\n+this priority, though this is not enforced.\n+\n+5. Default settings per-command\n -------------------------------\n \n Each :doc:`Scrapy tool </topics/commands>` command can have its own default\n@@ -97,7 +104,7 @@ settings, which override the global default settings. Those custom command\n settings are specified in the ``default_settings`` attribute of the command\n class.\n \n-5. Default global settings\n+6. Default global settings\n --------------------------\n \n The global defaults are located in the ``scrapy.settings.default_settings``\n@@ -201,6 +208,16 @@ to any particular component. In that case the module of that component will be\n shown, typically an extension, middleware or pipeline. It also means that the\n component must be enabled in order for the setting to have any effect.\n \n+.. setting:: ADDONS\n+\n+ADDONS\n+------\n+\n+Default: ``{}``\n+\n+A dict containing paths to the add-ons enabled in your project and their\n+priorities. For more information, see :ref:`topics-addons`.\n+\n .. setting:: AWS_ACCESS_KEY_ID\n \n AWS_ACCESS_KEY_ID\n@@ -964,7 +981,6 @@ some of them need to be enabled through a setting.\n For more information See the :ref:`extensions user guide  <topics-extensions>`\n and the :ref:`list of available extensions <topics-extensions-ref>`.\n \n-\n .. setting:: FEED_TEMPDIR\n \n FEED_TEMPDIR\ndiff --git a/scrapy/addons.py b/scrapy/addons.py\nnew file mode 100644\nindex 00000000000..02dd4fde85b\n--- /dev/null\n+++ b/scrapy/addons.py\n@@ -0,0 +1,54 @@\n+import logging\n+from typing import TYPE_CHECKING, Any, List\n+\n+from scrapy.exceptions import NotConfigured\n+from scrapy.settings import Settings\n+from scrapy.utils.conf import build_component_list\n+from scrapy.utils.misc import create_instance, load_object\n+\n+if TYPE_CHECKING:\n+    from scrapy.crawler import Crawler\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+class AddonManager:\n+    \"\"\"This class facilitates loading and storing :ref:`topics-addons`.\"\"\"\n+\n+    def __init__(self, crawler: \"Crawler\") -> None:\n+        self.crawler: \"Crawler\" = crawler\n+        self.addons: List[Any] = []\n+\n+    def load_settings(self, settings: Settings) -> None:\n+        \"\"\"Load add-ons and configurations from a settings object.\n+\n+        This will load the add-on for every add-on path in the\n+        ``ADDONS`` setting and execute their ``update_settings`` methods.\n+\n+        :param settings: The :class:`~scrapy.settings.Settings` object from \\\n+            which to read the add-on configuration\n+        :type settings: :class:`~scrapy.settings.Settings`\n+        \"\"\"\n+        enabled: List[Any] = []\n+        for clspath in build_component_list(settings[\"ADDONS\"]):\n+            try:\n+                addoncls = load_object(clspath)\n+                addon = create_instance(\n+                    addoncls, settings=settings, crawler=self.crawler\n+                )\n+                addon.update_settings(settings)\n+                self.addons.append(addon)\n+            except NotConfigured as e:\n+                if e.args:\n+                    logger.warning(\n+                        \"Disabled %(clspath)s: %(eargs)s\",\n+                        {\"clspath\": clspath, \"eargs\": e.args[0]},\n+                        extra={\"crawler\": self.crawler},\n+                    )\n+        logger.info(\n+            \"Enabled addons:\\n%(addons)s\",\n+            {\n+                \"addons\": enabled,\n+            },\n+            extra={\"crawler\": self.crawler},\n+        )\ndiff --git a/scrapy/crawler.py b/scrapy/crawler.py\nindex 69ff07bb719..bf69cee2626 100644\n--- a/scrapy/crawler.py\n+++ b/scrapy/crawler.py\n@@ -18,6 +18,7 @@\n from zope.interface.verify import verifyClass\n \n from scrapy import Spider, signals\n+from scrapy.addons import AddonManager\n from scrapy.core.engine import ExecutionEngine\n from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.extension import ExtensionManager\n@@ -68,6 +69,9 @@ def __init__(\n         self.settings: Settings = settings.copy()\n         self.spidercls.update_settings(self.settings)\n \n+        self.addons: AddonManager = AddonManager(self)\n+        self.addons.load_settings(self.settings)\n+\n         self.signals: SignalManager = SignalManager(self)\n \n         self.stats: StatsCollector = load_object(self.settings[\"STATS_CLASS\"])(self)\ndiff --git a/scrapy/middleware.py b/scrapy/middleware.py\nindex 03e92b56506..04b838d2d11 100644\n--- a/scrapy/middleware.py\n+++ b/scrapy/middleware.py\n@@ -46,10 +46,9 @@ def from_settings(cls, settings: Settings, crawler=None):\n                 enabled.append(clspath)\n             except NotConfigured as e:\n                 if e.args:\n-                    clsname = clspath.split(\".\")[-1]\n                     logger.warning(\n-                        \"Disabled %(clsname)s: %(eargs)s\",\n-                        {\"clsname\": clsname, \"eargs\": e.args[0]},\n+                        \"Disabled %(clspath)s: %(eargs)s\",\n+                        {\"clspath\": clspath, \"eargs\": e.args[0]},\n                         extra={\"crawler\": crawler},\n                     )\n \ndiff --git a/scrapy/settings/__init__.py b/scrapy/settings/__init__.py\nindex 8b3bdbabe27..0f5cf85acc0 100644\n--- a/scrapy/settings/__init__.py\n+++ b/scrapy/settings/__init__.py\n@@ -9,6 +9,7 @@\n SETTINGS_PRIORITIES = {\n     \"default\": 0,\n     \"command\": 10,\n+    \"addon\": 15,\n     \"project\": 20,\n     \"spider\": 30,\n     \"cmdline\": 40,\ndiff --git a/scrapy/settings/default_settings.py b/scrapy/settings/default_settings.py\nindex a4cb555bd9d..ef1b7ea99b4 100644\n--- a/scrapy/settings/default_settings.py\n+++ b/scrapy/settings/default_settings.py\n@@ -17,6 +17,8 @@\n from importlib import import_module\n from pathlib import Path\n \n+ADDONS = {}\n+\n AJAXCRAWL_ENABLED = False\n \n ASYNCIO_EVENT_LOOP = None\ndiff --git a/scrapy/utils/misc.py b/scrapy/utils/misc.py\nindex 70187ba748a..b3c28da9239 100644\n--- a/scrapy/utils/misc.py\n+++ b/scrapy/utils/misc.py\n@@ -67,7 +67,7 @@ def load_object(path: Union[str, Callable]) -> Any:\n         if callable(path):\n             return path\n         raise TypeError(\n-            \"Unexpected argument type, expected string \" f\"or object, got: {type(path)}\"\n+            f\"Unexpected argument type, expected string or object, got: {type(path)}\"\n         )\n \n     try:\ndiff --git a/sep/sep-021.rst b/sep/sep-021.rst\ndeleted file mode 100644\nindex e8affa94332..00000000000\n--- a/sep/sep-021.rst\n+++ /dev/null\n@@ -1,113 +0,0 @@\n-=======  ===================\n-SEP      21\n-Title    Add-ons\n-Author   Pablo Hoffman\n-Created  2014-02-14\n-Status   Draft\n-=======  ===================\n-\n-================\n-SEP-021: Add-ons\n-================\n-\n-This proposal introduces add-ons, a unified way to manage Scrapy extensions,\n-middlewares and pipelines.\n-\n-Scrapy currently supports many hooks and mechanisms for extending its\n-functionality, but no single entry point for enabling and configuring them.\n-Instead, the hooks are spread over:\n-\n-* Spider middlewares (SPIDER_MIDDLEWARES)\n-* Downloader middlewares (DOWNLOADER_MIDDLEWARES)\n-* Downloader handlers (DOWNLOADER_HANDLERS)\n-* Item pipelines (ITEM_PIPELINES)\n-* Feed exporters and storages (FEED_EXPORTERS, FEED_STORAGES)\n-* Overridable components (DUPEFILTER_CLASS, STATS_CLASS, SCHEDULER, SPIDER_MANAGER_CLASS, ITEM_PROCESSOR, etc)\n-* Generic extensions (EXTENSIONS)\n-* CLI commands (COMMANDS_MODULE)\n-\n-One problem of this approach is that enabling an extension often requires\n-modifying many settings, often in a coordinated way, which is complex and error\n-prone. Add-ons are meant to fix this by providing a simple mechanism for\n-enabling extensions.\n-\n-Design goals and non-goals\n-==========================\n-\n-Goals:\n-\n-* simple to manage: adding or removing extensions should be just a matter of\n-  adding or removing lines in a ``scrapy.cfg`` file\n-* backward compatibility with enabling extension the \"old way\" (i.e. modifying\n-  settings directly)\n-\n-Non-goals:\n-\n-* a way to publish, distribute or discover extensions (use pypi for that)\n-\n-\n-Managing add-ons\n-================\n-\n-Add-ons are defined in the ``scrapy.cfg`` file, inside the ``[addons]``\n-section.\n-\n-To enable the \"httpcache\" addon, either shipped with Scrapy or in the Python\n-search path, create an entry for it in your ``scrapy.cfg``, like this::\n-\n-    [addons]\n-    httpcache = \n-\n-You may also specify the full path to an add-on (which may be either a .py file\n-or a folder containing __init__.py)::\n-\n-    [addons]\n-    mongodb_pipeline = /path/to/mongodb_pipeline.py\n-\n-\n-Writing add-ons\n-===============\n-\n-Add-ons are Python modules that implement the following callbacks.\n-\n-addon_configure\n----------------\n-\n-Receives the Settings object and modifies it to enable the required components.\n-If it raises an exception, Scrapy will print it and exit.\n-\n-Examples:\n-\n-.. code-block:: python\n-\n-    def addon_configure(settings):\n-        settings.overrides[\"DOWNLOADER_MIDDLEWARES\"].update(\n-            {\n-                \"scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware\": 900,\n-            }\n-        )\n-\n-.. code-block:: python\n-\n-    def addon_configure(settings):\n-        try:\n-            import boto\n-        except ImportError:\n-            raise RuntimeError(\"boto library is required\")\n-\n-\n-crawler_ready\n--------------\n-\n-``crawler_ready`` receives a Crawler object after it has been initialized and\n-is meant to be used to perform post-initialization checks like making sure the\n-extension and its dependencies were configured properly. If it raises an\n-exception, Scrapy will print and exit.\n-\n-Examples:\n-\n-.. code-block:: python\n-\n-    def crawler_ready(crawler):\n-        if \"some.other.addon\" not in crawler.extensions.enabled:\n-            raise RuntimeError(\"Some other addon is required to use this addon\")\n",
    "test_patch": "diff --git a/tests/test_addons.py b/tests/test_addons.py\nnew file mode 100644\nindex 00000000000..5d053ed52d9\n--- /dev/null\n+++ b/tests/test_addons.py\n@@ -0,0 +1,158 @@\n+import itertools\n+import unittest\n+from typing import Any, Dict\n+\n+from scrapy import Spider\n+from scrapy.crawler import Crawler, CrawlerRunner\n+from scrapy.exceptions import NotConfigured\n+from scrapy.settings import BaseSettings, Settings\n+from scrapy.utils.test import get_crawler\n+\n+\n+class SimpleAddon:\n+    def update_settings(self, settings):\n+        pass\n+\n+\n+def get_addon_cls(config: Dict[str, Any]) -> type:\n+    class AddonWithConfig:\n+        def update_settings(self, settings: BaseSettings):\n+            settings.update(config, priority=\"addon\")\n+\n+    return AddonWithConfig\n+\n+\n+class CreateInstanceAddon:\n+    def __init__(self, crawler: Crawler) -> None:\n+        super().__init__()\n+        self.crawler = crawler\n+        self.config = crawler.settings.getdict(\"MYADDON\")\n+\n+    @classmethod\n+    def from_crawler(cls, crawler: Crawler):\n+        return cls(crawler)\n+\n+    def update_settings(self, settings):\n+        settings.update(self.config, \"addon\")\n+\n+\n+class AddonTest(unittest.TestCase):\n+    def test_update_settings(self):\n+        settings = BaseSettings()\n+        settings.set(\"KEY1\", \"default\", priority=\"default\")\n+        settings.set(\"KEY2\", \"project\", priority=\"project\")\n+        addon_config = {\"KEY1\": \"addon\", \"KEY2\": \"addon\", \"KEY3\": \"addon\"}\n+        testaddon = get_addon_cls(addon_config)()\n+        testaddon.update_settings(settings)\n+        self.assertEqual(settings[\"KEY1\"], \"addon\")\n+        self.assertEqual(settings[\"KEY2\"], \"project\")\n+        self.assertEqual(settings[\"KEY3\"], \"addon\")\n+\n+\n+class AddonManagerTest(unittest.TestCase):\n+    def test_load_settings(self):\n+        settings_dict = {\n+            \"ADDONS\": {\"tests.test_addons.SimpleAddon\": 0},\n+        }\n+        crawler = get_crawler(settings_dict=settings_dict)\n+        manager = crawler.addons\n+        self.assertIsInstance(manager.addons[0], SimpleAddon)\n+\n+    def test_notconfigured(self):\n+        class NotConfiguredAddon:\n+            def update_settings(self, settings):\n+                raise NotConfigured()\n+\n+        settings_dict = {\n+            \"ADDONS\": {NotConfiguredAddon: 0},\n+        }\n+        crawler = get_crawler(settings_dict=settings_dict)\n+        manager = crawler.addons\n+        self.assertFalse(manager.addons)\n+\n+    def test_load_settings_order(self):\n+        # Get three addons with different settings\n+        addonlist = []\n+        for i in range(3):\n+            addon = get_addon_cls({\"KEY1\": i})\n+            addon.number = i\n+            addonlist.append(addon)\n+        # Test for every possible ordering\n+        for ordered_addons in itertools.permutations(addonlist):\n+            expected_order = [a.number for a in ordered_addons]\n+            settings = {\"ADDONS\": {a: i for i, a in enumerate(ordered_addons)}}\n+            crawler = get_crawler(settings_dict=settings)\n+            manager = crawler.addons\n+            self.assertEqual([a.number for a in manager.addons], expected_order)\n+            self.assertEqual(crawler.settings.getint(\"KEY1\"), expected_order[-1])\n+\n+    def test_create_instance(self):\n+        settings_dict = {\n+            \"ADDONS\": {\"tests.test_addons.CreateInstanceAddon\": 0},\n+            \"MYADDON\": {\"MYADDON_KEY\": \"val\"},\n+        }\n+        crawler = get_crawler(settings_dict=settings_dict)\n+        manager = crawler.addons\n+        self.assertIsInstance(manager.addons[0], CreateInstanceAddon)\n+        self.assertEqual(crawler.settings.get(\"MYADDON_KEY\"), \"val\")\n+\n+    def test_settings_priority(self):\n+        config = {\n+            \"KEY\": 15,  # priority=addon\n+        }\n+        settings_dict = {\n+            \"ADDONS\": {get_addon_cls(config): 1},\n+        }\n+        crawler = get_crawler(settings_dict=settings_dict)\n+        self.assertEqual(crawler.settings.getint(\"KEY\"), 15)\n+\n+        settings = Settings(settings_dict)\n+        settings.set(\"KEY\", 0, priority=\"default\")\n+        runner = CrawlerRunner(settings)\n+        crawler = runner.create_crawler(Spider)\n+        self.assertEqual(crawler.settings.getint(\"KEY\"), 15)\n+\n+        settings_dict = {\n+            \"KEY\": 20,  # priority=project\n+            \"ADDONS\": {get_addon_cls(config): 1},\n+        }\n+        settings = Settings(settings_dict)\n+        settings.set(\"KEY\", 0, priority=\"default\")\n+        runner = CrawlerRunner(settings)\n+        crawler = runner.create_crawler(Spider)\n+        self.assertEqual(crawler.settings.getint(\"KEY\"), 20)\n+\n+    def test_fallback_workflow(self):\n+        FALLBACK_SETTING = \"MY_FALLBACK_DOWNLOAD_HANDLER\"\n+\n+        class AddonWithFallback:\n+            def update_settings(self, settings):\n+                if not settings.get(FALLBACK_SETTING):\n+                    settings.set(\n+                        FALLBACK_SETTING,\n+                        settings.getwithbase(\"DOWNLOAD_HANDLERS\")[\"https\"],\n+                        \"addon\",\n+                    )\n+                settings[\"DOWNLOAD_HANDLERS\"][\"https\"] = \"AddonHandler\"\n+\n+        settings_dict = {\n+            \"ADDONS\": {AddonWithFallback: 1},\n+        }\n+        crawler = get_crawler(settings_dict=settings_dict)\n+        self.assertEqual(\n+            crawler.settings.getwithbase(\"DOWNLOAD_HANDLERS\")[\"https\"], \"AddonHandler\"\n+        )\n+        self.assertEqual(\n+            crawler.settings.get(FALLBACK_SETTING),\n+            \"scrapy.core.downloader.handlers.http.HTTPDownloadHandler\",\n+        )\n+\n+        settings_dict = {\n+            \"ADDONS\": {AddonWithFallback: 1},\n+            \"DOWNLOAD_HANDLERS\": {\"https\": \"UserHandler\"},\n+        }\n+        crawler = get_crawler(settings_dict=settings_dict)\n+        self.assertEqual(\n+            crawler.settings.getwithbase(\"DOWNLOAD_HANDLERS\")[\"https\"], \"AddonHandler\"\n+        )\n+        self.assertEqual(crawler.settings.get(FALLBACK_SETTING), \"UserHandler\")\ndiff --git a/tests/test_middleware.py b/tests/test_middleware.py\nindex 00ff746ee5a..a42c7b3d1e2 100644\n--- a/tests/test_middleware.py\n+++ b/tests/test_middleware.py\n@@ -39,7 +39,7 @@ def close_spider(self, spider):\n         pass\n \n     def __init__(self):\n-        raise NotConfigured\n+        raise NotConfigured(\"foo\")\n \n \n class TestMiddlewareManager(MiddlewareManager):\ndiff --git a/tests/test_utils_deprecate.py b/tests/test_utils_deprecate.py\nindex 2d9210410d4..eedb6f6af9c 100644\n--- a/tests/test_utils_deprecate.py\n+++ b/tests/test_utils_deprecate.py\n@@ -296,3 +296,7 @@ def test_unmatched_path_stays_the_same(self):\n             output = update_classpath(\"scrapy.unmatched.Path\")\n         self.assertEqual(output, \"scrapy.unmatched.Path\")\n         self.assertEqual(len(w), 0)\n+\n+    def test_returns_nonstring(self):\n+        for notastring in [None, True, [1, 2, 3], object()]:\n+            self.assertEqual(update_classpath(notastring), notastring)\n",
    "problem_statement": "NotConfigured logging breaks when the component is added by class object\nAs the log message for components that raise `NotConfigured` with a message assumes `clsname` is an import path string, it raises an AttributeError when it's a class instance. https://github.com/scrapy/scrapy/blob/bddbbc522aef00dc150e479e6288041cee2e95c9/scrapy/middleware.py#L49\n",
    "hints_text": "",
    "created_at": "2023-06-14T14:17:09Z",
    "version": "2.9",
    "PASS_TO_PASS": [],
    "FAIL_TO_PASS": [
      "tests/test_addons.py::AddonTest::test_update_settings",
      "tests/test_addons.py::AddonManagerTest::test_create_instance",
      "tests/test_addons.py::AddonManagerTest::test_fallback_workflow",
      "tests/test_addons.py::AddonManagerTest::test_load_settings",
      "tests/test_addons.py::AddonManagerTest::test_load_settings_order",
      "tests/test_addons.py::AddonManagerTest::test_notconfigured",
      "tests/test_addons.py::AddonManagerTest::test_settings_priority",
      "tests/test_utils_deprecate.py::UpdateClassPathTest::test_returns_nonstring"
    ]
  },
  {
    "repo": "scrapy/scrapy",
    "pull_number": 6608,
    "instance_id": "scrapy__scrapy-6608",
    "issue_numbers": [
      "6603"
    ],
    "base_commit": "b6d69e389576c137a33d14c9e9319891dce68442",
    "patch": "diff --git a/docs/topics/settings.rst b/docs/topics/settings.rst\nindex 76904a26ef0..ea12a7f8447 100644\n--- a/docs/topics/settings.rst\n+++ b/docs/topics/settings.rst\n@@ -418,6 +418,38 @@ This setting also affects :setting:`DOWNLOAD_DELAY` and\n :ref:`topics-autothrottle`: if :setting:`CONCURRENT_REQUESTS_PER_IP`\n is non-zero, download delay is enforced per IP, not per domain.\n \n+.. setting:: DEFAULT_DROPITEM_LOG_LEVEL\n+\n+DEFAULT_DROPITEM_LOG_LEVEL\n+--------------------------\n+\n+Default: ``\"WARNING\"``\n+\n+Default :ref:`log level <levels>` of messages about dropped items.\n+\n+When an item is dropped by raising :exc:`scrapy.exceptions.DropItem` from the\n+:func:`process_item` method of an :ref:`item pipeline <topics-item-pipeline>`,\n+a message is logged, and by default its log level is the one configured in this\n+setting.\n+\n+You may specify this log level as an integer (e.g. ``20``), as a log level\n+constant (e.g. ``logging.INFO``) or as a string with the name of a log level\n+constant (e.g. ``\"INFO\"``).\n+\n+When writing an item pipeline, you can force a different log level by setting\n+:attr:`scrapy.exceptions.DropItem.log_level` in your\n+:exc:`scrapy.exceptions.DropItem` exception. For example:\n+\n+.. code-block:: python\n+\n+   from scrapy.exceptions import DropItem\n+\n+\n+   class MyPipeline:\n+       def process_item(self, item, spider):\n+           if not item.get(\"price\"):\n+               raise DropItem(\"Missing price data\", log_level=\"INFO\")\n+           return item\n \n .. setting:: DEFAULT_ITEM_CLASS\n \ndiff --git a/scrapy/exceptions.py b/scrapy/exceptions.py\nindex 96566ba864f..f37f881a7da 100644\n--- a/scrapy/exceptions.py\n+++ b/scrapy/exceptions.py\n@@ -5,6 +5,8 @@\n new exceptions here without documenting them there.\n \"\"\"\n \n+from __future__ import annotations\n+\n from typing import Any\n \n # Internal\n@@ -58,6 +60,10 @@ def __init__(self, *, fail: bool = True):\n class DropItem(Exception):\n     \"\"\"Drop item from the item pipeline\"\"\"\n \n+    def __init__(self, message: str, log_level: str | None = None):\n+        super().__init__(message)\n+        self.log_level = log_level\n+\n \n class NotSupported(Exception):\n     \"\"\"Indicates a feature or method is not supported\"\"\"\ndiff --git a/scrapy/logformatter.py b/scrapy/logformatter.py\nindex 544f4adfe42..eff81d28fc6 100644\n--- a/scrapy/logformatter.py\n+++ b/scrapy/logformatter.py\n@@ -120,8 +120,12 @@ def dropped(\n         spider: Spider,\n     ) -> LogFormatterResult:\n         \"\"\"Logs a message when an item is dropped while it is passing through the item pipeline.\"\"\"\n+        if (level := getattr(exception, \"log_level\", None)) is None:\n+            level = spider.crawler.settings[\"DEFAULT_DROPITEM_LOG_LEVEL\"]\n+        if isinstance(level, str):\n+            level = getattr(logging, level)\n         return {\n-            \"level\": logging.WARNING,\n+            \"level\": level,\n             \"msg\": DROPPEDMSG,\n             \"args\": {\n                 \"exception\": exception,\ndiff --git a/scrapy/settings/default_settings.py b/scrapy/settings/default_settings.py\nindex 0bbde118e95..7ef365f686d 100644\n--- a/scrapy/settings/default_settings.py\n+++ b/scrapy/settings/default_settings.py\n@@ -49,6 +49,8 @@\n COOKIES_ENABLED = True\n COOKIES_DEBUG = False\n \n+DEFAULT_DROPITEM_LOG_LEVEL = \"WARNING\"\n+\n DEFAULT_ITEM_CLASS = \"scrapy.item.Item\"\n \n DEFAULT_REQUEST_HEADERS = {\n",
    "test_patch": "diff --git a/tests/test_logformatter.py b/tests/test_logformatter.py\nindex 5a92521cc3f..0d1e82f0b5a 100644\n--- a/tests/test_logformatter.py\n+++ b/tests/test_logformatter.py\n@@ -1,5 +1,7 @@\n+import logging\n import unittest\n \n+import pytest\n from testfixtures import LogCapture\n from twisted.internet import defer\n from twisted.python.failure import Failure\n@@ -26,6 +28,7 @@ class LogFormatterTestCase(unittest.TestCase):\n     def setUp(self):\n         self.formatter = LogFormatter()\n         self.spider = Spider(\"default\")\n+        self.spider.crawler = get_crawler()\n \n     def test_crawled_with_referer(self):\n         req = Request(\"http://www.example.com\")\n@@ -68,6 +71,62 @@ def test_dropped(self):\n         assert all(isinstance(x, str) for x in lines)\n         self.assertEqual(lines, [\"Dropped: \\u2018\", \"{}\"])\n \n+    def test_dropitem_default_log_level(self):\n+        item = {}\n+        exception = DropItem(\"Test drop\")\n+        response = Response(\"http://www.example.com\")\n+        spider = Spider(\"foo\")\n+        spider.crawler = get_crawler(Spider)\n+\n+        logkws = self.formatter.dropped(item, exception, response, spider)\n+        self.assertEqual(logkws[\"level\"], logging.WARNING)\n+\n+        spider.crawler.settings.frozen = False\n+        spider.crawler.settings[\"DEFAULT_DROPITEM_LOG_LEVEL\"] = logging.INFO\n+        spider.crawler.settings.frozen = True\n+        logkws = self.formatter.dropped(item, exception, response, spider)\n+        self.assertEqual(logkws[\"level\"], logging.INFO)\n+\n+        spider.crawler.settings.frozen = False\n+        spider.crawler.settings[\"DEFAULT_DROPITEM_LOG_LEVEL\"] = \"INFO\"\n+        spider.crawler.settings.frozen = True\n+        logkws = self.formatter.dropped(item, exception, response, spider)\n+        self.assertEqual(logkws[\"level\"], logging.INFO)\n+\n+        spider.crawler.settings.frozen = False\n+        spider.crawler.settings[\"DEFAULT_DROPITEM_LOG_LEVEL\"] = 10\n+        spider.crawler.settings.frozen = True\n+        logkws = self.formatter.dropped(item, exception, response, spider)\n+        self.assertEqual(logkws[\"level\"], logging.DEBUG)\n+\n+        spider.crawler.settings.frozen = False\n+        spider.crawler.settings[\"DEFAULT_DROPITEM_LOG_LEVEL\"] = 0\n+        spider.crawler.settings.frozen = True\n+        logkws = self.formatter.dropped(item, exception, response, spider)\n+        self.assertEqual(logkws[\"level\"], logging.NOTSET)\n+\n+        unsupported_value = object()\n+        spider.crawler.settings.frozen = False\n+        spider.crawler.settings[\"DEFAULT_DROPITEM_LOG_LEVEL\"] = unsupported_value\n+        spider.crawler.settings.frozen = True\n+        logkws = self.formatter.dropped(item, exception, response, spider)\n+        self.assertEqual(logkws[\"level\"], unsupported_value)\n+\n+        with pytest.raises(TypeError):\n+            logging.log(logkws[\"level\"], \"message\")\n+\n+    def test_dropitem_custom_log_level(self):\n+        item = {}\n+        response = Response(\"http://www.example.com\")\n+\n+        exception = DropItem(\"Test drop\", log_level=\"INFO\")\n+        logkws = self.formatter.dropped(item, exception, response, self.spider)\n+        self.assertEqual(logkws[\"level\"], logging.INFO)\n+\n+        exception = DropItem(\"Test drop\", log_level=\"ERROR\")\n+        logkws = self.formatter.dropped(item, exception, response, self.spider)\n+        self.assertEqual(logkws[\"level\"], logging.ERROR)\n+\n     def test_item_error(self):\n         # In practice, the complete traceback is shown by passing the\n         # 'exc_info' argument to the logging function\n@@ -145,6 +204,7 @@ class LogformatterSubclassTest(LogFormatterTestCase):\n     def setUp(self):\n         self.formatter = LogFormatterSubclass()\n         self.spider = Spider(\"default\")\n+        self.spider.crawler = get_crawler(Spider)\n \n     def test_crawled_with_referer(self):\n         req = Request(\"http://www.example.com\")\n",
    "problem_statement": "Allow changing the log severity of DropItem\nBy default, when an item is dropped, a `WARNING` message is logged. [It would be nice to make that more flexible](https://github.com/zytedata/zyte-common-items/pull/126#discussion_r1900692613).\n",
    "hints_text": "",
    "created_at": "2025-01-05T20:32:00Z",
    "version": "2.12",
    "PASS_TO_PASS": [],
    "FAIL_TO_PASS": [
      "tests/test_logformatter.py::LogFormatterTestCase::test_dropitem_default_log_level",
      "tests/test_logformatter.py::LogFormatterTestCase::test_dropitem_custom_log_level"
    ]
  },
  {
    "repo": "scrapy/scrapy",
    "pull_number": 6606,
    "instance_id": "scrapy__scrapy-6606",
    "issue_numbers": [
      "6600"
    ],
    "base_commit": "98ba61256deceba7b04b938a97005258f4ef5c66",
    "patch": "diff --git a/scrapy/cmdline.py b/scrapy/cmdline.py\nindex 065adccfb29..b08fd34095c 100644\n--- a/scrapy/cmdline.py\n+++ b/scrapy/cmdline.py\n@@ -96,10 +96,9 @@ def _get_project_only_cmds(settings: BaseSettings) -> set[str]:\n \n \n def _pop_command_name(argv: list[str]) -> str | None:\n-    for i, arg in enumerate(argv[1:]):\n-        if not arg.startswith(\"-\"):\n-            del argv[i]\n-            return arg\n+    for i in range(1, len(argv)):\n+        if not argv[i].startswith(\"-\"):\n+            return argv.pop(i)\n     return None\n \n \n",
    "test_patch": "diff --git a/tests/test_commands.py b/tests/test_commands.py\nindex 1aae3222e5c..50f09304333 100644\n--- a/tests/test_commands.py\n+++ b/tests/test_commands.py\n@@ -23,7 +23,7 @@\n from twisted.trial import unittest\n \n import scrapy\n-from scrapy.cmdline import _print_unknown_command_msg\n+from scrapy.cmdline import _pop_command_name, _print_unknown_command_msg\n from scrapy.commands import ScrapyCommand, ScrapyHelpFormatter, view\n from scrapy.commands.startproject import IGNORE\n from scrapy.settings import Settings\n@@ -1163,3 +1163,29 @@ def test_help_messages(self):\n         for command in self.commands:\n             _, out, _ = self.proc(command, \"-h\")\n             self.assertIn(\"Usage\", out)\n+\n+\n+class PopCommandNameTest(unittest.TestCase):\n+    def test_valid_command(self):\n+        argv = [\"scrapy\", \"crawl\", \"my_spider\"]\n+        command = _pop_command_name(argv)\n+        self.assertEqual(command, \"crawl\")\n+        self.assertEqual(argv, [\"scrapy\", \"my_spider\"])\n+\n+    def test_no_command(self):\n+        argv = [\"scrapy\"]\n+        command = _pop_command_name(argv)\n+        self.assertIsNone(command)\n+        self.assertEqual(argv, [\"scrapy\"])\n+\n+    def test_option_before_command(self):\n+        argv = [\"scrapy\", \"-h\", \"crawl\"]\n+        command = _pop_command_name(argv)\n+        self.assertEqual(command, \"crawl\")\n+        self.assertEqual(argv, [\"scrapy\", \"-h\"])\n+\n+    def test_option_after_command(self):\n+        argv = [\"scrapy\", \"crawl\", \"-h\"]\n+        command = _pop_command_name(argv)\n+        self.assertEqual(command, \"crawl\")\n+        self.assertEqual(argv, [\"scrapy\", \"-h\"])\n",
    "problem_statement": "Investigate off-by-1 in `scrapy.cmdline._pop_command_name()`\nIt looks like `del argv[i]` removes the wrong item in `scrapy.cmdline._pop_command_name()` but as we don't seem to see any problems because of this it's worth investigating what exactly happens here and either fixing or refactoring the code.\n",
    "hints_text": "Hi. I looked into `scrapy.cmdline._pop_command_name()` and the goal is to extract the command name from `argv `list. The `i` variable in the function is used to track the position in the original `argv `list and it's value is 0. However, since `argv[1:]` is a slice of `argv`, it starts at index 0 of the slice, not the original list, so when `del argv[i]` is executed, it deletes the wrong element because `i` does not align with the original list's index. I created a sample scrapy project [using this tutorial](https://docs.scrapy.org/en/master/intro/tutorial.html) and ran the command `scrapy crawl quotes`. I can see that the value of argv at the start is a list `['C:\\\\Users\\\\Hp\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\Scripts\\\\scrapy', 'crawl', 'quotes']`. It is expected that the `'crawl'` should be removed from `argv` and but after the for loop executes, `scrapy` is removed from the list and the value of `argv` is `['crawl', 'quotes']`. Screenshot below for reference.\r\n![image](https://github.com/user-attachments/assets/084f3f48-9d59-431b-bc90-1eaff5ec1fc9)\r\n\r\nAs the `for `loop iterates over `argv[1:]`, the value of `arg` is `'crawl' `and the `return arg` line returns `'crawl'`. So even though the wrong value was removed from `argv` due to incorrect index being used, the value returned by `_pop_command_name()` is the expected value.\r\n\r\nTo fix the issue, we need to ensure that the index `i` refers to the correct position in the original `argv` list. I can work on refactoring the code. However, I noticed that the function implementation for `scrapy.cmdline._pop_command_name()` in the master branch is different from what I see from the installed version of scrapy v2.12.0. I see there is a branch 2.12. For which branch should the PR be submitted?\nThanks for the analysis. As `argv` is modified in-place and used after `_pop_command_name()` is called (`parser.parse_known_args(args=argv[1:])`, I wonder why it works even though `argv` is incorrect. I guess `parse_known_args()` only cares about dash-prefixed argv entries, but it's possible that a dash-prefixed one will be deleted because of off-by-1 and then the behavior will be unexpected. It would be nice to find such cases. `scrapy -h crawl` doesn't work while `scrapy crawl -h` does, which may be the only way to break this, and `scrapy -h` says `scrapy <command> [options] [args]` which suggests the options can't go before the command name. If it's indeed the only way to break this, and the fix fixes such commands, it would still be actually useful.\r\n\r\n> For which branch should the PR be submitted?\r\n\r\nAlways for master.\n> it's possible that a dash-prefixed one will be deleted because of off-by-1 and then the behavior will be unexpected.\r\n\r\nThat's correct. If I use the command `scrapy crawl quote`, then the `parser.parse_known_args(args=argv[1:])` works even if `argv` is incorrect because the value passed to `parse_known_args` is `argv[1:]` which will always be `'quote'`, irrespective of whether `scrapy` or `crawl` is popped from the original arg list, and since `_pop_command_name` unintentionally returns the correct command, the program runs without an error. Same goes for `scrapy crawl -h`. However, if I try the command `scrapy -h crawl`, then `-h` gets deleted from the arg list and the value returned by `_pop_command_name` is `'crawl'`, which instructs the program to run the crawl command. As the line `parser.parse_known_args(args=argv[1:])` is executed, the value of `argv[1:]` also remains `'crawl'` and since there is no spider with the name 'crawl' in my scrapy project, the program exits with an error. Will work on fixing it and submit a PR today.\r\n\r\nI have a question unrelated to the problem. I would like to understand why the implementation of scrapy.cmdline._pop_command_name() from the scrapy package I installed from pip is different from the master branch. I thought that master branch contains the latest changes that should also be present in the latest package installed from pip.\n> I would like to understand why the implementation of scrapy.cmdline._pop_command_name() from the scrapy package I installed from pip is different from the master branch. \r\n\r\nBecause it was changed in master after 2.12.0 was released.\r\n\r\n> I thought that master branch contains the latest changes that should also be present in the latest package installed from pip.\r\n\r\nWell no, it contains changes that were not released yet.",
    "created_at": "2025-01-04T10:41:31Z",
    "version": "2.12",
    "PASS_TO_PASS": [],
    "FAIL_TO_PASS": [
      "tests/test_commands.py::PopCommandNameTest::test_valid_command",
      "tests/test_commands.py::PopCommandNameTest::test_no_command",
      "tests/test_commands.py::PopCommandNameTest::test_option_before_command",
      "tests/test_commands.py::PopCommandNameTest::test_option_after_command"
    ]
  },
  {
    "repo": "scrapy/scrapy",
    "pull_number": 6050,
    "instance_id": "scrapy__scrapy-6050",
    "issue_numbers": [
      "6049"
    ],
    "base_commit": "dba37674e6eaa6c2030c8eb35ebf8127cd488062",
    "patch": "diff --git a/scrapy/downloadermiddlewares/retry.py b/scrapy/downloadermiddlewares/retry.py\nindex 205bb48b103..380623cea3e 100644\n--- a/scrapy/downloadermiddlewares/retry.py\n+++ b/scrapy/downloadermiddlewares/retry.py\n@@ -24,9 +24,8 @@\n retry_logger = getLogger(__name__)\n \n \n-class BackwardsCompatibilityMetaclass(type):\n-    @property\n-    def EXCEPTIONS_TO_RETRY(cls):\n+def backwards_compatibility_getattr(self, name):\n+    if name == \"EXCEPTIONS_TO_RETRY\":\n         warnings.warn(\n             \"Attribute RetryMiddleware.EXCEPTIONS_TO_RETRY is deprecated. \"\n             \"Use the RETRY_EXCEPTIONS setting instead.\",\n@@ -37,6 +36,13 @@ def EXCEPTIONS_TO_RETRY(cls):\n             load_object(x) if isinstance(x, str) else x\n             for x in Settings().getlist(\"RETRY_EXCEPTIONS\")\n         )\n+    raise AttributeError(\n+        f\"{self.__class__.__name__!r} object has no attribute {name!r}\"\n+    )\n+\n+\n+class BackwardsCompatibilityMetaclass(type):\n+    __getattr__ = backwards_compatibility_getattr\n \n \n def get_retry_request(\n@@ -137,15 +143,14 @@ def __init__(self, settings):\n         )\n         self.priority_adjust = settings.getint(\"RETRY_PRIORITY_ADJUST\")\n \n-        if not hasattr(\n-            self, \"EXCEPTIONS_TO_RETRY\"\n-        ):  # If EXCEPTIONS_TO_RETRY is not \"overriden\"\n+        try:\n+            self.exceptions_to_retry = self.__getattribute__(\"EXCEPTIONS_TO_RETRY\")\n+        except AttributeError:\n+            # If EXCEPTIONS_TO_RETRY is not \"overridden\"\n             self.exceptions_to_retry = tuple(\n                 load_object(x) if isinstance(x, str) else x\n                 for x in settings.getlist(\"RETRY_EXCEPTIONS\")\n             )\n-        else:\n-            self.exceptions_to_retry = self.EXCEPTIONS_TO_RETRY\n \n     @classmethod\n     def from_crawler(cls, crawler):\n@@ -175,3 +180,5 @@ def _retry(self, request, reason, spider):\n             max_retry_times=max_retry_times,\n             priority_adjust=priority_adjust,\n         )\n+\n+    __getattr__ = backwards_compatibility_getattr\n",
    "test_patch": "diff --git a/tests/test_downloadermiddleware_retry.py b/tests/test_downloadermiddleware_retry.py\nindex 97ae1e29a27..66117584052 100644\n--- a/tests/test_downloadermiddleware_retry.py\n+++ b/tests/test_downloadermiddleware_retry.py\n@@ -122,7 +122,7 @@ def test_exception_to_retry_added(self):\n         req = Request(f\"http://www.scrapytest.org/{exc.__name__}\")\n         self._test_retry_exception(req, exc(\"foo\"), mw)\n \n-    def test_exception_to_retry_customMiddleware(self):\n+    def test_exception_to_retry_custom_middleware(self):\n         exc = ValueError\n \n         with warnings.catch_warnings(record=True) as warns:\n@@ -138,6 +138,21 @@ class MyRetryMiddleware(RetryMiddleware):\n         assert isinstance(req, Request)\n         self.assertEqual(req.meta[\"retry_times\"], 1)\n \n+    def test_exception_to_retry_custom_middleware_self(self):\n+        class MyRetryMiddleware(RetryMiddleware):\n+            def process_exception(self, request, exception, spider):\n+                if isinstance(exception, self.EXCEPTIONS_TO_RETRY):\n+                    return self._retry(request, exception, spider)\n+\n+        exc = OSError\n+        mw2 = MyRetryMiddleware.from_crawler(self.crawler)\n+        req = Request(f\"http://www.scrapytest.org/{exc.__name__}\")\n+        with warnings.catch_warnings(record=True) as warns:\n+            req = mw2.process_exception(req, exc(\"foo\"), self.spider)\n+        assert isinstance(req, Request)\n+        self.assertEqual(req.meta[\"retry_times\"], 1)\n+        self.assertEqual(len(warns), 1)\n+\n     def _test_retry_exception(self, req, exception, mw=None):\n         if mw is None:\n             mw = self.mw\n",
    "problem_statement": "`downloadermiddlewares.retry.BackwardsCompatibilityMetaclass` does not provide backward compatibility for middleware instances\n# Description\r\n\r\nPreviously, `EXCEPTIONS_TO_RETRY` was an attribute of `RetryMiddleware`. This allows:\r\n- `RetryMiddleware` subclasses could access `EXCEPTIONS_TO_RETRY` via `cls.EXCEPTIONS_TO_RETRY`.\r\n- `RetryMiddleware` instances and instances of its subclasses could access `EXCEPTIONS_TO_RETRY` via `self.EXCEPTIONS_TO_RETRY`.\r\n\r\nIn 2.10 `EXCEPTIONS_TO_RETRY` was removed and added as a property to `BackwardsCompatibilityMetaclass`. This added compatibility only for the first point.\r\n\r\n# Steps to Reproduce\r\n\r\n```python\r\nclass MyRetryMiddleware(RetryMiddleware):\r\n\r\n    def process_exception(self, request, exception, spider):\r\n        if isinstance(exception, self.EXCEPTIONS_TO_RETRY) and not request.meta.get('dont_retry', False):\r\n            # update request\r\n            return self._retry(request, exception, spider)\r\n```\r\n\r\n# Expected behavior\r\n\r\nA warning about `EXCEPTIONS_TO_RETRY` deprecation.\r\n\r\n# Actual behavior\r\n\r\nAttributeError: 'MyRetryMiddleware' object has no attribute 'EXCEPTIONS_TO_RETRY'\r\n\r\n# Versions\r\n\r\n```\r\nScrapy       : 2.10.1\r\nlxml         : 4.9.3.0\r\nlibxml2      : 2.10.3\r\ncssselect    : 1.2.0\r\nparsel       : 1.8.1\r\nw3lib        : 2.1.2\r\nTwisted      : 22.10.0\r\nPython       : 3.11.4 (tags/v3.11.4:d2340ef, Jun  7 2023, 05:45:37) [MSC v.1934 64 bit (AMD64)]\r\npyOpenSSL    : 23.2.0 (OpenSSL 3.1.2 1 Aug 2023)\r\ncryptography : 41.0.3\r\nPlatform     : Windows-10-10.0.19044-SP0\r\n```\r\n\n",
    "hints_text": "The fix is actually very tricky. Here is a sample snippet:\r\n```python\r\nDEPRECATED_ATTRIBUTE = 'A'\r\n\r\n\r\ndef __getattr__(self, item):\r\n    if item == DEPRECATED_ATTRIBUTE:\r\n        return 'A does not exist'\r\n\r\n    raise AttributeError(f'{self.__class__.__name__!r} object has no attribute {item!r}')\r\n\r\n\r\nclass Meta(type):\r\n    __getattr__ = __getattr__\r\n\r\n\r\nclass Data(metaclass=Meta):\r\n    def __init__(self):\r\n        try:\r\n            self.a = self.__getattribute__(DEPRECATED_ATTRIBUTE)\r\n        except AttributeError:\r\n            self.a = 'a here'\r\n\r\n    __getattr__ = __getattr__\r\n\r\n\r\nclass DataDefineA(Data):\r\n    A = 1\r\n\r\n\r\nclass DataNoA(Data): pass\r\n\r\n\r\nprint(Data.A)\r\nprint(Data().A)\r\nprint(Data().a)\r\n\r\nprint(DataDefineA.A)\r\nprint(DataDefineA().A)\r\nprint(DataDefineA().a)\r\n\r\nprint(DataNoA.A)\r\nprint(DataNoA().A)\r\nprint(DataNoA().a)\r\n```\r\n\r\nOutput:\r\n```\r\nA does not exist\r\nA does not exist\r\na here\r\n1\r\n1\r\n1\r\nA does not exist\r\nA does not exist\r\na here\r\n```\r\n\r\nSuch solution achieves both compatibility issues. In addition, if a subclass or its instance defines attribute `A`, then it will be assigned to attribute `a`.\r\n\r\nRewritten `retry.py`:\r\n\r\n```python\r\nDEPRECATED_ATTRIBUTE = 'EXCEPTIONS_TO_RETRY'\r\n\r\n\r\ndef backwards_compatibility_getattr(self, item):\r\n    if item == DEPRECATED_ATTRIBUTE:\r\n        warnings.warn(\r\n            f\"Attribute RetryMiddleware.{DEPRECATED_ATTRIBUTE} is deprecated. \"\r\n            \"Use the RETRY_EXCEPTIONS setting instead.\",\r\n            ScrapyDeprecationWarning,\r\n            stacklevel=2,\r\n            )\r\n\r\n        return tuple(\r\n            load_object(x) if isinstance(x, str) else x\r\n            for x in Settings().getlist(\"RETRY_EXCEPTIONS\")\r\n            )\r\n\r\n    raise AttributeError(f'{self.__class__.__name__!r} object has no attribute {item!r}')\r\n\r\n\r\nclass BackwardsCompatibilityMetaclass(type):\r\n    __getattr__ = backwards_compatibility_getattr\r\n\r\n\r\nclass RetryMiddleware(metaclass=BackwardsCompatibilityMetaclass):\r\n    def __init__(self, settings):\r\n        if not settings.getbool(\"RETRY_ENABLED\"):\r\n            raise NotConfigured\r\n        self.max_retry_times = settings.getint(\"RETRY_TIMES\")\r\n        self.retry_http_codes = set(\r\n            int(x) for x in settings.getlist(\"RETRY_HTTP_CODES\")\r\n        )\r\n        self.priority_adjust = settings.getint(\"RETRY_PRIORITY_ADJUST\")\r\n\r\n        try:\r\n            self.exceptions_to_retry = self.__getattribute__(DEPRECATED_ATTRIBUTE)\r\n        except AttributeError:\r\n            # If EXCEPTIONS_TO_RETRY is not \"overridden\"\r\n            self.exceptions_to_retry = tuple(\r\n                load_object(x) if isinstance(x, str) else x\r\n                for x in settings.getlist(\"RETRY_EXCEPTIONS\")\r\n                )\r\n\r\n    __getattr__ = backwards_compatibility_getattr\r\n```",
    "created_at": "2023-09-14T12:08:06Z",
    "version": "2.10",
    "PASS_TO_PASS": [],
    "FAIL_TO_PASS": [
      "tests/test_downloadermiddleware_retry.py::RetryTest::test_exception_to_retry_custom_middleware_self"
    ]
  },
  {
    "repo": "scrapy/scrapy",
    "pull_number": 6013,
    "instance_id": "scrapy__scrapy-6013",
    "issue_numbers": [
      "6011"
    ],
    "base_commit": "7c497688f8e20339d766573ae6ce2e7782beb1da",
    "patch": "diff --git a/scrapy/settings/__init__.py b/scrapy/settings/__init__.py\nindex bc82cc098ac..ba9727bacf5 100644\n--- a/scrapy/settings/__init__.py\n+++ b/scrapy/settings/__init__.py\n@@ -238,7 +238,7 @@ def getdict(\n     def getdictorlist(\n         self,\n         name: _SettingsKeyT,\n-        default: Union[Dict[Any, Any], List[Any], None] = None,\n+        default: Union[Dict[Any, Any], List[Any], Tuple[Any], None] = None,\n     ) -> Union[Dict[Any, Any], List[Any]]:\n         \"\"\"Get a setting value as either a :class:`dict` or a :class:`list`.\n \n@@ -271,6 +271,8 @@ def getdictorlist(\n                 return value_loaded\n             except ValueError:\n                 return value.split(\",\")\n+        if isinstance(value, tuple):\n+            return list(value)\n         assert isinstance(value, (dict, list))\n         return copy.deepcopy(value)\n \n",
    "test_patch": "diff --git a/.github/workflows/tests-ubuntu.yml b/.github/workflows/tests-ubuntu.yml\nindex 54b3fbaa2c0..c2b6866286e 100644\n--- a/.github/workflows/tests-ubuntu.yml\n+++ b/.github/workflows/tests-ubuntu.yml\n@@ -48,13 +48,13 @@ jobs:\n           env:\n             TOXENV: botocore\n \n-        - python-version: \"3.12.0-beta.4\"\n+        - python-version: \"3.12.0-rc.1\"\n           env:\n             TOXENV: py\n-        - python-version: \"3.12.0-beta.4\"\n+        - python-version: \"3.12.0-rc.1\"\n           env:\n             TOXENV: asyncio\n-        - python-version: \"3.12.0-beta.4\"\n+        - python-version: \"3.12.0-rc.1\"\n           env:\n             TOXENV: extra-deps\n \ndiff --git a/tests/test_feedexport.py b/tests/test_feedexport.py\nindex 42fa25b1df8..6b82974fada 100644\n--- a/tests/test_feedexport.py\n+++ b/tests/test_feedexport.py\n@@ -1321,6 +1321,17 @@ def test_export_dicts(self):\n         yield self.assertExportedCsv(items, [\"foo\", \"egg\"], rows_csv)\n         yield self.assertExportedJsonLines(items, rows_jl)\n \n+    @defer.inlineCallbacks\n+    def test_export_tuple(self):\n+        items = [\n+            {\"foo\": \"bar1\", \"egg\": \"spam1\"},\n+            {\"foo\": \"bar2\", \"egg\": \"spam2\", \"baz\": \"quux\"},\n+        ]\n+\n+        settings = {\"FEED_EXPORT_FIELDS\": (\"foo\", \"baz\")}\n+        rows = [{\"foo\": \"bar1\", \"baz\": \"\"}, {\"foo\": \"bar2\", \"baz\": \"quux\"}]\n+        yield self.assertExported(items, [\"foo\", \"baz\"], rows, settings=settings)\n+\n     @defer.inlineCallbacks\n     def test_export_feed_export_fields(self):\n         # FEED_EXPORT_FIELDS option allows to order export fields\n",
    "problem_statement": "AssertionError   scrapy.settings in getdictorlist\n<!--\r\n\r\nThanks for taking an interest in Scrapy!\r\n\r\nIf you have a question that starts with \"How to...\", please see the Scrapy Community page: https://scrapy.org/community/.\r\nThe GitHub issue tracker's purpose is to deal with bug reports and feature requests for the project itself.\r\n\r\nKeep in mind that by filing an issue, you are expected to comply with Scrapy's Code of Conduct, including treating everyone with respect: https://github.com/scrapy/scrapy/blob/master/CODE_OF_CONDUCT.md\r\n\r\nThe following is a suggested template to structure your issue, you can find more guidelines at https://doc.scrapy.org/en/latest/contributing.html#reporting-bugs\r\n\r\n-->\r\n\r\n### Description\r\n\r\nAssertionError after upgrade to scrapy==2.10.0\r\n\r\n### Steps to Reproduce\r\n\r\n1. in settings.py:\r\n    ```python\r\n    FEED_EXPORT_FIELDS = tuple( ### if change to list - Ok\r\n      're_num idgood num code title price artikul valuta url_id url_rsp '\r\n      'is_auto_valuta code_nohash url_item'.split())\r\n    ```\r\n\r\n**Actual behavior:** [What actually happens]\r\n```\r\nTraceback (most recent call last):\r\n  File \"scrapy\", line 8, in <module>\r\n    sys.exit(execute())\r\n  File \"scrapy/cmdline.py\", line 161, in execute\r\n    _run_print_help(parser, _run_command, cmd, args, opts)\r\n  File \"scrapy/cmdline.py\", line 114, in _run_print_help\r\n    func(*a, **kw)\r\n  File \"scrapy/cmdline.py\", line 169, in _run_command\r\n    cmd.run(args, opts)\r\n  File \"scrapy/commands/crawl.py\", line 23, in run\r\n    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\r\n  File \"scrapy/crawler.py\", line 244, in crawl\r\n    crawler = self.create_crawler(crawler_or_spidercls)\r\n  File \"scrapy/crawler.py\", line 280, in create_crawler\r\n    return self._create_crawler(crawler_or_spidercls)\r\n  File \"scrapy/crawler.py\", line 363, in _create_crawler\r\n    return Crawler(spidercls, self.settings, init_reactor=init_reactor)\r\n  File \"scrapy/crawler.py\", line 119, in __init__\r\n    self.extensions: ExtensionManager = ExtensionManager.from_crawler(self)\r\n  File \"scrapy/middleware.py\", line 67, in from_crawler\r\n    return cls.from_settings(crawler.settings, crawler)\r\n  File \"scrapy/middleware.py\", line 44, in from_settings\r\n    mw = create_instance(mwcls, settings, crawler)\r\n  File \"scrapy/utils/misc.py\", line 188, in create_instance\r\n    instance = objcls.from_crawler(crawler, *args, **kwargs)\r\n  File \"scrapy/extensions/feedexport.py\", line 398, in from_crawler\r\n    exporter = cls(crawler)\r\n  File \"scrapy/extensions/feedexport.py\", line 436, in __init__\r\n    self.feeds[uri] = feed_complete_default_values_from_settings(\r\n  File \"scrapy/utils/conf.py\", line 136, in feed_complete_default_values_from_settings\r\n    out.setdefault(\"fields\", settings.getdictorlist(\"FEED_EXPORT_FIELDS\") or None)\r\n  File \"scrapy/settings/__init__.py\", line 274, in getdictorlist\r\n    assert isinstance(value, (dict, list))\r\nAssertionError\r\n```\r\n**Reproduces how often:** [What percentage of the time does it reproduce?]\r\n\r\n### Versions\r\n\r\n```\r\n$ scrapy version --verbose\r\nScrapy       : 2.10.0\r\nlxml         : 4.9.3.0\r\nlibxml2      : 2.10.3\r\ncssselect    : 1.2.0\r\nparsel       : 1.8.1\r\nw3lib        : 2.1.2\r\nTwisted      : 22.10.0\r\nPython       : 3.11.4 (main, Jun  8 2023, 09:53:06) [GCC 11.3.0]\r\npyOpenSSL    : 23.2.0 (OpenSSL 3.1.2 1 Aug 2023)\r\ncryptography : 41.0.3\r\nPlatform     : Linux-6.2.0-26-generic-x86_64-with-glibc2.35\r\n```\r\n\r\n### Additional context\r\n\r\nAny additional information, configuration, data or output from commands that might be necessary to reproduce or understand the issue. Please try not to include screenshots of code or the command line, paste the contents as text instead. You can use [GitHub Flavored Markdown](https://help.github.com/en/articles/creating-and-highlighting-code-blocks) to make the text look better.\r\n\n",
    "hints_text": "This doesn't seem to be a bug but intentional behavior, why would you need to pass a tuple object?\n@wRAR I tested locally and indeed `tuple` object on <2.10 works to filtering out fields, should we consider this a bug and put tuple object in assertion list?\nInitially `getlist()` was used to handle `FEED_EXPORT_FIELDS`, later `getdictorlist()` was added and used there (this isn't what was changed in 2.10.0, I'm explaining the behavior difference between the two functions). But `getlist()` uses `list()` while `getdictorlist()` doesn't. So both functions shouldn't return tuple but convert it to a list. It makes sense to check if `value` is not a dict and convert it to a list in that case.",
    "created_at": "2023-08-13T06:45:53Z",
    "version": "2.10",
    "PASS_TO_PASS": [],
    "FAIL_TO_PASS": [
      "tests/test_feedexport.py::FeedExportTest::test_export_tuple"
    ]
  },
  {
    "repo": "scrapy/scrapy",
    "pull_number": 5993,
    "instance_id": "scrapy__scrapy-5993",
    "issue_numbers": [
      "5726"
    ],
    "base_commit": "06ebdee35dc7ab5fa86c5076db341eae33485c37",
    "patch": "diff --git a/scrapy/utils/conf.py b/scrapy/utils/conf.py\nindex 43a8b65a5c7..1889f757190 100644\n--- a/scrapy/utils/conf.py\n+++ b/scrapy/utils/conf.py\n@@ -50,11 +50,17 @@ def _validate_values(compdict):\n                     \"please provide a real number or None instead\"\n                 )\n \n-    if isinstance(custom, (list, tuple)):\n-        _check_components(custom)\n-        return type(custom)(convert(c) for c in custom)\n-\n     if custom is not None:\n+        warnings.warn(\n+            \"The 'custom' attribute of build_component_list() is deprecated. \"\n+            \"Please merge its value into 'compdict' manually or change your \"\n+            \"code to use Settings.getwithbase().\",\n+            category=ScrapyDeprecationWarning,\n+            stacklevel=2,\n+        )\n+        if isinstance(custom, (list, tuple)):\n+            _check_components(custom)\n+            return type(custom)(convert(c) for c in custom)\n         compdict.update(custom)\n \n     _validate_values(compdict)\n",
    "test_patch": "diff --git a/tests/test_utils_conf.py b/tests/test_utils_conf.py\nindex 78ed9a7c9a7..dc3f01d574f 100644\n--- a/tests/test_utils_conf.py\n+++ b/tests/test_utils_conf.py\n@@ -1,6 +1,8 @@\n import unittest\n import warnings\n \n+import pytest\n+\n from scrapy.exceptions import ScrapyDeprecationWarning, UsageError\n from scrapy.settings import BaseSettings, Settings\n from scrapy.utils.conf import (\n@@ -21,44 +23,45 @@ def test_build_dict(self):\n     def test_backward_compatible_build_dict(self):\n         base = {\"one\": 1, \"two\": 2, \"three\": 3, \"five\": 5, \"six\": None}\n         custom = {\"two\": None, \"three\": 8, \"four\": 4}\n-        self.assertEqual(\n-            build_component_list(base, custom, convert=lambda x: x),\n-            [\"one\", \"four\", \"five\", \"three\"],\n-        )\n+        with pytest.warns(ScrapyDeprecationWarning, match=\"The 'custom' attribute\"):\n+            self.assertEqual(\n+                build_component_list(base, custom, convert=lambda x: x),\n+                [\"one\", \"four\", \"five\", \"three\"],\n+            )\n \n     def test_return_list(self):\n         custom = [\"a\", \"b\", \"c\"]\n-        self.assertEqual(\n-            build_component_list(None, custom, convert=lambda x: x), custom\n-        )\n+        with pytest.warns(ScrapyDeprecationWarning, match=\"The 'custom' attribute\"):\n+            self.assertEqual(\n+                build_component_list(None, custom, convert=lambda x: x), custom\n+            )\n \n     def test_map_dict(self):\n         custom = {\"one\": 1, \"two\": 2, \"three\": 3}\n-        self.assertEqual(\n-            build_component_list({}, custom, convert=lambda x: x.upper()),\n-            [\"ONE\", \"TWO\", \"THREE\"],\n-        )\n+        with pytest.warns(ScrapyDeprecationWarning, match=\"The 'custom' attribute\"):\n+            self.assertEqual(\n+                build_component_list({}, custom, convert=lambda x: x.upper()),\n+                [\"ONE\", \"TWO\", \"THREE\"],\n+            )\n \n     def test_map_list(self):\n         custom = [\"a\", \"b\", \"c\"]\n-        self.assertEqual(\n-            build_component_list(None, custom, lambda x: x.upper()), [\"A\", \"B\", \"C\"]\n-        )\n+        with pytest.warns(ScrapyDeprecationWarning, match=\"The 'custom' attribute\"):\n+            self.assertEqual(\n+                build_component_list(None, custom, lambda x: x.upper()), [\"A\", \"B\", \"C\"]\n+            )\n \n     def test_duplicate_components_in_dict(self):\n         duplicate_dict = {\"one\": 1, \"two\": 2, \"ONE\": 4}\n-        self.assertRaises(\n-            ValueError,\n-            build_component_list,\n-            {},\n-            duplicate_dict,\n-            convert=lambda x: x.lower(),\n-        )\n+        with self.assertRaises(ValueError):\n+            with pytest.warns(ScrapyDeprecationWarning, match=\"The 'custom' attribute\"):\n+                build_component_list({}, duplicate_dict, convert=lambda x: x.lower())\n \n     def test_duplicate_components_in_list(self):\n         duplicate_list = [\"a\", \"b\", \"a\"]\n         with self.assertRaises(ValueError) as cm:\n-            build_component_list(None, duplicate_list, convert=lambda x: x)\n+            with pytest.warns(ScrapyDeprecationWarning, match=\"The 'custom' attribute\"):\n+                build_component_list(None, duplicate_list, convert=lambda x: x)\n         self.assertIn(str(duplicate_list), str(cm.exception))\n \n     def test_duplicate_components_in_basesettings(self):\n@@ -76,9 +79,8 @@ def test_duplicate_components_in_basesettings(self):\n         )\n         # Same priority raises ValueError\n         duplicate_bs.set(\"ONE\", duplicate_bs[\"ONE\"], priority=20)\n-        self.assertRaises(\n-            ValueError, build_component_list, duplicate_bs, convert=lambda x: x.lower()\n-        )\n+        with self.assertRaises(ValueError):\n+            build_component_list(duplicate_bs, convert=lambda x: x.lower())\n \n     def test_valid_numbers(self):\n         # work well with None and numeric values\n@@ -92,15 +94,9 @@ def test_valid_numbers(self):\n         self.assertEqual(build_component_list(d, convert=lambda x: x), [\"b\", \"c\", \"a\"])\n         # raise exception for invalid values\n         d = {\"one\": \"5\"}\n-        self.assertRaises(ValueError, build_component_list, {}, d, convert=lambda x: x)\n-        d = {\"one\": \"1.0\"}\n-        self.assertRaises(ValueError, build_component_list, {}, d, convert=lambda x: x)\n-        d = {\"one\": [1, 2, 3]}\n-        self.assertRaises(ValueError, build_component_list, {}, d, convert=lambda x: x)\n-        d = {\"one\": {\"a\": \"a\", \"b\": 2}}\n-        self.assertRaises(ValueError, build_component_list, {}, d, convert=lambda x: x)\n-        d = {\"one\": \"lorem ipsum\"}\n-        self.assertRaises(ValueError, build_component_list, {}, d, convert=lambda x: x)\n+        with self.assertRaises(ValueError):\n+            with pytest.warns(ScrapyDeprecationWarning, match=\"The 'custom' attribute\"):\n+                build_component_list({}, d, convert=lambda x: x)\n \n \n class UtilsConfTestCase(unittest.TestCase):\n",
    "problem_statement": "Backward compatibility in utils.conf.build_component_list\nThere is some code from 2015 in `scrapy.utils.conf.build_component_list` marked as \"Backward compatibility for old (base, custom) call signature\", which was added in #1586. I couldn't understand after a quick glance why is it \"backward compatibility\" but if it's something deprecated we should deprecare it properly with a message, and if it's a properly supported code path we should remove the comments.\n",
    "hints_text": "Actually, as #1586 \"is a partial reversal of #1149\", the `(base, custom, convert=update_classpath)` signature is the oldest one, #1149 replaced it with the `(compdict, convert=update_classpath)` one and #1586 restored the support for both. So I think we should properly deprecate the old signature and later remove the support for it.\nI've tentatively added this to the next milestone so that we could deprecate this earlier and remove this earlier, but it shouldn't be a blocker.\nHi @wRAR, thank you for the information. In that case should we proceed to merge the linked PR's commit and then move ahead with the deprecation process?\nYeah, I think it's fine to merge it (but not close this issue) as the status of that code is now documented here.",
    "created_at": "2023-08-02T09:00:03Z",
    "version": "2.9",
    "PASS_TO_PASS": [],
    "FAIL_TO_PASS": [
      "tests/test_utils_conf.py::BuildComponentListTest::test_backward_compatible_build_dict",
      "tests/test_utils_conf.py::BuildComponentListTest::test_return_list",
      "tests/test_utils_conf.py::BuildComponentListTest::test_map_dict",
      "tests/test_utils_conf.py::BuildComponentListTest::test_map_list",
      "tests/test_utils_conf.py::BuildComponentListTest::test_duplicate_components_in_dict",
      "tests/test_utils_conf.py::BuildComponentListTest::test_duplicate_components_in_list",
      "tests/test_utils_conf.py::BuildComponentListTest::test_duplicate_components_in_basesettings",
      "tests/test_utils_conf.py::BuildComponentListTest::test_valid_numbers"
    ]
  },
  {
    "repo": "scrapy/scrapy",
    "pull_number": 5952,
    "instance_id": "scrapy__scrapy-5952",
    "issue_numbers": [
      "3090",
      "3111"
    ],
    "base_commit": "58300e066fc330892fb7ccbeceef6d8ac4efc4dc",
    "patch": "diff --git a/scrapy/exporters.py b/scrapy/exporters.py\nindex 4538c9ee10f..8254ea63ef1 100644\n--- a/scrapy/exporters.py\n+++ b/scrapy/exporters.py\n@@ -133,6 +133,13 @@ def _beautify_newline(self):\n         if self.indent is not None:\n             self.file.write(b\"\\n\")\n \n+    def _add_comma_after_first(self):\n+        if self.first_item:\n+            self.first_item = False\n+        else:\n+            self.file.write(b\",\")\n+            self._beautify_newline()\n+\n     def start_exporting(self):\n         self.file.write(b\"[\")\n         self._beautify_newline()\n@@ -142,14 +149,10 @@ def finish_exporting(self):\n         self.file.write(b\"]\")\n \n     def export_item(self, item):\n-        if self.first_item:\n-            self.first_item = False\n-        else:\n-            self.file.write(b\",\")\n-            self._beautify_newline()\n         itemdict = dict(self._get_serialized_fields(item))\n-        data = self.encoder.encode(itemdict)\n-        self.file.write(to_bytes(data, self.encoding))\n+        data = to_bytes(self.encoder.encode(itemdict), self.encoding)\n+        self._add_comma_after_first()\n+        self.file.write(data)\n \n \n class XmlItemExporter(BaseItemExporter):\n",
    "test_patch": "diff --git a/tests/test_exporters.py b/tests/test_exporters.py\nindex 63bebcf7a26..cb24ddd8ecf 100644\n--- a/tests/test_exporters.py\n+++ b/tests/test_exporters.py\n@@ -599,6 +599,20 @@ def test_two_items(self):\n     def test_two_dict_items(self):\n         self.assertTwoItemsExported(ItemAdapter(self.i).asdict())\n \n+    def test_two_items_with_failure_between(self):\n+        i1 = TestItem(name=\"Joseph\\xa3\", age=\"22\")\n+        i2 = TestItem(\n+            name=\"Maria\", age=1j\n+        )  # Invalid datetimes didn't consistently fail between Python versions\n+        i3 = TestItem(name=\"Jesus\", age=\"44\")\n+        self.ie.start_exporting()\n+        self.ie.export_item(i1)\n+        self.assertRaises(TypeError, self.ie.export_item, i2)\n+        self.ie.export_item(i3)\n+        self.ie.finish_exporting()\n+        exported = json.loads(to_unicode(self.output.getvalue()))\n+        self.assertEqual(exported, [dict(i1), dict(i3)])\n+\n     def test_nested_item(self):\n         i1 = self.item_class(name=\"Joseph\\xa3\", age=\"22\")\n         i2 = self.item_class(name=\"Maria\", age=i1)\n@@ -637,6 +651,24 @@ def test_nonstring_types_item(self):\n         self.assertEqual(exported, [item])\n \n \n+class JsonItemExporterToBytesTest(BaseItemExporterTest):\n+    def _get_exporter(self, **kwargs):\n+        kwargs[\"encoding\"] = \"latin\"\n+        return JsonItemExporter(self.output, **kwargs)\n+\n+    def test_two_items_with_failure_between(self):\n+        i1 = TestItem(name=\"Joseph\", age=\"22\")\n+        i2 = TestItem(name=\"\\u263a\", age=\"11\")\n+        i3 = TestItem(name=\"Jesus\", age=\"44\")\n+        self.ie.start_exporting()\n+        self.ie.export_item(i1)\n+        self.assertRaises(UnicodeEncodeError, self.ie.export_item, i2)\n+        self.ie.export_item(i3)\n+        self.ie.finish_exporting()\n+        exported = json.loads(to_unicode(self.output.getvalue(), encoding=\"latin\"))\n+        self.assertEqual(exported, [dict(i1), dict(i3)])\n+\n+\n class JsonItemExporterDataclassTest(JsonItemExporterTest):\n     item_class = TestDataClass\n     custom_field_item_class = CustomFieldDataclass\n",
    "problem_statement": "JsonItemExporter puts lone comma in the output if encoder fails\nIf `JsonItemExporter` is unable to encode the item, it still writes a delimiter (comma) to the output file. Here is a sample spider:\r\n\r\n```\r\n# -*- coding: utf-8 -*-\r\nimport datetime\r\nimport scrapy\r\n\r\nclass DummySpider(scrapy.Spider):\r\n    name = 'dummy'\r\n    start_urls = ['http://example.org/']\r\n\r\n    def parse(self, response):\r\n        yield {'date': datetime.date(2018, 1, 1)}\r\n        yield {'date': datetime.date(1234, 1, 1)}\r\n        yield {'date': datetime.date(2019, 1, 1)})\r\n```\r\n\r\nEncoding the second items fails:\r\n\r\n```\r\n2018-01-25 09:05:57 [scrapy.utils.signal] ERROR: Error caught on signal handler: <bound method ?.item_scraped of <scrapy.extensions.feedexport.FeedExporter object at 0x7fcbfbd81250>>\r\nTraceback (most recent call last):\r\n  File \"/home/pasmen/SW/Python/data-sci-env/local/lib/python2.7/site-packages/twisted/internet/defer.py\", line 150, in maybeDeferred\r\n    result = f(*args, **kw)\r\n  File \"/home/pasmen/SW/Python/data-sci-env/local/lib/python2.7/site-packages/pydispatch/robustapply.py\", line 55, in robustApply\r\n    return receiver(*arguments, **named)\r\n  File \"/home/pasmen/SW/Python/data-sci-env/local/lib/python2.7/site-packages/scrapy/extensions/feedexport.py\", line 224, in item_scraped\r\n    slot.exporter.export_item(item)\r\n  File \"/home/pasmen/SW/Python/data-sci-env/local/lib/python2.7/site-packages/scrapy/exporters.py\", line 130, in export_item\r\n    data = self.encoder.encode(itemdict)\r\n  File \"/usr/lib/python2.7/json/encoder.py\", line 207, in encode\r\n    chunks = self.iterencode(o, _one_shot=True)\r\n  File \"/usr/lib/python2.7/json/encoder.py\", line 270, in iterencode\r\n    return _iterencode(o, 0)\r\n  File \"/home/pasmen/SW/Python/data-sci-env/local/lib/python2.7/site-packages/scrapy/utils/serialize.py\", line 22, in default\r\n    return o.strftime(self.DATE_FORMAT)\r\nValueError: year=1234 is before 1900; the datetime strftime() methods require year >= 1900\r\n```\r\n\r\nThe output looks like this:\r\n\r\n```\r\n[\r\n{\"date\": \"2018-01-01\"},\r\n,\r\n{\"date\": \"2019-01-01\"}\r\n]\r\n```\r\n\r\nThis seems not to be a valid JSON file as e.g. `json.load()` and `jq` fail to parse it.\r\n\r\nI think the problem is in [`export_item`](https://github.com/scrapy/scrapy/blob/108f8c4fd20a47bb94e010e9c6296f7ed9fdb2bd/scrapy/exporters.py#L123) method of `JsonItemExporter` class where it outputs the comma before decoding the item. The correct approach would be to try to decode the item (possibly with other needed operations) and perform the write atomically.\nPartial fix for #3090 - only addresses JSON feeds.\nQuick fix for #3090, but the general problem of failure-during-multistep-serialisation also probably affects our XML exporter. A more broad fix would be to buffer changes and then \"commit\" them once everything has succeeded, but that's a bigger change than I felt comfortable writing without some discussion. :)\n",
    "hints_text": "Thanks @tlinhart - will take a look at making a fix to that right away. :)\n",
    "created_at": "2023-06-15T07:06:58Z",
    "version": "2.9",
    "PASS_TO_PASS": [],
    "FAIL_TO_PASS": [
      "tests/test_exporters.py::JsonItemExporterTest::test_two_items_with_failure_between",
      "tests/test_exporters.py::JsonItemExporterToBytesTest::test_two_items_with_failure_between"
    ]
  },
  {
    "repo": "scrapy/scrapy",
    "pull_number": 5929,
    "instance_id": "scrapy__scrapy-5929",
    "issue_numbers": [
      "5929"
    ],
    "base_commit": "52c072640aa61884de05214cb1bdda07c2a87bef",
    "patch": "diff --git a/docs/topics/downloader-middleware.rst b/docs/topics/downloader-middleware.rst\nindex 7665a901a7e..a8e5b23bf92 100644\n--- a/docs/topics/downloader-middleware.rst\n+++ b/docs/topics/downloader-middleware.rst\n@@ -915,6 +915,7 @@ settings (see the settings documentation for more info):\n * :setting:`RETRY_ENABLED`\n * :setting:`RETRY_TIMES`\n * :setting:`RETRY_HTTP_CODES`\n+* :setting:`RETRY_EXCEPTIONS`\n \n .. reqmeta:: dont_retry\n \n@@ -966,6 +967,37 @@ In some cases you may want to add 400 to :setting:`RETRY_HTTP_CODES` because\n it is a common code used to indicate server overload. It is not included by\n default because HTTP specs say so.\n \n+.. setting:: RETRY_EXCEPTIONS\n+\n+RETRY_EXCEPTIONS\n+^^^^^^^^^^^^^^^^\n+\n+Default::\n+\n+    [\n+        'twisted.internet.defer.TimeoutError',\n+        'twisted.internet.error.TimeoutError',\n+        'twisted.internet.error.DNSLookupError',\n+        'twisted.internet.error.ConnectionRefusedError',\n+        'twisted.internet.error.ConnectionDone',\n+        'twisted.internet.error.ConnectError',\n+        'twisted.internet.error.ConnectionLost',\n+        'twisted.internet.error.TCPTimedOutError',\n+        'twisted.web.client.ResponseFailed',\n+        IOError,\n+        'scrapy.core.downloader.handlers.http11.TunnelError',\n+    ]\n+\n+List of exceptions to retry.\n+\n+Each list entry may be an exception type or its import path as a string.\n+\n+An exception will not be caught when the exception type is not in\n+:setting:`RETRY_EXCEPTIONS` or when the maximum number of retries for a request\n+has been exceeded (see :setting:`RETRY_TIMES`). To learn about uncaught\n+exception propagation, see\n+:meth:`~scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception`.\n+\n .. setting:: RETRY_PRIORITY_ADJUST\n \n RETRY_PRIORITY_ADJUST\ndiff --git a/scrapy/downloadermiddlewares/retry.py b/scrapy/downloadermiddlewares/retry.py\nindex 081642a4b81..50cbc3111a1 100644\n--- a/scrapy/downloadermiddlewares/retry.py\n+++ b/scrapy/downloadermiddlewares/retry.py\n@@ -9,31 +9,36 @@\n Failed pages are collected on the scraping process and rescheduled at the end,\n once the spider has finished crawling all regular (non failed) pages.\n \"\"\"\n+import warnings\n from logging import Logger, getLogger\n from typing import Optional, Union\n \n-from twisted.internet import defer\n-from twisted.internet.error import (\n-    ConnectError,\n-    ConnectionDone,\n-    ConnectionLost,\n-    ConnectionRefusedError,\n-    DNSLookupError,\n-    TCPTimedOutError,\n-    TimeoutError,\n-)\n-from twisted.web.client import ResponseFailed\n-\n-from scrapy.core.downloader.handlers.http11 import TunnelError\n-from scrapy.exceptions import NotConfigured\n+from scrapy.exceptions import NotConfigured, ScrapyDeprecationWarning\n from scrapy.http.request import Request\n+from scrapy.settings import Settings\n from scrapy.spiders import Spider\n+from scrapy.utils.misc import load_object\n from scrapy.utils.python import global_object_name\n from scrapy.utils.response import response_status_message\n \n retry_logger = getLogger(__name__)\n \n \n+class BackwardsCompatibilityMetaclass(type):\n+    @property\n+    def EXCEPTIONS_TO_RETRY(cls):\n+        warnings.warn(\n+            \"Attribute RetryMiddleware.EXCEPTIONS_TO_RETRY is deprecated. \"\n+            \"Use the RETRY_EXCEPTIONS setting instead.\",\n+            ScrapyDeprecationWarning,\n+            stacklevel=2,\n+        )\n+        return tuple(\n+            load_object(x) if isinstance(x, str) else x\n+            for x in Settings().getlist(\"RETRY_EXCEPTIONS\")\n+        )\n+\n+\n def get_retry_request(\n     request: Request,\n     *,\n@@ -121,23 +126,7 @@ def parse(self, response):\n     return None\n \n \n-class RetryMiddleware:\n-    # IOError is raised by the HttpCompression middleware when trying to\n-    # decompress an empty response\n-    EXCEPTIONS_TO_RETRY = (\n-        defer.TimeoutError,\n-        TimeoutError,\n-        DNSLookupError,\n-        ConnectionRefusedError,\n-        ConnectionDone,\n-        ConnectError,\n-        ConnectionLost,\n-        TCPTimedOutError,\n-        ResponseFailed,\n-        IOError,\n-        TunnelError,\n-    )\n-\n+class RetryMiddleware(metaclass=BackwardsCompatibilityMetaclass):\n     def __init__(self, settings):\n         if not settings.getbool(\"RETRY_ENABLED\"):\n             raise NotConfigured\n@@ -147,6 +136,16 @@ def __init__(self, settings):\n         )\n         self.priority_adjust = settings.getint(\"RETRY_PRIORITY_ADJUST\")\n \n+        if not hasattr(\n+            self, \"EXCEPTIONS_TO_RETRY\"\n+        ):  # If EXCEPTIONS_TO_RETRY is not \"overriden\"\n+            self.exceptions_to_retry = tuple(\n+                load_object(x) if isinstance(x, str) else x\n+                for x in settings.getlist(\"RETRY_EXCEPTIONS\")\n+            )\n+        else:\n+            self.exceptions_to_retry = self.EXCEPTIONS_TO_RETRY\n+\n     @classmethod\n     def from_crawler(cls, crawler):\n         return cls(crawler.settings)\n@@ -160,7 +159,7 @@ def process_response(self, request, response, spider):\n         return response\n \n     def process_exception(self, request, exception, spider):\n-        if isinstance(exception, self.EXCEPTIONS_TO_RETRY) and not request.meta.get(\n+        if isinstance(exception, self.exceptions_to_retry) and not request.meta.get(\n             \"dont_retry\", False\n         ):\n             return self._retry(request, exception, spider)\ndiff --git a/scrapy/settings/default_settings.py b/scrapy/settings/default_settings.py\nindex 260ec1701c7..89837b4abf3 100644\n--- a/scrapy/settings/default_settings.py\n+++ b/scrapy/settings/default_settings.py\n@@ -258,6 +258,21 @@\n RETRY_TIMES = 2  # initial response + 2 retries = 3 requests\n RETRY_HTTP_CODES = [500, 502, 503, 504, 522, 524, 408, 429]\n RETRY_PRIORITY_ADJUST = -1\n+RETRY_EXCEPTIONS = [\n+    \"twisted.internet.defer.TimeoutError\",\n+    \"twisted.internet.error.TimeoutError\",\n+    \"twisted.internet.error.DNSLookupError\",\n+    \"twisted.internet.error.ConnectionRefusedError\",\n+    \"twisted.internet.error.ConnectionDone\",\n+    \"twisted.internet.error.ConnectError\",\n+    \"twisted.internet.error.ConnectionLost\",\n+    \"twisted.internet.error.TCPTimedOutError\",\n+    \"twisted.web.client.ResponseFailed\",\n+    # IOError is raised by the HttpCompression middleware when trying to\n+    # decompress an empty response\n+    IOError,\n+    \"scrapy.core.downloader.handlers.http11.TunnelError\",\n+]\n \n ROBOTSTXT_OBEY = False\n ROBOTSTXT_PARSER = \"scrapy.robotstxt.ProtegoRobotParser\"\n",
    "test_patch": "diff --git a/tests/test_downloadermiddleware_retry.py b/tests/test_downloadermiddleware_retry.py\nindex 63bd618489b..97ae1e29a27 100644\n--- a/tests/test_downloadermiddleware_retry.py\n+++ b/tests/test_downloadermiddleware_retry.py\n@@ -1,5 +1,6 @@\n import logging\n import unittest\n+import warnings\n \n from testfixtures import LogCapture\n from twisted.internet import defer\n@@ -15,6 +16,7 @@\n from scrapy.downloadermiddlewares.retry import RetryMiddleware, get_retry_request\n from scrapy.exceptions import IgnoreRequest\n from scrapy.http import Request, Response\n+from scrapy.settings.default_settings import RETRY_EXCEPTIONS\n from scrapy.spiders import Spider\n from scrapy.utils.test import get_crawler\n \n@@ -110,19 +112,48 @@ def test_twistederrors(self):\n             == 2\n         )\n \n-    def _test_retry_exception(self, req, exception):\n+    def test_exception_to_retry_added(self):\n+        exc = ValueError\n+        settings_dict = {\n+            \"RETRY_EXCEPTIONS\": list(RETRY_EXCEPTIONS) + [exc],\n+        }\n+        crawler = get_crawler(Spider, settings_dict=settings_dict)\n+        mw = RetryMiddleware.from_crawler(crawler)\n+        req = Request(f\"http://www.scrapytest.org/{exc.__name__}\")\n+        self._test_retry_exception(req, exc(\"foo\"), mw)\n+\n+    def test_exception_to_retry_customMiddleware(self):\n+        exc = ValueError\n+\n+        with warnings.catch_warnings(record=True) as warns:\n+\n+            class MyRetryMiddleware(RetryMiddleware):\n+                EXCEPTIONS_TO_RETRY = RetryMiddleware.EXCEPTIONS_TO_RETRY + (exc,)\n+\n+            self.assertEqual(len(warns), 1)\n+\n+        mw2 = MyRetryMiddleware.from_crawler(self.crawler)\n+        req = Request(f\"http://www.scrapytest.org/{exc.__name__}\")\n+        req = mw2.process_exception(req, exc(\"foo\"), self.spider)\n+        assert isinstance(req, Request)\n+        self.assertEqual(req.meta[\"retry_times\"], 1)\n+\n+    def _test_retry_exception(self, req, exception, mw=None):\n+        if mw is None:\n+            mw = self.mw\n+\n         # first retry\n-        req = self.mw.process_exception(req, exception, self.spider)\n+        req = mw.process_exception(req, exception, self.spider)\n         assert isinstance(req, Request)\n         self.assertEqual(req.meta[\"retry_times\"], 1)\n \n         # second retry\n-        req = self.mw.process_exception(req, exception, self.spider)\n+        req = mw.process_exception(req, exception, self.spider)\n         assert isinstance(req, Request)\n         self.assertEqual(req.meta[\"retry_times\"], 2)\n \n         # discard it\n-        req = self.mw.process_exception(req, exception, self.spider)\n+        req = mw.process_exception(req, exception, self.spider)\n         self.assertEqual(req, None)\n \n \n",
    "problem_statement": "Configurable exception list retry middleware 2701\nResolves https://github.com/scrapy/scrapy/issues/2701, closes https://github.com/scrapy/scrapy/pull/2984, closes https://github.com/scrapy/scrapy/pull/3334.\n",
    "hints_text": "",
    "created_at": "2023-05-09T15:17:38Z",
    "version": "2.9",
    "PASS_TO_PASS": [],
    "FAIL_TO_PASS": [
      "tests/test_downloadermiddleware_retry.py::RetryTest::test_exception_to_retry_added",
      "tests/test_downloadermiddleware_retry.py::RetryTest::test_exception_to_retry_customMiddleware",
      "tests/test_downloadermiddleware_retry.py::RetryTest::test_twistederrors"
    ]
  },
  {
    "repo": "scrapy/scrapy",
    "pull_number": 5917,
    "instance_id": "scrapy__scrapy-5917",
    "issue_numbers": [
      "5914"
    ],
    "base_commit": "b50c032ee9a75d1c9b42f1126637fdc655b141a8",
    "patch": "diff --git a/scrapy/http/response/text.py b/scrapy/http/response/text.py\nindex 73bb811dedb..d580a7876ee 100644\n--- a/scrapy/http/response/text.py\n+++ b/scrapy/http/response/text.py\n@@ -100,11 +100,13 @@ def urljoin(self, url):\n     @memoizemethod_noargs\n     def _headers_encoding(self):\n         content_type = self.headers.get(b\"Content-Type\", b\"\")\n-        return http_content_type_encoding(to_unicode(content_type))\n+        return http_content_type_encoding(to_unicode(content_type, encoding=\"latin-1\"))\n \n     def _body_inferred_encoding(self):\n         if self._cached_benc is None:\n-            content_type = to_unicode(self.headers.get(b\"Content-Type\", b\"\"))\n+            content_type = to_unicode(\n+                self.headers.get(b\"Content-Type\", b\"\"), encoding=\"latin-1\"\n+            )\n             benc, ubody = html_to_unicode(\n                 content_type,\n                 self.body,\ndiff --git a/scrapy/responsetypes.py b/scrapy/responsetypes.py\nindex f01e9096ccd..58884f21a13 100644\n--- a/scrapy/responsetypes.py\n+++ b/scrapy/responsetypes.py\n@@ -51,7 +51,9 @@ def from_content_type(self, content_type, content_encoding=None):\n         header\"\"\"\n         if content_encoding:\n             return Response\n-        mimetype = to_unicode(content_type).split(\";\")[0].strip().lower()\n+        mimetype = (\n+            to_unicode(content_type, encoding=\"latin-1\").split(\";\")[0].strip().lower()\n+        )\n         return self.from_mimetype(mimetype)\n \n     def from_content_disposition(self, content_disposition):\n",
    "test_patch": "diff --git a/tests/test_http_response.py b/tests/test_http_response.py\nindex dbc9f1feff9..a05b702aa71 100644\n--- a/tests/test_http_response.py\n+++ b/tests/test_http_response.py\n@@ -448,6 +448,13 @@ def test_encoding(self):\n             body=codecs.BOM_UTF8 + b\"\\xc2\\xa3\",\n             headers={\"Content-type\": [\"text/html; charset=cp1251\"]},\n         )\n+        r9 = self.response_class(\n+            \"http://www.example.com\",\n+            body=b\"\\x80\",\n+            headers={\n+                \"Content-type\": [b\"application/x-download; filename=\\x80dummy.txt\"]\n+            },\n+        )\n \n         self.assertEqual(r1._headers_encoding(), \"utf-8\")\n         self.assertEqual(r2._headers_encoding(), None)\n@@ -458,9 +465,12 @@ def test_encoding(self):\n         self.assertEqual(r4._headers_encoding(), None)\n         self.assertEqual(r5._headers_encoding(), None)\n         self.assertEqual(r8._headers_encoding(), \"cp1251\")\n+        self.assertEqual(r9._headers_encoding(), None)\n         self.assertEqual(r8._declared_encoding(), \"utf-8\")\n+        self.assertEqual(r9._declared_encoding(), None)\n         self._assert_response_encoding(r5, \"utf-8\")\n         self._assert_response_encoding(r8, \"utf-8\")\n+        self._assert_response_encoding(r9, \"cp1252\")\n         assert (\n             r4._body_inferred_encoding() is not None\n             and r4._body_inferred_encoding() != \"ascii\"\n@@ -470,6 +480,7 @@ def test_encoding(self):\n         self._assert_response_values(r3, \"iso-8859-1\", \"\\xa3\")\n         self._assert_response_values(r6, \"gb18030\", \"\\u2015\")\n         self._assert_response_values(r7, \"gb18030\", \"\\u2015\")\n+        self._assert_response_values(r9, \"cp1252\", \"\u20ac\")\n \n         # TextResponse (and subclasses) must be passed a encoding when instantiating with unicode bodies\n         self.assertRaises(\ndiff --git a/tests/test_responsetypes.py b/tests/test_responsetypes.py\nindex 85996051830..6e1ed82f0c2 100644\n--- a/tests/test_responsetypes.py\n+++ b/tests/test_responsetypes.py\n@@ -42,6 +42,7 @@ def test_from_content_type(self):\n             (\"application/octet-stream\", Response),\n             (\"application/x-json; encoding=UTF8;charset=UTF-8\", TextResponse),\n             (\"application/json-amazonui-streaming;charset=UTF-8\", TextResponse),\n+            (b\"application/x-download; filename=\\x80dummy.txt\", Response),\n         ]\n         for source, cls in mappings:\n             retcls = responsetypes.from_content_type(source)\n",
    "problem_statement": "Exception with non-UTF-8 Content-Type\n<!--\r\n\r\nThanks for taking an interest in Scrapy!\r\n\r\nIf you have a question that starts with \"How to...\", please see the Scrapy Community page: https://scrapy.org/community/.\r\nThe GitHub issue tracker's purpose is to deal with bug reports and feature requests for the project itself.\r\n\r\nKeep in mind that by filing an issue, you are expected to comply with Scrapy's Code of Conduct, including treating everyone with respect: https://github.com/scrapy/scrapy/blob/master/CODE_OF_CONDUCT.md\r\n\r\nThe following is a suggested template to structure your issue, you can find more guidelines at https://doc.scrapy.org/en/latest/contributing.html#reporting-bugs\r\n\r\n-->\r\n\r\n### Description\r\n\r\nI am trying to download a document from the link http://pravo.gov.ru/proxy/ips/?savertf=&link_id=0&nd=128284801&intelsearch=&firstDoc=1&page=all\r\nEverything works fine in the browser, but when I try to automate this process through scrapy, everything break down.\r\n### Steps to Reproduce\r\n\r\n1.Create new spider `scrapy genspider test pravo.gov.ru`\r\n2. Paste code\r\n```\r\nimport scrapy\r\n\r\n\r\nclass TestSpider(scrapy.Spider):\r\n    name = \"test\"\r\n    allowed_domains = [\"pravo.gov.ru\"]\r\n    start_urls = [\"http://pravo.gov.ru/proxy/ips/\"]\r\n\r\n    def parse(self, response):\r\n        yield scrapy.Request(\r\n            self.start_urls[0] + f'?savertf=&link_id=0&nd=128284801&intelsearch=&firstDoc=1&page=all',\r\n            callback=self.test_parse,\r\n            encoding='cp1251') #The same behavior without this line\r\n           \r\n\r\n    def test_parse(self, response):\r\n        pass\r\n```\r\n\r\n4. run\r\n***OR***\r\nrun `scrapy fetch http://pravo.gov.ru/proxy/ips/?savertf=&link_id=0&nd=128284801&intelsearch=&firstDoc=1&page=all\"\"`\r\n**Expected behavior:** The document on the link is downloaded\r\n**Actual behavior:** \r\n2023-04-28 00:07:35 [scrapy.core.scraper] ERROR: Error downloading <GET http://pravo.gov.ru/proxy/ips/?savertf=&link_id=0&nd=128284801&intelsearch=&fir\r\nstDoc=1&page=all>\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python310\\site-packages\\twisted\\internet\\defer.py\", line 1693, in _inlineCallbacks\r\n    result = context.run(\r\n  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python310\\site-packages\\twisted\\python\\failure.py\", line 518, in throwExceptionIntoGenerator\r\n    return g.throw(self.type, self.value, self.tb)\r\n  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python310\\site-packages\\scrapy\\core\\downloader\\middleware.py\", line 52, in process_request\r\n    return (yield download_func(request=request, spider=spider))\r\n  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python310\\site-packages\\twisted\\internet\\defer.py\", line 892, in _runCallbacks\r\n    current.result = callback(  # type: ignore[misc]\r\n  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python310\\site-packages\\scrapy\\core\\downloader\\handlers\\http11.py\", line 501, in _cb_bodydone\r\n    respcls = responsetypes.from_args(headers=headers, url=url, body=result[\"body\"])\r\n  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python310\\site-packages\\scrapy\\responsetypes.py\", line 113, in from_args\r\n    cls = self.from_headers(headers)\r\n  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python310\\site-packages\\scrapy\\responsetypes.py\", line 75, in from_headers\r\n    cls = self.from_content_type(\r\n  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python310\\site-packages\\scrapy\\responsetypes.py\", line 55, in from_content_type\r\n    mimetype = to_unicode(content_type).split(\";\")[0].strip().lower()\r\n  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python310\\site-packages\\scrapy\\utils\\python.py\", line 97, in to_unicode\r\n    return text.decode(encoding, errors)\r\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xcf in position 33: invalid continuation byte\r\n\r\n**Reproduces how often:** 100% runs\r\n\r\n### Versions\r\n\r\nScrapy       : 2.8.0\r\nlxml         : 4.9.2.0\r\nlibxml2      : 2.9.12\r\ncssselect    : 1.2.0\r\nparsel       : 1.8.1\r\nw3lib        : 2.1.1\r\nTwisted      : 22.10.0\r\nPython       : 3.10.9 | packaged by Anaconda, Inc. | (main, Mar  1 2023, 18:18:15) [MSC v.1916 64 bit (AMD64)]\r\npyOpenSSL    : 23.1.1 (OpenSSL 3.1.0 14 Mar 2023)\r\ncryptography : 40.0.2\r\nPlatform     : Windows-10-10.0.19044-SP0\r\n### Additional context\r\n\r\nThese documents are encoded in cp1251 encoding, which is clearly indicated in their headers :  \r\nContent-Transfer-Encoding: quoted-printable\r\nContent-Type: text/html; charset=\"windows-1251\"\r\nThe same behavior when trying to save a file using FilesPipeline\n",
    "hints_text": "> which is clearly indicated in their headers\r\n\r\nI can't confirm this. E.g. this is from curl:\r\n\r\n```\r\n< HTTP/1.1 200 OK\r\n< Server: nginx\r\n< Date: Fri, 28 Apr 2023 07:50:55 GMT\r\n< Content-Type: application/x-download; filename=\ufffd-593-24_04_2023.rtf\r\n< Content-Length: 15961\r\n< Connection: keep-alive\r\n< Content-Disposition: attachment; filename=\ufffd-593-24_04_2023.rtf\r\n```\r\n\r\nI see the same in a browser.\n> > which is clearly indicated in their headers\r\n> \r\n> I can't confirm this. E.g. this is from curl:\r\n> \r\n> ```\r\n> < HTTP/1.1 200 OK\r\n> < Server: nginx\r\n> < Date: Fri, 28 Apr 2023 07:50:55 GMT\r\n> < Content-Type: application/x-download; filename=\ufffd-593-24_04_2023.rtf\r\n> < Content-Length: 15961\r\n> < Connection: keep-alive\r\n> < Content-Disposition: attachment; filename=\ufffd-593-24_04_2023.rtf\r\n> ```\r\n> \r\n> I see the same in a browser.\r\n\r\nI wrote about the headers of the file itself:\r\n```\r\n------=_NextPart_01CAD650.0093E2A0\r\nContent-Location: file:///C:/B1334631/001.htm\r\nContent-Transfer-Encoding: quoted-printable\r\nContent-Type: text/html; charset=\"windows-1251\"\r\n```\r\nIf I use the requests library in python, then the response from the server comes without any errors.\r\n```\r\nimport requests\r\nresponse = requests.get('http://pravo.gov.ru/proxy/ips/?savertf=&link_id=5&nd=128284801&&page=all')\r\nprint(response.content.decode('cp1251'))\r\nprint(response.headers)\r\n```\r\n`MIME-Version: 1.0\r\nContent-Type: multipart/related; boundary=\"----=_NextPart_01CAD650.0093E2A0\"\r\n..................................................................................\r\n`\r\n`\r\n{'Server': 'nginx', 'Date': 'Fri, 28 Apr 2023 08:51:06 GMT', 'Content-Type': 'application/x-download; filename=\u00cf-593-24_04_2023.rtf', 'Content-Length': '15961', 'Connection': 'keep-alive', 'Content-Disposition': 'attachment; filename=\u00cf-593-24_04_2023.rtf'}\r\n`\r\nI don't understand why exactly an encoding error occurs when trying to make a scrapy request \r\n`scrapy.Request('http://pravo.gov.ru/proxy/ips/?savertf=&link_id=5&nd=128284801&&page=all')\r\n`\n> I wrote about the headers of the file itself:\r\n\r\nI don't think these matter to any of the libraries you mentioned. Even your `requests` sample prints them as a part of the response body.\r\n\r\n> If I use the requests library in python, then the response from the server comes without any errors.\r\n\r\nSure, it doesn't try to decode the response, unlike Scrapy.\r\n\r\nThe _actual_ problem is that the `Content-Type` header value is in CP1251 (I guess?): the actual exception happens when trying to get the response MIME type and the code assumes it's in UTF-8 while it's actually `b'application/x-download; filename=\\xcf-593-24_04_2023.rtf'`. We can specify the latin1 encoding instead of the implicit utf-8 when converting the `Content-Type` header value to `str` (`scrapy.http.response.text.TextResponse._headers_encoding()`, `scrapy.http.response.text.TextResponse._body_inferred_encoding()`, `scrapy.responsetypes.ResponseTypes.from_content_type()`) and this will prevent the exceptions in this case, I don't think we can do anything better. I also don't think we need the file name from this header value (or from the `Content-Disposition` header value which has the same problem) as we don't support `Content-Disposition: attachment`, and return the raw response body directly so we don't need to guess the encoding for unquoted non-ASCII file names.\nI use this code to download documents from this site:\r\n```\r\nyield scrapy.Request(\r\n          self.start_urls[0] + f'?savertf=&link_id=5&nd=128284801&&page=all',\r\n          callback=self.test_parse,\r\n          meta={'download_file': True}\r\n      )\r\n\r\n\r\n```\r\n```\r\nclass FileDownloaderMiddleware:\r\n    def process_request(self, request, spider):\r\n        if request.meta.get('download_file'):\r\n            response = requests.get(request.url)\r\n            response.raise_for_status()\r\n            return scrapy.http.Response(\r\n                url=request.url,\r\n                body=response.content,\r\n                headers=response.headers,\r\n            )\r\n        return None\r\n\r\n```",
    "created_at": "2023-05-02T15:56:07Z",
    "version": "2.8",
    "PASS_TO_PASS": [],
    "FAIL_TO_PASS": [
      "tests/test_http_response.py::TextResponseTest::test_encoding",
      "tests/test_responsetypes.py::ResponseTypesTest::test_from_content_type"
    ]
  },
  {
    "repo": "scrapy/scrapy",
    "pull_number": 5885,
    "instance_id": "scrapy__scrapy-5885",
    "issue_numbers": [
      "5872"
    ],
    "base_commit": "96033ce5a7f857942e3c6d488c8aab5b4aa03295",
    "patch": "diff --git a/scrapy/utils/python.py b/scrapy/utils/python.py\nindex fc50e0f1240..818fa5d6b21 100644\n--- a/scrapy/utils/python.py\n+++ b/scrapy/utils/python.py\n@@ -174,33 +174,33 @@ def binary_is_text(data):\n \n \n def get_func_args(func, stripself=False):\n-    \"\"\"Return the argument name list of a callable\"\"\"\n-    if inspect.isfunction(func):\n-        spec = inspect.getfullargspec(func)\n-        func_args = spec.args + spec.kwonlyargs\n-    elif inspect.isclass(func):\n-        return get_func_args(func.__init__, True)\n-    elif inspect.ismethod(func):\n-        return get_func_args(func.__func__, True)\n-    elif inspect.ismethoddescriptor(func):\n-        return []\n-    elif isinstance(func, partial):\n-        return [\n-            x\n-            for x in get_func_args(func.func)[len(func.args) :]\n-            if not (func.keywords and x in func.keywords)\n-        ]\n-    elif hasattr(func, \"__call__\"):\n-        if inspect.isroutine(func):\n-            return []\n-        if getattr(func, \"__name__\", None) == \"__call__\":\n-            return []\n-        return get_func_args(func.__call__, True)\n+    \"\"\"Return the argument name list of a callable object\"\"\"\n+    if not callable(func):\n+        raise TypeError(f\"func must be callable, got '{type(func).__name__}'\")\n+\n+    args = []\n+    try:\n+        sig = inspect.signature(func)\n+    except ValueError:\n+        return args\n+\n+    if isinstance(func, partial):\n+        partial_args = func.args\n+        partial_kw = func.keywords\n+\n+        for name, param in sig.parameters.items():\n+            if param.name in partial_args:\n+                continue\n+            if partial_kw and param.name in partial_kw:\n+                continue\n+            args.append(name)\n     else:\n-        raise TypeError(f\"{type(func)} is not callable\")\n-    if stripself:\n-        func_args.pop(0)\n-    return func_args\n+        for name in sig.parameters.keys():\n+            args.append(name)\n+\n+    if stripself and args and args[0] == \"self\":\n+        args = args[1:]\n+    return args\n \n \n def get_spec(func):\n",
    "test_patch": "diff --git a/tests/test_utils_python.py b/tests/test_utils_python.py\nindex 57f40c2e5fd..80d2e8da100 100644\n--- a/tests/test_utils_python.py\n+++ b/tests/test_utils_python.py\n@@ -235,20 +235,16 @@ def __call__(self, a, b, c):\n         self.assertEqual(get_func_args(partial_f3), [\"c\"])\n         self.assertEqual(get_func_args(cal), [\"a\", \"b\", \"c\"])\n         self.assertEqual(get_func_args(object), [])\n+        self.assertEqual(get_func_args(str.split, stripself=True), [\"sep\", \"maxsplit\"])\n+        self.assertEqual(get_func_args(\" \".join, stripself=True), [\"iterable\"])\n \n         if platform.python_implementation() == \"CPython\":\n-            # TODO: how do we fix this to return the actual argument names?\n-            self.assertEqual(get_func_args(str.split), [])\n-            self.assertEqual(get_func_args(\" \".join), [])\n+            # doesn't work on CPython: https://bugs.python.org/issue42785\n             self.assertEqual(get_func_args(operator.itemgetter(2)), [])\n         elif platform.python_implementation() == \"PyPy\":\n-            self.assertEqual(\n-                get_func_args(str.split, stripself=True), [\"sep\", \"maxsplit\"]\n-            )\n             self.assertEqual(\n                 get_func_args(operator.itemgetter(2), stripself=True), [\"obj\"]\n             )\n-            self.assertEqual(get_func_args(\" \".join, stripself=True), [\"iterable\"])\n \n     def test_without_none_values(self):\n         self.assertEqual(without_none_values([1, None, 3, 4]), [1, 3, 4])\n",
    "problem_statement": "get_func_args does not fully work in CPython\nAs [shown in tests](https://github.com/scrapy/scrapy/blob/ada917307844950a81226f020b596d5932187f6e/tests/test_utils_python.py#L240-L243), `get_func_args` does not work in CPython with inputs like `str.split`, `\"\".join` or `itemgetter(2)`.\n",
    "hints_text": "If/when somebody works on this, please check if `get_func_args` can just be replaced by `inspect.signature` (which \"Accepts a wide range of Python callables, from plain functions and classes to functools.partial() objects.\" but wasn't available when `get_func_args` was written).\nI'm on it. ",
    "created_at": "2023-04-04T12:18:17Z",
    "version": "2.8",
    "PASS_TO_PASS": [],
    "FAIL_TO_PASS": [
      "tests/test_utils_python.py::UtilsPythonTestCase::test_get_func_args"
    ]
  },
  {
    "repo": "scrapy/scrapy",
    "pull_number": 5847,
    "instance_id": "scrapy__scrapy-5847",
    "issue_numbers": [
      "872",
      "5633"
    ],
    "base_commit": "9e0bfc4a3dc92ac7f0fc0aff6254374045733299",
    "patch": "diff --git a/docs/topics/feed-exports.rst b/docs/topics/feed-exports.rst\nindex b4ac93b1d7f..89968a5f6da 100644\n--- a/docs/topics/feed-exports.rst\n+++ b/docs/topics/feed-exports.rst\n@@ -572,9 +572,12 @@ to ``.json`` or ``.xml``.\n FEED_STORE_EMPTY\n ----------------\n \n-Default: ``False``\n+Default: ``True``\n \n Whether to export empty feeds (i.e. feeds with no items).\n+If ``False``, and there are no items to export, no new files are created and \n+existing files are not modified, even if the :ref:`overwrite feed option \n+<feed-options>` is enabled.\n \n .. setting:: FEED_STORAGES\n \ndiff --git a/scrapy/extensions/feedexport.py b/scrapy/extensions/feedexport.py\nindex bcf0b779a7d..d088450a790 100644\n--- a/scrapy/extensions/feedexport.py\n+++ b/scrapy/extensions/feedexport.py\n@@ -277,8 +277,6 @@ def _store_in_thread(self, file):\n class FeedSlot:\n     def __init__(\n         self,\n-        file,\n-        exporter,\n         storage,\n         uri,\n         format,\n@@ -286,9 +284,14 @@ def __init__(\n         batch_id,\n         uri_template,\n         filter,\n+        feed_options,\n+        spider,\n+        exporters,\n+        settings,\n+        crawler,\n     ):\n-        self.file = file\n-        self.exporter = exporter\n+        self.file = None\n+        self.exporter = None\n         self.storage = storage\n         # feed params\n         self.batch_id = batch_id\n@@ -297,15 +300,44 @@ def __init__(\n         self.uri_template = uri_template\n         self.uri = uri\n         self.filter = filter\n+        # exporter params\n+        self.feed_options = feed_options\n+        self.spider = spider\n+        self.exporters = exporters\n+        self.settings = settings\n+        self.crawler = crawler\n         # flags\n         self.itemcount = 0\n         self._exporting = False\n+        self._fileloaded = False\n \n     def start_exporting(self):\n+        if not self._fileloaded:\n+            self.file = self.storage.open(self.spider)\n+            if \"postprocessing\" in self.feed_options:\n+                self.file = PostProcessingManager(\n+                    self.feed_options[\"postprocessing\"], self.file, self.feed_options\n+                )\n+            self.exporter = self._get_exporter(\n+                file=self.file,\n+                format=self.feed_options[\"format\"],\n+                fields_to_export=self.feed_options[\"fields\"],\n+                encoding=self.feed_options[\"encoding\"],\n+                indent=self.feed_options[\"indent\"],\n+                **self.feed_options[\"item_export_kwargs\"],\n+            )\n+            self._fileloaded = True\n+\n         if not self._exporting:\n             self.exporter.start_exporting()\n             self._exporting = True\n \n+    def _get_instance(self, objcls, *args, **kwargs):\n+        return create_instance(objcls, self.settings, self.crawler, *args, **kwargs)\n+\n+    def _get_exporter(self, file, format, *args, **kwargs):\n+        return self._get_instance(self.exporters[format], file, *args, **kwargs)\n+\n     def finish_exporting(self):\n         if self._exporting:\n             self.exporter.finish_exporting()\n@@ -406,11 +438,16 @@ def get_file(slot_):\n                 return slot_.file.file\n             return slot_.file\n \n-        slot.finish_exporting()\n-        if not slot.itemcount and not slot.store_empty:\n-            # We need to call slot.storage.store nonetheless to get the file\n-            # properly closed.\n-            return defer.maybeDeferred(slot.storage.store, get_file(slot))\n+        if slot.itemcount:\n+            # Normal case\n+            slot.finish_exporting()\n+        elif slot.store_empty and slot.batch_id == 1:\n+            # Need to store the empty file\n+            slot.start_exporting()\n+            slot.finish_exporting()\n+        else:\n+            # In this case, the file is not stored, so no processing is required.\n+            return None\n \n         logmsg = f\"{slot.format} feed ({slot.itemcount} items) in: {slot.uri}\"\n         d = defer.maybeDeferred(slot.storage.store, get_file(slot))\n@@ -455,23 +492,7 @@ def _start_new_batch(self, batch_id, uri, feed_options, spider, uri_template):\n         :param uri_template: template of uri which contains %(batch_time)s or %(batch_id)d to create new uri\n         \"\"\"\n         storage = self._get_storage(uri, feed_options)\n-        file = storage.open(spider)\n-        if \"postprocessing\" in feed_options:\n-            file = PostProcessingManager(\n-                feed_options[\"postprocessing\"], file, feed_options\n-            )\n-\n-        exporter = self._get_exporter(\n-            file=file,\n-            format=feed_options[\"format\"],\n-            fields_to_export=feed_options[\"fields\"],\n-            encoding=feed_options[\"encoding\"],\n-            indent=feed_options[\"indent\"],\n-            **feed_options[\"item_export_kwargs\"],\n-        )\n         slot = FeedSlot(\n-            file=file,\n-            exporter=exporter,\n             storage=storage,\n             uri=uri,\n             format=feed_options[\"format\"],\n@@ -479,9 +500,12 @@ def _start_new_batch(self, batch_id, uri, feed_options, spider, uri_template):\n             batch_id=batch_id,\n             uri_template=uri_template,\n             filter=self.filters[uri_template],\n+            feed_options=feed_options,\n+            spider=spider,\n+            exporters=self.exporters,\n+            settings=self.settings,\n+            crawler=getattr(self, \"crawler\", None),\n         )\n-        if slot.store_empty:\n-            slot.start_exporting()\n         return slot\n \n     def item_scraped(self, item, spider):\n@@ -565,14 +589,6 @@ def _storage_supported(self, uri, feed_options):\n         else:\n             logger.error(\"Unknown feed storage scheme: %(scheme)s\", {\"scheme\": scheme})\n \n-    def _get_instance(self, objcls, *args, **kwargs):\n-        return create_instance(\n-            objcls, self.settings, getattr(self, \"crawler\", None), *args, **kwargs\n-        )\n-\n-    def _get_exporter(self, file, format, *args, **kwargs):\n-        return self._get_instance(self.exporters[format], file, *args, **kwargs)\n-\n     def _get_storage(self, uri, feed_options):\n         \"\"\"Fork of create_instance specific to feed storage classes\n \ndiff --git a/scrapy/settings/default_settings.py b/scrapy/settings/default_settings.py\nindex 260ec1701c7..ea63d35c52b 100644\n--- a/scrapy/settings/default_settings.py\n+++ b/scrapy/settings/default_settings.py\n@@ -141,7 +141,7 @@\n FEED_TEMPDIR = None\n FEEDS = {}\n FEED_URI_PARAMS = None  # a function to extend uri arguments\n-FEED_STORE_EMPTY = False\n+FEED_STORE_EMPTY = True\n FEED_EXPORT_ENCODING = None\n FEED_EXPORT_FIELDS = None\n FEED_STORAGES = {}\n",
    "test_patch": "diff --git a/tests/test_feedexport.py b/tests/test_feedexport.py\nindex b1059099a37..faf4bd22319 100644\n--- a/tests/test_feedexport.py\n+++ b/tests/test_feedexport.py\n@@ -726,10 +726,9 @@ def run_and_export(self, spider_cls, settings):\n                 yield crawler.crawl()\n \n             for file_path, feed_options in FEEDS.items():\n-                if not Path(file_path).exists():\n-                    continue\n-\n-                content[feed_options[\"format\"]] = Path(file_path).read_bytes()\n+                content[feed_options[\"format\"]] = (\n+                    Path(file_path).read_bytes() if Path(file_path).exists() else None\n+                )\n \n         finally:\n             for file_path in FEEDS.keys():\n@@ -946,9 +945,10 @@ def test_export_no_items_not_store_empty(self):\n                 \"FEEDS\": {\n                     self._random_temp_filename(): {\"format\": fmt},\n                 },\n+                \"FEED_STORE_EMPTY\": False,\n             }\n             data = yield self.exported_no_data(settings)\n-            self.assertEqual(b\"\", data[fmt])\n+            self.assertEqual(None, data[fmt])\n \n     @defer.inlineCallbacks\n     def test_start_finish_exporting_items(self):\n@@ -1064,8 +1064,7 @@ def test_export_no_items_multiple_feeds(self):\n         with LogCapture() as log:\n             yield self.exported_no_data(settings)\n \n-        print(log)\n-        self.assertEqual(str(log).count(\"Storage.store is called\"), 3)\n+        self.assertEqual(str(log).count(\"Storage.store is called\"), 0)\n \n     @defer.inlineCallbacks\n     def test_export_multiple_item_classes(self):\n@@ -1732,10 +1731,9 @@ def run_and_export(self, spider_cls, settings):\n                 yield crawler.crawl()\n \n             for file_path, feed_options in FEEDS.items():\n-                if not Path(file_path).exists():\n-                    continue\n-\n-                content[str(file_path)] = Path(file_path).read_bytes()\n+                content[str(file_path)] = (\n+                    Path(file_path).read_bytes() if Path(file_path).exists() else None\n+                )\n \n         finally:\n             for file_path in FEEDS.keys():\n@@ -2236,6 +2234,9 @@ def run_and_export(self, spider_cls, settings):\n \n             for path, feed in FEEDS.items():\n                 dir_name = Path(path).parent\n+                if not dir_name.exists():\n+                    content[feed[\"format\"]] = []\n+                    continue\n                 for file in sorted(dir_name.iterdir()):\n                     content[feed[\"format\"]].append(file.read_bytes())\n         finally:\n@@ -2419,10 +2420,11 @@ def test_export_no_items_not_store_empty(self):\n                     / self._file_mark: {\"format\": fmt},\n                 },\n                 \"FEED_EXPORT_BATCH_ITEM_COUNT\": 1,\n+                \"FEED_STORE_EMPTY\": False,\n             }\n             data = yield self.exported_no_data(settings)\n             data = dict(data)\n-            self.assertEqual(b\"\", data[fmt][0])\n+            self.assertEqual(0, len(data[fmt]))\n \n     @defer.inlineCallbacks\n     def test_export_no_items_store_empty(self):\n@@ -2536,9 +2538,6 @@ def test_batch_item_count_feeds_setting(self):\n             for expected_batch, got_batch in zip(expected, data[fmt]):\n                 self.assertEqual(expected_batch, got_batch)\n \n-    @pytest.mark.skipif(\n-        sys.platform == \"win32\", reason=\"Odd behaviour on file creation/output\"\n-    )\n     @defer.inlineCallbacks\n     def test_batch_path_differ(self):\n         \"\"\"\n@@ -2560,7 +2559,7 @@ def test_batch_path_differ(self):\n             \"FEED_EXPORT_BATCH_ITEM_COUNT\": 1,\n         }\n         data = yield self.exported_data(items, settings)\n-        self.assertEqual(len(items), len([_ for _ in data[\"json\"] if _]))\n+        self.assertEqual(len(items), len(data[\"json\"]))\n \n     @defer.inlineCallbacks\n     def test_stats_batch_file_success(self):\n@@ -2647,7 +2646,7 @@ def parse(self, response):\n             crawler = get_crawler(TestSpider, settings)\n             yield crawler.crawl()\n \n-        self.assertEqual(len(CustomS3FeedStorage.stubs), len(items) + 1)\n+        self.assertEqual(len(CustomS3FeedStorage.stubs), len(items))\n         for stub in CustomS3FeedStorage.stubs[:-1]:\n             stub.assert_no_pending_responses()\n \n",
    "problem_statement": "FileFeedStorage creates empty file when no items are scraped\nWhen no items are scraped, the corresponding file is created none the less, because it is created in by the `storage.open` call in `FeedExporter.open_spider`. This behavior ignores the the setting of `FEED_STORE_EMPTY` when using file export.\n\nMy proposal for this would be to add a `cleanup` method to the `IFeedStorage` interface. Then  `FeedExporter.close_spider` can call that method before returning in case `slot.itemcount` is zero and `self.store_empty` is `False`. `cleanup` could also be called internally from the `store` methods of the `IFeedStorage` implementations.\n\nAddress BatchDeliveriesTest.test_batch_path_differ failures on Windows\nAfter https://github.com/scrapy/scrapy/pull/5632, the only remaining test failure seems to be that of `BatchDeliveriesTest.test_batch_path_differ`, which seems to be failing in Windows for Python 3.7 (and pinned dependencies), Python 3.9+, but not Python 3.8, for some reason.\r\n\r\nIt is also weird that the error seems to suggest an extra item is yielded (4 != 3) in those scenarios. I cannot imagine what could cause an extra item yield in a specific platform.\n",
    "hints_text": "@gbirke I have met the same problem, there are so many empty files when you save data to files, and your proposal sounds like a good idea to me.  `IFeedStorage` need to be added a new method called in `close_spider` of `FeedExporter`\n\nI'm having a similar issue, where the `JSONItemExporter` will write out the opening `[` when calling it's `start_exporting` method but if there are no items scraped, it's `finish_exporting` method never gets called, and the closing `]` is never added, so any resources consuming that JSON file will throw errors because of the invalid JSON format. \n\nThe bug has not been resolved yet. I don't believe the resolution proposed in the PR #2258 was sufficient. Although it may fix the issue of broken JSON, the problem of FileFeedStorage creating empty files remains.\r\nThis is because FileFeedStorage.open open the actual file within it, which generates the file even if there is no writing happening.\r\nIn my opinion, one of the following is necessary:\r\n1. Only open the file when the first item is received, similar to how start_exporting works.\r\n2. Make FileFeedStorage use a TemporaryFile like BlockingFeedStorage does.\nSo I hope this issue will be reopened.\nhttps://github.com/scrapy/scrapy/pull/916#issuecomment-247939197\r\n> The problem with removing file is that it doesn't play nicely with the default behavior of appending to the result.\r\n\r\nWhile removing the file may cause problems with appending, I think delaying the direct opening of the file would  be able to work well with the appending behavior and prevent the creation of an empty file.\nSince there doesn't seem to be much demand for the patch, I am thinking of making my own patch. However, I am not sure which of the two solutions suggested earlier is better. \r\nCreating a temporary file and writing to it seems like unnecessary overhead. On the other hand, opening the file for the first time at the moment of writing and trying to write to it would require a major change in logic and look bad.\nMy gut says postponing file creation until there is an item to write is the way to go, however ugly it might be. But my opinion may change depending on how ugly things can get :sweat_smile: \nLet's see how the return value of IFeedStorage.open() is used.\r\nhttps://github.com/scrapy/scrapy/blob/8fbebfa943c3352f5ba49f46531a6ccdd0b52b60/scrapy/extensions/feedexport.py#L426-L450\r\nHere is the first problem.\r\nThe _FeedSlot is a private class within feedexporter, so it is easy to fix. ItemExporter, on the other hand, is a class outside of the module. I don't yet know the inner workings of scrapy, so I don't know how much modification to the source code would be required to change the interface of the ItemExporter.\nThen it seems like a simple answer to let _FeedSlot manage whether the file and ItemExporter are loaded.\r\nThe _FeedSlot is given the necessary settings to open the file and initialize the ItemExporter in the constructor, and then processes them when the file or ItemExporter is first requested.\nI have finished creating the fix and am checking for conflicts with the test, but in doing so I am not sure if this should be done as a \"fix\".\r\nIs this something that should be a destructive change to FEED_STORE_EMPTY as a bug...? Or should I create a new option to not export files when empty feeds?\r\n\r\nAt least the docs.\r\n\r\n> FEED_STORE_EMPTY\r\n> Default: False\r\n> Whether to export empty feeds (i.e. feeds with no items).\r\n\r\nI feel this part is one of the reasons for this confusion.\r\nWhat does \"Whether\" mean?\r\nWrite out an empty feed or nothing?\r\nDoes \"Nothing\" mean no file is created? Does it mean an empty file is created?\nTo me the documentation reads clearly enough: an empty feed is a feed with no items, and `FEED_STORE_EMPTY=False` should cause such feeds to not be exported.\r\n\r\nThat doesn\u2019t mean we cannot improve it, e.g. by explicitly saying that a file without items is not even created. But it means that it feels like considering the creation of a file a bug when `FEED_STORE_EMPTY=False` seems OK to me.\nIf the current behavior is a bug, then I feel it is more necessary to fix the behavior of other features as well.\r\nThis fix results in the command line option -O or --overwrite-output no longer creating an empty file, which causes the test to fail.\r\nIt is easy to modify the test to a test case such that the item exists, but I feel that is not testing an edge case.\r\nWhen fixing this Issue as a bug, it feels more natural to implicitly set FEED_STORE_EMPTY=True when --output or --overwrite-output, or to set the default for FEED_STORE_EMPTY to True. What would you suggest?\r\n\n> It feels more natural to implicitly set FEED_STORE_EMPTY=True when --output or --overwrite-output, or to set the default for FEED_STORE_EMPTY to True. What would you suggest?\r\n\r\nThat\u2019s a very good question, and I am not entirely sure of what would be best. I think it may make sense to do that, but only for `-O`, not for `-o`, and in any case it should be possible to override with `-s FEED_STORE_EMPTY=False`.\r\n\r\nChanging the global default may not be good, since it would be a backward incompatible case\u2026 unless there is no single scenario where this bug does not happen, in which case `FEED_STORE_EMPTY=True` has been the de-facto default at least since the bug was introduced, and making it the actual default as we fix the bug would be *less* backward-incompatible.\r\n\r\n@kmike @wRAR Any thoughts?\n>  unless there is no single scenario where this bug does not happen, in which case FEED_STORE_EMPTY=True has been the de-facto default at least since the bug was introduced,\r\n\r\nA modest change in behavior will occur.\r\nWhereas before a completely empty file was generated, that modification will now generate a formatted empty representation, e.g., [] for json, xml declaration and empty elements for xml, etc.\r\nHowever, my personal opinion is that empty files were never really expected in the first place.\nApparently, I can indirectly solve #5633 and directly solve [the issue mentioned in this PR](https://github.com/scrapy/scrapy/pull/5639#issuecomment-1259655714) while solving this issue.\r\nThe creation of empty files during batch testing is essentially the same problem as this issue, and could be solved at the same time by adding an exception to match the batch process.\r\nSince the empty file creation will no longer occur, there is no need for Windows exception handling related to this behavior.\n",
    "created_at": "2023-03-08T14:14:54Z",
    "version": "2.9",
    "PASS_TO_PASS": [],
    "FAIL_TO_PASS": [
      "tests/test_feedexport.py::FeedExportTest::test_export_no_items_not_store_empty",
      "tests/test_feedexport.py::BatchDeliveriesTest::test_export_no_items_not_store_empty",
      "tests/test_feedexport.py::BatchDeliveriesTest::test_batch_path_differ",
      "tests/test_feedexport.py::BatchDeliveriesTest::test_s3_export"
    ]
  },
  {
    "repo": "scrapy/scrapy",
    "pull_number": 5833,
    "instance_id": "scrapy__scrapy-5833",
    "issue_numbers": [
      "960",
      "960",
      "5735"
    ],
    "base_commit": "d60b4edd11436e61284615ec7ce89f8ac7e46d9a",
    "patch": "diff --git a/docs/topics/feed-exports.rst b/docs/topics/feed-exports.rst\nindex eef0bb5ca89..5eea6aaf9cd 100644\n--- a/docs/topics/feed-exports.rst\n+++ b/docs/topics/feed-exports.rst\n@@ -101,12 +101,12 @@ The storages backends supported out of the box are:\n \n -   :ref:`topics-feed-storage-fs`\n -   :ref:`topics-feed-storage-ftp`\n--   :ref:`topics-feed-storage-s3` (requires botocore_)\n+-   :ref:`topics-feed-storage-s3` (requires boto3_)\n -   :ref:`topics-feed-storage-gcs` (requires `google-cloud-storage`_)\n -   :ref:`topics-feed-storage-stdout`\n \n Some storage backends may be unavailable if the required external libraries are\n-not available. For example, the S3 backend is only available if the botocore_\n+not available. For example, the S3 backend is only available if the boto3_\n library is installed.\n \n \n@@ -193,7 +193,7 @@ The feeds are stored on `Amazon S3`_.\n \n     -   ``s3://aws_key:aws_secret@mybucket/path/to/export.csv``\n \n--   Required external libraries: `botocore`_ >= 1.4.87\n+-   Required external libraries: `boto3`_ >= 1.20.0\n \n The AWS credentials can be passed as user/password in the URI, or they can be\n passed through the following settings:\n@@ -779,6 +779,6 @@ source spider in the feed URI:\n \n .. _URIs: https://en.wikipedia.org/wiki/Uniform_Resource_Identifier\n .. _Amazon S3: https://aws.amazon.com/s3/\n-.. _botocore: https://github.com/boto/botocore\n+.. _boto3: https://github.com/boto/boto3\n .. _Canned ACL: https://docs.aws.amazon.com/AmazonS3/latest/dev/acl-overview.html#canned-acl\n .. _Google Cloud Storage: https://cloud.google.com/storage/\ndiff --git a/scrapy/extensions/feedexport.py b/scrapy/extensions/feedexport.py\nindex cd26b577896..83849ca1361 100644\n--- a/scrapy/extensions/feedexport.py\n+++ b/scrapy/extensions/feedexport.py\n@@ -30,6 +30,13 @@\n \n logger = logging.getLogger(__name__)\n \n+try:\n+    import boto3  # noqa: F401\n+\n+    IS_BOTO3_AVAILABLE = True\n+except ImportError:\n+    IS_BOTO3_AVAILABLE = False\n+\n \n def build_storage(builder, uri, *args, feed_options=None, preargs=(), **kwargs):\n     argument_names = get_func_args(builder)\n@@ -173,16 +180,38 @@ def __init__(\n         self.keyname = u.path[1:]  # remove first \"/\"\n         self.acl = acl\n         self.endpoint_url = endpoint_url\n-        import botocore.session\n-\n-        session = botocore.session.get_session()\n-        self.s3_client = session.create_client(\n-            \"s3\",\n-            aws_access_key_id=self.access_key,\n-            aws_secret_access_key=self.secret_key,\n-            aws_session_token=self.session_token,\n-            endpoint_url=self.endpoint_url,\n-        )\n+\n+        if IS_BOTO3_AVAILABLE:\n+            import boto3.session\n+\n+            session = boto3.session.Session()\n+\n+            self.s3_client = session.client(\n+                \"s3\",\n+                aws_access_key_id=self.access_key,\n+                aws_secret_access_key=self.secret_key,\n+                aws_session_token=self.session_token,\n+                endpoint_url=self.endpoint_url,\n+            )\n+        else:\n+            warnings.warn(\n+                \"`botocore` usage has been deprecated for S3 feed \"\n+                \"export, please use `boto3` to avoid problems\",\n+                category=ScrapyDeprecationWarning,\n+            )\n+\n+            import botocore.session\n+\n+            session = botocore.session.get_session()\n+\n+            self.s3_client = session.create_client(\n+                \"s3\",\n+                aws_access_key_id=self.access_key,\n+                aws_secret_access_key=self.secret_key,\n+                aws_session_token=self.session_token,\n+                endpoint_url=self.endpoint_url,\n+            )\n+\n         if feed_options and feed_options.get(\"overwrite\", True) is False:\n             logger.warning(\n                 \"S3 does not support appending to files. To \"\n@@ -205,10 +234,16 @@ def from_crawler(cls, crawler, uri, *, feed_options=None):\n \n     def _store_in_thread(self, file):\n         file.seek(0)\n-        kwargs = {\"ACL\": self.acl} if self.acl else {}\n-        self.s3_client.put_object(\n-            Bucket=self.bucketname, Key=self.keyname, Body=file, **kwargs\n-        )\n+        if IS_BOTO3_AVAILABLE:\n+            kwargs = {\"ExtraArgs\": {\"ACL\": self.acl}} if self.acl else {}\n+            self.s3_client.upload_fileobj(\n+                Bucket=self.bucketname, Key=self.keyname, Fileobj=file, **kwargs\n+            )\n+        else:\n+            kwargs = {\"ACL\": self.acl} if self.acl else {}\n+            self.s3_client.put_object(\n+                Bucket=self.bucketname, Key=self.keyname, Body=file, **kwargs\n+            )\n         file.close()\n \n \ndiff --git a/tox.ini b/tox.ini\nindex 5a9d9cf29a6..80fc287355a 100644\n--- a/tox.ini\n+++ b/tox.ini\n@@ -18,8 +18,6 @@ deps =\n     mitmproxy >= 4.0.4, < 8; python_version < '3.9' and implementation_name != 'pypy'\n     # newer markupsafe is incompatible with deps of old mitmproxy (which we get on Python 3.7 and lower)\n     markupsafe < 2.1.0; python_version < '3.8' and implementation_name != 'pypy'\n-    # Extras\n-    botocore>=1.4.87\n passenv =\n     S3_TEST_FILE_URI\n     AWS_ACCESS_KEY_ID\n@@ -90,11 +88,6 @@ deps =\n \n     # mitmproxy 4.0.4+ requires upgrading some of the pinned dependencies\n     # above, hence we do not install it in pinned environments at the moment\n-\n-    # Extras\n-    botocore==1.4.87\n-    google-cloud-storage==1.29.0\n-    Pillow==7.1.0\n setenv =\n     _SCRAPY_PINNED=true\n install_command =\n@@ -121,14 +114,26 @@ setenv =\n basepython = python3\n deps =\n     {[testenv]deps}\n-    boto\n+    boto3\n     google-cloud-storage\n     # Twisted[http2] currently forces old mitmproxy because of h2 version\n     # restrictions in their deps, so we need to pin old markupsafe here too.\n     markupsafe < 2.1.0\n     robotexclusionrulesparser\n-    Pillow>=4.0.0\n-    Twisted[http2]>=17.9.0\n+    Pillow\n+    Twisted[http2]\n+\n+[testenv:extra-deps-pinned]\n+basepython = python3.7\n+deps =\n+    {[pinned]deps}\n+    boto3==1.20.0\n+    google-cloud-storage==1.29.0\n+    Pillow==7.1.0\n+    robotexclusionrulesparser==1.6.2\n+install_command = {[pinned]install_command}\n+setenv =\n+    {[pinned]setenv}\n \n [testenv:asyncio]\n commands =\n@@ -187,3 +192,24 @@ deps = {[docs]deps}\n setenv = {[docs]setenv}\n commands =\n     sphinx-build -W -b linkcheck . {envtmpdir}/linkcheck\n+\n+\n+# Run S3 tests with botocore installed but without boto3.\n+\n+[testenv:botocore]\n+deps =\n+    {[testenv]deps}\n+    botocore>=1.4.87\n+commands =\n+    pytest --cov=scrapy --cov-report=xml --cov-report= {posargs:tests -k s3}\n+\n+[testenv:botocore-pinned]\n+basepython = python3.7\n+deps =\n+    {[pinned]deps}\n+    botocore==1.4.87\n+install_command = {[pinned]install_command}\n+setenv =\n+    {[pinned]setenv}\n+commands =\n+    pytest --cov=scrapy --cov-report=xml --cov-report= {posargs:tests -k s3}\n",
    "test_patch": "diff --git a/.github/workflows/tests-ubuntu.yml b/.github/workflows/tests-ubuntu.yml\nindex 8fcf90a1814..96b26a1f89a 100644\n--- a/.github/workflows/tests-ubuntu.yml\n+++ b/.github/workflows/tests-ubuntu.yml\n@@ -37,10 +37,19 @@ jobs:\n         - python-version: pypy3.7\n           env:\n             TOXENV: pypy3-pinned\n+        - python-version: 3.7.13\n+          env:\n+            TOXENV: extra-deps-pinned\n+        - python-version: 3.7.13\n+          env:\n+            TOXENV: botocore-pinned\n \n         - python-version: \"3.11\"\n           env:\n             TOXENV: extra-deps\n+        - python-version: \"3.11\"\n+          env:\n+            TOXENV: botocore\n \n     steps:\n     - uses: actions/checkout@v3\ndiff --git a/tests/test_feedexport.py b/tests/test_feedexport.py\nindex eafe1b3342f..7df3e6dd3d3 100644\n--- a/tests/test_feedexport.py\n+++ b/tests/test_feedexport.py\n@@ -35,6 +35,7 @@\n from scrapy.exceptions import NotConfigured, ScrapyDeprecationWarning\n from scrapy.exporters import CsvItemExporter, JsonItemExporter\n from scrapy.extensions.feedexport import (\n+    IS_BOTO3_AVAILABLE,\n     BlockingFeedStorage,\n     FeedExporter,\n     FileFeedStorage,\n@@ -235,8 +236,10 @@ def test_invalid_folder(self):\n \n \n class S3FeedStorageTest(unittest.TestCase):\n-    def test_parse_credentials(self):\n+    def setUp(self):\n         skip_if_no_boto()\n+\n+    def test_parse_credentials(self):\n         aws_credentials = {\n             \"AWS_ACCESS_KEY_ID\": \"settings_key\",\n             \"AWS_SECRET_ACCESS_KEY\": \"settings_secret\",\n@@ -272,8 +275,6 @@ def test_parse_credentials(self):\n \n     @defer.inlineCallbacks\n     def test_store(self):\n-        skip_if_no_boto()\n-\n         settings = {\n             \"AWS_ACCESS_KEY_ID\": \"access_key\",\n             \"AWS_SECRET_ACCESS_KEY\": \"secret_key\",\n@@ -285,30 +286,39 @@ def test_store(self):\n         verifyObject(IFeedStorage, storage)\n \n         file = mock.MagicMock()\n-        from botocore.stub import Stubber\n-\n-        with Stubber(storage.s3_client) as stub:\n-            stub.add_response(\n-                \"put_object\",\n-                expected_params={\n-                    \"Body\": file,\n-                    \"Bucket\": bucket,\n-                    \"Key\": key,\n-                },\n-                service_response={},\n-            )\n \n+        if IS_BOTO3_AVAILABLE:\n+            storage.s3_client = mock.MagicMock()\n             yield storage.store(file)\n-\n-            stub.assert_no_pending_responses()\n             self.assertEqual(\n-                file.method_calls,\n-                [\n-                    mock.call.seek(0),\n-                    # The call to read does not happen with Stubber\n-                    mock.call.close(),\n-                ],\n+                storage.s3_client.upload_fileobj.call_args,\n+                mock.call(Bucket=bucket, Key=key, Fileobj=file),\n             )\n+        else:\n+            from botocore.stub import Stubber\n+\n+            with Stubber(storage.s3_client) as stub:\n+                stub.add_response(\n+                    \"put_object\",\n+                    expected_params={\n+                        \"Body\": file,\n+                        \"Bucket\": bucket,\n+                        \"Key\": key,\n+                    },\n+                    service_response={},\n+                )\n+\n+                yield storage.store(file)\n+\n+                stub.assert_no_pending_responses()\n+                self.assertEqual(\n+                    file.method_calls,\n+                    [\n+                        mock.call.seek(0),\n+                        # The call to read does not happen with Stubber\n+                        mock.call.close(),\n+                    ],\n+                )\n \n     def test_init_without_acl(self):\n         storage = S3FeedStorage(\"s3://mybucket/export.csv\", \"access_key\", \"secret_key\")\n@@ -391,8 +401,7 @@ def test_from_crawler_with_endpoint_url(self):\n         self.assertEqual(storage.endpoint_url, \"https://example.com\")\n \n     @defer.inlineCallbacks\n-    def test_store_botocore_without_acl(self):\n-        skip_if_no_boto()\n+    def test_store_without_acl(self):\n         storage = S3FeedStorage(\n             \"s3://mybucket/export.csv\",\n             \"access_key\",\n@@ -404,11 +413,18 @@ def test_store_botocore_without_acl(self):\n \n         storage.s3_client = mock.MagicMock()\n         yield storage.store(BytesIO(b\"test file\"))\n-        self.assertNotIn(\"ACL\", storage.s3_client.put_object.call_args[1])\n+        if IS_BOTO3_AVAILABLE:\n+            acl = (\n+                storage.s3_client.upload_fileobj.call_args[1]\n+                .get(\"ExtraArgs\", {})\n+                .get(\"ACL\")\n+            )\n+        else:\n+            acl = storage.s3_client.put_object.call_args[1].get(\"ACL\")\n+        self.assertIsNone(acl)\n \n     @defer.inlineCallbacks\n-    def test_store_botocore_with_acl(self):\n-        skip_if_no_boto()\n+    def test_store_with_acl(self):\n         storage = S3FeedStorage(\n             \"s3://mybucket/export.csv\", \"access_key\", \"secret_key\", \"custom-acl\"\n         )\n@@ -418,9 +434,11 @@ def test_store_botocore_with_acl(self):\n \n         storage.s3_client = mock.MagicMock()\n         yield storage.store(BytesIO(b\"test file\"))\n-        self.assertEqual(\n-            storage.s3_client.put_object.call_args[1].get(\"ACL\"), \"custom-acl\"\n-        )\n+        if IS_BOTO3_AVAILABLE:\n+            acl = storage.s3_client.upload_fileobj.call_args[1][\"ExtraArgs\"][\"ACL\"]\n+        else:\n+            acl = storage.s3_client.put_object.call_args[1][\"ACL\"]\n+        self.assertEqual(acl, \"custom-acl\")\n \n     def test_overwrite_default(self):\n         with LogCapture() as log:\n@@ -888,15 +906,10 @@ def test_stats_file_failed(self):\n     @defer.inlineCallbacks\n     def test_stats_multiple_file(self):\n         settings = {\n-            \"AWS_ACCESS_KEY_ID\": \"access_key\",\n-            \"AWS_SECRET_ACCESS_KEY\": \"secret_key\",\n             \"FEEDS\": {\n                 printf_escape(path_to_url(str(self._random_temp_filename()))): {\n                     \"format\": \"json\",\n                 },\n-                \"s3://bucket/key/foo.csv\": {\n-                    \"format\": \"csv\",\n-                },\n                 \"stdout:\": {\n                     \"format\": \"xml\",\n                 },\n@@ -908,18 +921,12 @@ def test_stats_multiple_file(self):\n         self.assertIn(\n             \"feedexport/success_count/FileFeedStorage\", crawler.stats.get_stats()\n         )\n-        self.assertIn(\n-            \"feedexport/success_count/S3FeedStorage\", crawler.stats.get_stats()\n-        )\n         self.assertIn(\n             \"feedexport/success_count/StdoutFeedStorage\", crawler.stats.get_stats()\n         )\n         self.assertEqual(\n             crawler.stats.get_value(\"feedexport/success_count/FileFeedStorage\"), 1\n         )\n-        self.assertEqual(\n-            crawler.stats.get_value(\"feedexport/success_count/S3FeedStorage\"), 1\n-        )\n         self.assertEqual(\n             crawler.stats.get_value(\"feedexport/success_count/StdoutFeedStorage\"), 1\n         )\n@@ -2535,7 +2542,6 @@ def test_stats_batch_file_success(self):\n     @defer.inlineCallbacks\n     def test_s3_export(self):\n         skip_if_no_boto()\n-\n         bucket = \"mybucket\"\n         items = [\n             self.MyItem({\"foo\": \"bar1\", \"egg\": \"spam1\"}),\n@@ -2707,6 +2713,9 @@ class S3FeedStoragePreFeedOptionsTest(unittest.TestCase):\n \n     maxDiff = None\n \n+    def setUp(self):\n+        skip_if_no_boto()\n+\n     def test_init(self):\n         settings_dict = {\n             \"FEED_URI\": \"file:///tmp/foobar\",\ndiff --git a/tests/test_pipeline_files.py b/tests/test_pipeline_files.py\nindex 9701e5d4eeb..e0bcfcfeabb 100644\n--- a/tests/test_pipeline_files.py\n+++ b/tests/test_pipeline_files.py\n@@ -225,12 +225,16 @@ def file_path(self, request, response=None, info=None, item=None):\n \n \n class FilesPipelineTestCaseFieldsMixin:\n+    def setUp(self):\n+        self.tempdir = mkdtemp()\n+\n+    def tearDown(self):\n+        rmtree(self.tempdir)\n+\n     def test_item_fields_default(self):\n         url = \"http://www.example.com/files/1.txt\"\n         item = self.item_class(name=\"item1\", file_urls=[url])\n-        pipeline = FilesPipeline.from_settings(\n-            Settings({\"FILES_STORE\": \"s3://example/files/\"})\n-        )\n+        pipeline = FilesPipeline.from_settings(Settings({\"FILES_STORE\": self.tempdir}))\n         requests = list(pipeline.get_media_requests(item, None))\n         self.assertEqual(requests[0].url, url)\n         results = [(True, {\"url\": url})]\n@@ -245,7 +249,7 @@ def test_item_fields_override_settings(self):\n         pipeline = FilesPipeline.from_settings(\n             Settings(\n                 {\n-                    \"FILES_STORE\": \"s3://example/files/\",\n+                    \"FILES_STORE\": self.tempdir,\n                     \"FILES_URLS_FIELD\": \"custom_file_urls\",\n                     \"FILES_RESULT_FIELD\": \"custom_files\",\n                 }\n",
    "problem_statement": "S3 Feed Export throws boto error: \"Connection Reset By Peer\"\nPosted details on SO: http://stackoverflow.com/questions/27131693/scrapyd-s3-feed-export-connection-reset-by-peer\n\nS3 Feed Export throws boto error: \"Connection Reset By Peer\"\nPosted details on SO: http://stackoverflow.com/questions/27131693/scrapyd-s3-feed-export-connection-reset-by-peer\n\nS3 backend can't handle uploads larger than 5GB \n### Description\r\n\r\nWhen feeds larger than 5GB are sent using AWS S3 backend, I'm receiving the follow error:\r\n\r\n```bash\r\n2022-11-24 18:45:31 [scrapy.extensions.feedexport] ERROR: Error storing csv feed (55 items) in: s3://scrapy-test/large_export.csv\r\nTraceback (most recent call last):\r\n  File \"/Users/ogabrielsantos/crawler-scrapy/venv/lib/python3.10/site-packages/twisted/python/threadpool.py\", line 244, in inContext\r\n    result = inContext.theWork()  # type: ignore[attr-defined]\r\n  File \"/Users/ogabrielsantos/crawler-scrapy/venv/lib/python3.10/site-packages/twisted/python/threadpool.py\", line 260, in <lambda>\r\n    inContext.theWork = lambda: context.call(  # type: ignore[attr-defined]\r\n  File \"/Users/ogabrielsantos/crawler-scrapy/venv/lib/python3.10/site-packages/twisted/python/context.py\", line 117, in callWithContext\r\n    return self.currentContext().callWithContext(ctx, func, *args, **kw)\r\n  File \"/Users/ogabrielsantos/crawler-scrapy/venv/lib/python3.10/site-packages/twisted/python/context.py\", line 82, in callWithContext\r\n    return func(*args, **kw)\r\n  File \"/Users/ogabrielsantos/crawler-scrapy/venv/lib/python3.10/site-packages/scrapy/extensions/feedexport.py\", line 196, in _store_in_thread\r\n    self.s3_client.put_object(\r\n  File \"/Users/ogabrielsantos/crawler-scrapy/venv/lib/python3.10/site-packages/botocore/client.py\", line 530, in _api_call\r\n    return self._make_api_call(operation_name, kwargs)\r\n  File \"/Users/ogabrielsantos/crawler-scrapy/venv/lib/python3.10/site-packages/botocore/client.py\", line 960, in _make_api_call\r\n    raise error_class(parsed_response, operation_name)\r\nbotocore.exceptions.ClientError: An error occurred (EntityTooLarge) when calling the PutObject operation: Your proposed upload exceeds the maximum allowed size\r\n```\r\n\r\n### Steps to Reproduce\r\n\r\nI've write a minimum exemple code to simulate this issue:\r\n\r\n```python\r\nfrom scrapy.spiders import Spider\r\n\r\n\r\nclass LargeExportSpider(Spider):\r\n    name = \"large_export\"\r\n    start_urls = [\"http://news.ycombinator.com/\"]\r\n    custom_settings = {\r\n        \"FEEDS\": {\r\n            \"s3://scrapy-test/large_export.csv\": {\r\n                \"format\": \"csv\",\r\n            },\r\n        },\r\n    }\r\n\r\n    def parse(self, response, **kwargs):\r\n        text = \"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque iaculis odio efficitur, ultricies\"\r\n\r\n        for _ in range(0, 55):  # creates a 5.3GB csv file\r\n            yield {\"name\": \"John Doe\", \"text\": text * 1000000}\r\n\r\n```\r\n\r\n### Versions\r\n`scrapy version --verbose`:\r\n```bash\r\nScrapy       : 2.7.1\r\nlxml         : 4.9.1.0\r\nlibxml2      : 2.9.13\r\ncssselect    : 1.2.0\r\nparsel       : 1.7.0\r\nw3lib        : 2.0.1\r\nTwisted      : 22.10.0\r\nPython       : 3.10.8 (main, Oct 13 2022, 09:48:40) [Clang 14.0.0 (clang-1400.0.29.102)]\r\npyOpenSSL    : 22.1.0 (OpenSSL 3.0.5 5 Jul 2022)\r\ncryptography : 38.0.1\r\nPlatform     : macOS-13.0.1-arm64-arm-64bit\r\n```\r\n\r\n`requirements.txt`:\r\n```\r\nbotocore==1.29.16\r\nScrapy==2.7.1\r\n```\r\n\r\n### Additional context\r\n\r\nDoing some investigation, I've seen that `S3FeedStorage` uses `put_object` which, as [per documentation](https://docs.aws.amazon.com/AmazonS3/latest/userguide/upload-objects.html), has a limit of 5GB per uploaded object:\r\n\r\nhttps://github.com/scrapy/scrapy/blob/6ded3cf4cd134b615239babe28bb28c3ff524b05/scrapy/extensions/feedexport.py#L196\r\n\r\nLooks like `boto3` [already have an upload method](https://boto3.amazonaws.com/v1/documentation/api/1.16.53/guide/s3-uploading-files.html) which handles multipart files, but scrapy relies on `botocore`.\n",
    "hints_text": "Scrapy uses boto for feed exports, so it is likely a boto issue (the one you've linked at SO, https://github.com/boto/boto/issues/2207). Do you know a workaround?\n\nTo send big feed files to S3 and avoid this bug, the fix is to use multipart upload. PR #1559 used to implement this for boto2.\n\nIt would be great if someone can resurrect this WIP #1559\n\nLooking at the upstream API, it seems like implementing this change is a matter of:\r\n- Documenting the need to install boto3 for S3 support, rather than botocore.\r\n- Implement code that uploads using boto3\u2019s method instead of botocore to upload files. It seems the interfaces are similar.\r\n- If boto3 is not installed, but botocore is, fall back to the current implementation, and log a deprecation warning.\nhttps://github.com/scrapy/scrapy/pull/4077 is an interesting approach, which starts uploading to S3 right away, rather than storing the whole output on disk first, and then uploading it.\r\n\r\nBut I would rather have [a simpler approach](https://github.com/scrapy/scrapy/issues/960#issuecomment-1326872050) than none. We can always implement the https://github.com/scrapy/scrapy/pull/4077 approach afterwards.\nSee also the [workaround](https://github.com/scrapy/scrapy/issues/5735#issuecomment-1327465852) by @ogabrielsantos.\nI'd like to have a go at [the simpler approach](https://github.com/scrapy/scrapy/issues/960#issuecomment-1326872050), if that's alright.\nScrapy uses boto for feed exports, so it is likely a boto issue (the one you've linked at SO, https://github.com/boto/boto/issues/2207). Do you know a workaround?\n\nTo send big feed files to S3 and avoid this bug, the fix is to use multipart upload. PR #1559 used to implement this for boto2.\n\nIt would be great if someone can resurrect this WIP #1559\n\nLooking at the upstream API, it seems like implementing this change is a matter of:\r\n- Documenting the need to install boto3 for S3 support, rather than botocore.\r\n- Implement code that uploads using boto3\u2019s method instead of botocore to upload files. It seems the interfaces are similar.\r\n- If boto3 is not installed, but botocore is, fall back to the current implementation, and log a deprecation warning.\nhttps://github.com/scrapy/scrapy/pull/4077 is an interesting approach, which starts uploading to S3 right away, rather than storing the whole output on disk first, and then uploading it.\r\n\r\nBut I would rather have [a simpler approach](https://github.com/scrapy/scrapy/issues/960#issuecomment-1326872050) than none. We can always implement the https://github.com/scrapy/scrapy/pull/4077 approach afterwards.\nSee also the [workaround](https://github.com/scrapy/scrapy/issues/5735#issuecomment-1327465852) by @ogabrielsantos.\nI'd like to have a go at [the simpler approach](https://github.com/scrapy/scrapy/issues/960#issuecomment-1326872050), if that's alright.\nSounds related to https://github.com/scrapy/scrapy/issues/960 (at the very least, the solution seems to be the same)\nI'm workarounding it until an official fix, this way:\r\n\r\n`requirements.txt`:\r\n```\r\nboto3==1.26.16\r\n```\r\n\r\n`crawlers/feed.py`:\r\n```python\r\nclass LargeS3FeedStorage(S3FeedStorage):\r\n    def __init__(\r\n        self,\r\n        uri,\r\n        access_key=None,\r\n        secret_key=None,\r\n        acl=None,\r\n        endpoint_url=None,\r\n        *,\r\n        feed_options=None,\r\n        session_token=None,\r\n    ):\r\n        super().__init__(\r\n            uri, access_key, secret_key, acl, endpoint_url, feed_options=feed_options, session_token=session_token\r\n        )\r\n\r\n        self.s3_client = boto3.client(\r\n            \"s3\",\r\n            aws_access_key_id=self.access_key,\r\n            aws_secret_access_key=self.secret_key,\r\n            aws_session_token=self.session_token,\r\n            endpoint_url=self.endpoint_url,\r\n        )\r\n\r\n    def _store_in_thread(self, file):\r\n        file.seek(0)\r\n        self.s3_client.upload_file(Filename=file.file.name, Bucket=self.bucketname, Key=self.keyname)\r\n        file.close()\r\n```\r\n\r\n`settings.py`:\r\n```python\r\nFEED_STORAGES = {\r\n    \"s3\": \"crawlers.feed.LargeS3FeedStorage\",\r\n}\r\n```\r\n\nOOPS. Accidentally closed by private repository merge. Reopening.",
    "created_at": "2023-02-17T15:16:03Z",
    "version": "2.8",
    "PASS_TO_PASS": [],
    "FAIL_TO_PASS": [
      "tests/test_feedexport.py::FeedExportTest::test_stats_multiple_file",
      "tests/test_feedexport.py::S3FeedStorageTest::test_parse_credentials",
      "tests/test_feedexport.py::S3FeedStorageTest::test_store",
      "tests/test_feedexport.py::S3FeedStorageTest::test_store_without_acl",
      "tests/test_feedexport.py::S3FeedStorageTest::test_store_with_acl",
      "tests/test_pipeline_files.py::FilesPipelineTestCaseFieldsDict::test_item_fields_default",
      "tests/test_pipeline_files.py::FilesPipelineTestCaseFieldsDict::test_item_fields_override_settings",
      "tests/test_pipeline_files.py::FilesPipelineTestCaseFieldsItem::test_item_fields_default",
      "tests/test_pipeline_files.py::FilesPipelineTestCaseFieldsItem::test_item_fields_override_settings",
      "tests/test_pipeline_files.py::FilesPipelineTestCaseFieldsDataClass::test_item_fields_default",
      "tests/test_pipeline_files.py::FilesPipelineTestCaseFieldsDataClass::test_item_fields_override_settings",
      "tests/test_pipeline_files.py::FilesPipelineTestCaseFieldsAttrsItem::test_item_fields_default",
      "tests/test_pipeline_files.py::FilesPipelineTestCaseFieldsAttrsItem::test_item_fields_override_settings"
    ]
  },
  {
    "repo": "scrapy/scrapy",
    "pull_number": 5808,
    "instance_id": "scrapy__scrapy-5808",
    "issue_numbers": [
      "3553",
      "3558"
    ],
    "base_commit": "9411cf4e708ea60c7a6972a6804334f2a799e5c6",
    "patch": "diff --git a/docs/topics/commands.rst b/docs/topics/commands.rst\nindex 54fd5d66311..106045fc073 100644\n--- a/docs/topics/commands.rst\n+++ b/docs/topics/commands.rst\n@@ -238,9 +238,6 @@ genspider\n \n Create a new spider in the current folder or in the current project's ``spiders`` folder, if called from inside a project. The ``<name>`` parameter is set as the spider's ``name``, while ``<domain or URL>`` is used to generate the ``allowed_domains`` and ``start_urls`` spider's attributes.\n \n-.. note:: Even if an HTTPS URL is specified, the protocol used in\n-          ``start_urls`` is always HTTP. This is a known issue: :issue:`3553`.\n-\n Usage example::\n \n     $ scrapy genspider -l\ndiff --git a/scrapy/commands/genspider.py b/scrapy/commands/genspider.py\nindex c1565a13848..68cbe8ff608 100644\n--- a/scrapy/commands/genspider.py\n+++ b/scrapy/commands/genspider.py\n@@ -31,6 +31,14 @@ def extract_domain(url):\n     return o.netloc\n \n \n+def verify_url_scheme(url):\n+    \"\"\"Check url for scheme and insert https if none found.\"\"\"\n+    parsed = urlparse(url)\n+    if parsed.scheme == \"\" and parsed.netloc == \"\":\n+        parsed = urlparse(\"//\" + url)._replace(scheme=\"https\")\n+    return parsed.geturl()\n+\n+\n class Command(ScrapyCommand):\n     requires_project = False\n     default_settings = {\"LOG_ENABLED\": False}\n@@ -91,7 +99,7 @@ def run(self, args, opts):\n             raise UsageError()\n \n         name, url = args[0:2]\n-        domain = extract_domain(url)\n+        url = verify_url_scheme(url)\n         module = sanitize_module_name(name)\n \n         if self.settings.get(\"BOT_NAME\") == module:\n@@ -103,18 +111,20 @@ def run(self, args, opts):\n \n         template_file = self._find_template(opts.template)\n         if template_file:\n-            self._genspider(module, name, domain, opts.template, template_file)\n+            self._genspider(module, name, url, opts.template, template_file)\n             if opts.edit:\n                 self.exitcode = os.system(f'scrapy edit \"{name}\"')\n \n-    def _genspider(self, module, name, domain, template_name, template_file):\n+    def _genspider(self, module, name, url, template_name, template_file):\n         \"\"\"Generate the spider module, based on the given template\"\"\"\n         capitalized_module = \"\".join(s.capitalize() for s in module.split(\"_\"))\n+        domain = extract_domain(url)\n         tvars = {\n             \"project_name\": self.settings.get(\"BOT_NAME\"),\n             \"ProjectName\": string_camelcase(self.settings.get(\"BOT_NAME\")),\n             \"module\": module,\n             \"name\": name,\n+            \"url\": url,\n             \"domain\": domain,\n             \"classname\": f\"{capitalized_module}Spider\",\n         }\ndiff --git a/scrapy/templates/spiders/basic.tmpl b/scrapy/templates/spiders/basic.tmpl\nindex d3ba19553a7..20e777271ee 100644\n--- a/scrapy/templates/spiders/basic.tmpl\n+++ b/scrapy/templates/spiders/basic.tmpl\n@@ -4,7 +4,7 @@ import scrapy\n class $classname(scrapy.Spider):\n     name = \"$name\"\n     allowed_domains = [\"$domain\"]\n-    start_urls = [\"http://$domain/\"]\n+    start_urls = [\"$url\"]\n \n     def parse(self, response):\n         pass\ndiff --git a/scrapy/templates/spiders/crawl.tmpl b/scrapy/templates/spiders/crawl.tmpl\nindex 2e467e63224..36d05e43a21 100644\n--- a/scrapy/templates/spiders/crawl.tmpl\n+++ b/scrapy/templates/spiders/crawl.tmpl\n@@ -6,7 +6,7 @@ from scrapy.spiders import CrawlSpider, Rule\n class $classname(CrawlSpider):\n     name = \"$name\"\n     allowed_domains = [\"$domain\"]\n-    start_urls = [\"http://$domain/\"]\n+    start_urls = [\"$url\"]\n \n     rules = (Rule(LinkExtractor(allow=r\"Items/\"), callback=\"parse_item\", follow=True),)\n \ndiff --git a/scrapy/templates/spiders/csvfeed.tmpl b/scrapy/templates/spiders/csvfeed.tmpl\nindex ce9c1dd202a..fe96878dc5d 100644\n--- a/scrapy/templates/spiders/csvfeed.tmpl\n+++ b/scrapy/templates/spiders/csvfeed.tmpl\n@@ -4,7 +4,7 @@ from scrapy.spiders import CSVFeedSpider\n class $classname(CSVFeedSpider):\n     name = \"$name\"\n     allowed_domains = [\"$domain\"]\n-    start_urls = [\"http://$domain/feed.csv\"]\n+    start_urls = [\"$url\"]\n     #headers = [\"id\", \"name\", \"description\", \"image_link\"]\n     #delimiter = \"\\t\"\n \ndiff --git a/scrapy/templates/spiders/xmlfeed.tmpl b/scrapy/templates/spiders/xmlfeed.tmpl\nindex 6b50e4cf465..ac62d78d1c7 100644\n--- a/scrapy/templates/spiders/xmlfeed.tmpl\n+++ b/scrapy/templates/spiders/xmlfeed.tmpl\n@@ -4,7 +4,7 @@ from scrapy.spiders import XMLFeedSpider\n class $classname(XMLFeedSpider):\n     name = \"$name\"\n     allowed_domains = [\"$domain\"]\n-    start_urls = [\"http://$domain/feed.xml\"]\n+    start_urls = [\"$url\"]\n     iterator = \"iternodes\"  # you can change this; see the docs\n     itertag = \"item\"  # change it accordingly\n \n",
    "test_patch": "diff --git a/tests/test_commands.py b/tests/test_commands.py\nindex 00ddcdd3ee6..014f50e92e5 100644\n--- a/tests/test_commands.py\n+++ b/tests/test_commands.py\n@@ -541,7 +541,7 @@ def test_url(self, url=\"test.com\", domain=\"test.com\"):\n             ).group(1),\n         )\n         self.assertEqual(\n-            f\"http://{domain}/\",\n+            f\"https://{domain}\",\n             self.find_in_file(\n                 Path(self.proj_mod_path, \"spiders\", \"test_name.py\"),\n                 r\"start_urls\\s*=\\s*\\[['\\\"](.+)['\\\"]\\]\",\n@@ -549,13 +549,64 @@ def test_url(self, url=\"test.com\", domain=\"test.com\"):\n         )\n \n     def test_url_schema(self):\n-        self.test_url(\"http://test.com\", \"test.com\")\n+        self.test_url(\"https://test.com\", \"test.com\")\n \n-    def test_url_path(self):\n-        self.test_url(\"test.com/some/other/page\", \"test.com\")\n+    def test_template_start_urls(\n+        self, url=\"test.com\", expected=\"https://test.com\", template=\"basic\"\n+    ):\n+        self.assertEqual(\n+            0, self.call(\"genspider\", \"-t\", template, \"--force\", \"test_name\", url)\n+        )\n+        self.assertEqual(\n+            expected,\n+            self.find_in_file(\n+                Path(self.proj_mod_path, \"spiders\", \"test_name.py\"),\n+                r\"start_urls\\s*=\\s*\\[['\\\"](.+)['\\\"]\\]\",\n+            ).group(1),\n+        )\n+\n+    def test_genspider_basic_start_urls(self):\n+        self.test_template_start_urls(\"https://test.com\", \"https://test.com\", \"basic\")\n+        self.test_template_start_urls(\"http://test.com\", \"http://test.com\", \"basic\")\n+        self.test_template_start_urls(\n+            \"http://test.com/other/path\", \"http://test.com/other/path\", \"basic\"\n+        )\n+        self.test_template_start_urls(\n+            \"test.com/other/path\", \"https://test.com/other/path\", \"basic\"\n+        )\n \n-    def test_url_schema_path(self):\n-        self.test_url(\"https://test.com/some/other/page\", \"test.com\")\n+    def test_genspider_crawl_start_urls(self):\n+        self.test_template_start_urls(\"https://test.com\", \"https://test.com\", \"crawl\")\n+        self.test_template_start_urls(\"http://test.com\", \"http://test.com\", \"crawl\")\n+        self.test_template_start_urls(\n+            \"http://test.com/other/path\", \"http://test.com/other/path\", \"crawl\"\n+        )\n+        self.test_template_start_urls(\n+            \"test.com/other/path\", \"https://test.com/other/path\", \"crawl\"\n+        )\n+        self.test_template_start_urls(\"test.com\", \"https://test.com\", \"crawl\")\n+\n+    def test_genspider_xmlfeed_start_urls(self):\n+        self.test_template_start_urls(\n+            \"https://test.com/feed.xml\", \"https://test.com/feed.xml\", \"xmlfeed\"\n+        )\n+        self.test_template_start_urls(\n+            \"http://test.com/feed.xml\", \"http://test.com/feed.xml\", \"xmlfeed\"\n+        )\n+        self.test_template_start_urls(\n+            \"test.com/feed.xml\", \"https://test.com/feed.xml\", \"xmlfeed\"\n+        )\n+\n+    def test_genspider_csvfeed_start_urls(self):\n+        self.test_template_start_urls(\n+            \"https://test.com/feed.csv\", \"https://test.com/feed.csv\", \"csvfeed\"\n+        )\n+        self.test_template_start_urls(\n+            \"http://test.com/feed.xml\", \"http://test.com/feed.xml\", \"csvfeed\"\n+        )\n+        self.test_template_start_urls(\n+            \"test.com/feed.csv\", \"https://test.com/feed.csv\", \"csvfeed\"\n+        )\n \n \n class GenspiderStandaloneCommandTest(ProjectTest):\n",
    "problem_statement": "Genspider prepend `http://` without checking it in the `domain`\ngenspider prepend http:// But when i enter address like https://example.com it becomes http://https://example.com that, when run scrapy crawl throws an error.\r\nWhat it should do, it should first check the receiving domain than take decision according to the passing domain whether it needs a http:// or nothing.\nFix [#3553] Add_http_if_no_scheme in domain in genspider\nFixed [#3553] \r\nbefore passing the domain, use built-in function from utils.url.add_http_if_no_scheme to add the http:// before the domain.\r\nUpdated template files by removing hard coded values.\n",
    "hints_text": "Hello everybody! I would like to contribute to Scrapy, specifically with this issue and I already fork the repo and made the changes but before issuing a pull request I would like to discuss it and reach to an agreement.\r\n\r\nI made the changes over 5 files:\r\n- **scrapy/commands/genspider.py**. Where `urllib.parse.urlparse` is being used on the domain parameter and prepend `http` scheme in the case is not there. But, this is a very tricky situation because of the command syntax, which asks for a domain and not an URL, so we're just putting a very big bet that it should be HTTP but what about if we ask for a URL instead of a domain, then the contradiction is solved right away since we can extract the domain without making any assumption. Don't you think fellas?\r\n- **Each template file**. In order to receive domain and URL values.\r\nI checked the test just for tests/test_commands.py, I didn't add test because I don't know how to make a test for this particular scenario where two attributes are set based on user input on a class created through a template. If someone has an idea how it could be made please I'm all ears\r\n\r\nLastly and maybe more important, I don't know how to try the code it's my first time and I don't have any idea how to execute it. So any help or guide would be really appreciated.\r\n\r\nThank you in advance and happy coding!!!! =D\n@ambarmendez I would suggest you open a pull request already, discussions over actual code are usually easier.\r\n\r\nAlso, make sure you check out previous pull requests (#3554, #3558). Feedback there, specially @kmike\u2019s, could answer some of your points.\nThank you @Gallaecio! Let's see how it goes =D\nNow URLs are supported as input: https://github.com/scrapy/scrapy/pull/4439\r\n\r\nHowever, we still simply extract the domain from them. Which means that the input protocol is not respected for `start_urls`, and instead it is always forced to be HTTP (as opposed to, for example, HTTPS).\nhttps://github.com/scrapy/scrapy/pull/3558 seems to aim to address the `start_urls` issue. The pull request needs some work, but if not resumed, maybe it can be taken as inspiration for alternative implementations.\nHi @Gallaecio, can I be assigned to work on this issue?\n@msenior85 No need. Feel free to open a pull request and include `Fixes #3553` in its description so that it shows up in this thread.\n# [Codecov](https://codecov.io/gh/scrapy/scrapy/pull/3558?src=pr&el=h1) Report\n> Merging [#3558](https://codecov.io/gh/scrapy/scrapy/pull/3558?src=pr&el=desc) into [master](https://codecov.io/gh/scrapy/scrapy/commit/094dde6fdb1b03351888e437828af5da03f46352?src=pr&el=desc) will **decrease** coverage by `0.01%`.\n> The diff coverage is `76.92%`.\n\n\n```diff\n@@            Coverage Diff             @@\n##           master    #3558      +/-   ##\n==========================================\n- Coverage   84.48%   84.47%   -0.02%     \n==========================================\n  Files         167      167              \n  Lines        9405     9416      +11     \n  Branches     1397     1399       +2     \n==========================================\n+ Hits         7946     7954       +8     \n- Misses       1201     1202       +1     \n- Partials      258      260       +2\n```\n\n| [Impacted Files](https://codecov.io/gh/scrapy/scrapy/pull/3558?src=pr&el=tree) | Coverage \u0394 | |\n|---|---|---|\n| [scrapy/commands/genspider.py](https://codecov.io/gh/scrapy/scrapy/pull/3558/diff?src=pr&el=tree#diff-c2NyYXB5L2NvbW1hbmRzL2dlbnNwaWRlci5weQ==) | `82.47% <76.92%> (-1.25%)` | :arrow_down: |\n",
    "created_at": "2023-01-27T08:45:12Z",
    "version": "2.8",
    "PASS_TO_PASS": [],
    "FAIL_TO_PASS": [
      "tests/test_commands.py::GenspiderCommandTest::test_url",
      "tests/test_commands.py::GenspiderCommandTest::test_url_schema",
      "tests/test_commands.py::GenspiderCommandTest::test_template_start_urls",
      "tests/test_commands.py::GenspiderCommandTest::test_genspider_basic_start_urls",
      "tests/test_commands.py::GenspiderCommandTest::test_genspider_crawl_start_urls",
      "tests/test_commands.py::GenspiderCommandTest::test_genspider_xmlfeed_start_urls",
      "tests/test_commands.py::GenspiderCommandTest::test_genspider_csvfeed_start_urls"
    ]
  },
  {
    "repo": "scrapy/scrapy",
    "pull_number": 5691,
    "instance_id": "scrapy__scrapy-5691",
    "issue_numbers": [
      "5323"
    ],
    "base_commit": "b33244e2f0d877b8911f949308222db0b076d665",
    "patch": "diff --git a/scrapy/utils/misc.py b/scrapy/utils/misc.py\nindex 1221b39b229..4d4fb9600f0 100644\n--- a/scrapy/utils/misc.py\n+++ b/scrapy/utils/misc.py\n@@ -226,7 +226,14 @@ def returns_none(return_node):\n         return value is None or isinstance(value, ast.NameConstant) and value.value is None\n \n     if inspect.isgeneratorfunction(callable):\n-        code = re.sub(r\"^[\\t ]+\", \"\", inspect.getsource(callable))\n+        src = inspect.getsource(callable)\n+        pattern = re.compile(r\"(^[\\t ]+)\")\n+        code = pattern.sub(\"\", src)\n+\n+        match = pattern.match(src)  # finds indentation\n+        if match:\n+            code = re.sub(f\"\\n{match.group(0)}\", \"\\n\", code)  # remove indentation\n+\n         tree = ast.parse(code)\n         for node in walk_callable(tree):\n             if isinstance(node, ast.Return) and not returns_none(node):\n",
    "test_patch": "diff --git a/tests/test_utils_misc/test_return_with_argument_inside_generator.py b/tests/test_utils_misc/test_return_with_argument_inside_generator.py\nindex 1c85ca35369..72277d70184 100644\n--- a/tests/test_utils_misc/test_return_with_argument_inside_generator.py\n+++ b/tests/test_utils_misc/test_return_with_argument_inside_generator.py\n@@ -165,6 +165,89 @@ def l2():\n             warn_on_generator_with_return_value(None, l2)\n             self.assertEqual(len(w), 0)\n \n+    def test_generators_return_none_with_decorator(self):\n+        def decorator(func):\n+            def inner_func():\n+                func()\n+            return inner_func\n+\n+        @decorator\n+        def f3():\n+            yield 1\n+            return None\n+\n+        @decorator\n+        def g3():\n+            yield 1\n+            return\n+\n+        @decorator\n+        def h3():\n+            yield 1\n+\n+        @decorator\n+        def i3():\n+            yield 1\n+            yield from generator_that_returns_stuff()\n+\n+        @decorator\n+        def j3():\n+            yield 1\n+\n+            def helper():\n+                return 0\n+\n+            yield helper()\n+\n+        @decorator\n+        def k3():\n+            \"\"\"\n+docstring\n+            \"\"\"\n+            url = \"\"\"\n+https://example.org\n+        \"\"\"\n+            yield url\n+            return\n+\n+        @decorator\n+        def l3():\n+            return\n+\n+        assert not is_generator_with_return_value(top_level_return_none)\n+        assert not is_generator_with_return_value(f3)\n+        assert not is_generator_with_return_value(g3)\n+        assert not is_generator_with_return_value(h3)\n+        assert not is_generator_with_return_value(i3)\n+        assert not is_generator_with_return_value(j3)  # not recursive\n+        assert not is_generator_with_return_value(k3)  # not recursive\n+        assert not is_generator_with_return_value(l3)\n+\n+        with warnings.catch_warnings(record=True) as w:\n+            warn_on_generator_with_return_value(None, top_level_return_none)\n+            self.assertEqual(len(w), 0)\n+        with warnings.catch_warnings(record=True) as w:\n+            warn_on_generator_with_return_value(None, f3)\n+            self.assertEqual(len(w), 0)\n+        with warnings.catch_warnings(record=True) as w:\n+            warn_on_generator_with_return_value(None, g3)\n+            self.assertEqual(len(w), 0)\n+        with warnings.catch_warnings(record=True) as w:\n+            warn_on_generator_with_return_value(None, h3)\n+            self.assertEqual(len(w), 0)\n+        with warnings.catch_warnings(record=True) as w:\n+            warn_on_generator_with_return_value(None, i3)\n+            self.assertEqual(len(w), 0)\n+        with warnings.catch_warnings(record=True) as w:\n+            warn_on_generator_with_return_value(None, j3)\n+            self.assertEqual(len(w), 0)\n+        with warnings.catch_warnings(record=True) as w:\n+            warn_on_generator_with_return_value(None, k3)\n+            self.assertEqual(len(w), 0)\n+        with warnings.catch_warnings(record=True) as w:\n+            warn_on_generator_with_return_value(None, l3)\n+            self.assertEqual(len(w), 0)\n+\n     @mock.patch(\"scrapy.utils.misc.is_generator_with_return_value\", new=_indentation_error)\n     def test_indentation_error(self):\n         with warnings.catch_warnings(record=True) as w:\n",
    "problem_statement": "Warning: Unable to determine whether or not callable is a generator with a return value [unable to parse decorated class methods]\n### Description\r\n\r\nIf you have a decorated spider method, `scrapy.utils.misc` [throws a warning](https://github.com/scrapy/scrapy/blob/master/scrapy/utils/misc.py#L240-L264) saying that it cannot determine if the callable is a generator with a return value.\r\n\r\n`ast.parse()` fails [here](https://github.com/scrapy/scrapy/blob/master/scrapy/utils/misc.py#L228-L230) when called with a decorated method.\r\n\r\n### Steps to Reproduce\r\n\r\nI just copied the logic from `misc.py` and used it to analyze a class with the same overall code structure:\r\n\r\n```python\r\nimport re\r\nimport ast\r\nimport inspect\r\n\r\nclass Foo:\r\n    @classmethod\r\n    def func(self):\r\n        \"\"\"\r\n        Description of func\r\n        \"\"\"  \r\n        return\r\n\r\ncode = re.sub(r\"^[\\t ]+\", \"\", inspect.getsource(Foo.func))\r\ntree = ast.parse(code)\r\n```\r\n\r\n```\r\npython3 test.py\r\n> IndentationError: unexpected indent\r\n```\r\n\r\nThe regex replacement isn't accounting for a possible decorator, so the code ends up looking like:\r\n\r\n```\r\n@classmethod\r\n    def func(self):\r\n        \"\"\"\r\n        Description of func\r\n        \"\"\"  \r\n        return\r\n```\r\n\r\n**Expected behavior:** I'd like to be able to use decorated methods without dealing with noisy logs.\r\n\r\n**Actual behavior:** My container logs are filled with tons of warning messages. The only workaround is to avoid the usage of decorators.\r\n\r\n**Reproduces how often:** 100% of the time to my knowledge\r\n\r\n### Versions\r\n\r\n```\r\n$ scrapy version --verbose\r\n\r\nScrapy       : 2.5.1\r\nlxml         : 4.6.4.0\r\nlibxml2      : 2.9.10\r\ncssselect    : 1.1.0\r\nparsel       : 1.6.0\r\nw3lib        : 1.22.0\r\nTwisted      : 21.7.0\r\nPython       : 3.8.2 (default, Dec 21 2020, 15:06:04) - [Clang 12.0.0 (clang-1200.0.32.29)]\r\npyOpenSSL    : 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021)\r\ncryptography : 35.0.0\r\nPlatform     : macOS-10.15.7-x86_64-i386-64bit\r\n```\r\n\r\n### Additional context\r\n\r\nThis is my first time filing a Scrapy issue. I'm happy to add more context if necessary, and apologies in advance if this has already been discussed elsewhere (fwiw I couldn't find anything).\n",
    "hints_text": "When this features was introduced in #3869, it used `dedent`, which would have avoided this issue. In #4935, this was changed to a simple regex, because apparently some people write code like:\r\n\r\n```python\r\nclass Bob:\r\n    def doit(self):\r\n        \"\"\"\r\nthis line is flush left\r\n        \"\"\"\r\n        if True:\r\n            yield 1234\r\n```\r\n\r\nAnyway, to support both decorators and non-indented heredocs, then the regex needs to be made smarter.\n```python test.py\r\nimport re\r\nimport ast\r\nimport inspect\r\n\r\nclass Foo:\r\n    @classmethod\r\n    def func(self):\r\n        \"\"\"\r\n        Description of func\r\n        \"\"\"  \r\n        return\r\n\r\n    @classmethod\r\n    def test(self):\r\n        \"\"\"\r\n    Description of test\r\n        \"\"\"\r\n        return\r\n\r\n\r\ndef parse(func):\r\n    src = inspect.getsource(func)\r\n    pattern = r\"^[\\t ]+\"\r\n    matches = re.findall(pattern, src)  # finds indentation\r\n    code = re.sub(pattern, \"\", src)\r\n    if matches:\r\n        code = re.sub(f\"\\n{matches[0]}\", \"\\n\", code)  # remove indentation\r\n\r\n    tree = ast.parse(code)\r\n\r\nparse(Foo.func)\r\nparse(Foo.test)\r\nparse(parse)\r\n```\r\n\r\nFinding indentation of callable and removing it works.",
    "created_at": "2022-10-25T03:26:33Z",
    "version": "2.7",
    "PASS_TO_PASS": [],
    "FAIL_TO_PASS": [
      "tests/test_utils_misc/test_return_with_argument_inside_generator.py::UtilsMiscPy3TestCase::test_generators_return_none_with_decorator"
    ]
  },
  {
    "repo": "scrapy/scrapy",
    "pull_number": 5689,
    "instance_id": "scrapy__scrapy-5689",
    "issue_numbers": [
      "5685",
      "5685"
    ],
    "base_commit": "92be5ba2572ec14e2580abe12d276e8aa24247b6",
    "patch": "diff --git a/scrapy/utils/defer.py b/scrapy/utils/defer.py\nindex 8fcf31cab54..38aefd6d02a 100644\n--- a/scrapy/utils/defer.py\n+++ b/scrapy/utils/defer.py\n@@ -26,7 +26,7 @@\n from twisted.python.failure import Failure\n \n from scrapy.exceptions import IgnoreRequest\n-from scrapy.utils.reactor import is_asyncio_reactor_installed\n+from scrapy.utils.reactor import is_asyncio_reactor_installed, get_asyncio_event_loop_policy\n \n \n def defer_fail(_failure: Failure) -> Deferred:\n@@ -269,7 +269,8 @@ def deferred_from_coro(o) -> Any:\n             return ensureDeferred(o)\n         else:\n             # wrapping the coroutine into a Future and then into a Deferred, this requires AsyncioSelectorReactor\n-            return Deferred.fromFuture(asyncio.ensure_future(o))\n+            event_loop = get_asyncio_event_loop_policy().get_event_loop()\n+            return Deferred.fromFuture(asyncio.ensure_future(o, loop=event_loop))\n     return o\n \n \n@@ -320,7 +321,8 @@ async def parse(self, response):\n                 d = treq.get('https://example.com/additional')\n                 additional_response = await deferred_to_future(d)\n     \"\"\"\n-    return d.asFuture(asyncio.get_event_loop())\n+    policy = get_asyncio_event_loop_policy()\n+    return d.asFuture(policy.get_event_loop())\n \n \n def maybe_deferred_to_future(d: Deferred) -> Union[Deferred, Future]:\ndiff --git a/scrapy/utils/reactor.py b/scrapy/utils/reactor.py\nindex 652733ce8b3..ddf354d886e 100644\n--- a/scrapy/utils/reactor.py\n+++ b/scrapy/utils/reactor.py\n@@ -51,6 +51,19 @@ def __call__(self):\n         return self._func(*self._a, **self._kw)\n \n \n+def get_asyncio_event_loop_policy():\n+    policy = asyncio.get_event_loop_policy()\n+    if (\n+        sys.version_info >= (3, 8)\n+        and sys.platform == \"win32\"\n+        and not isinstance(policy, asyncio.WindowsSelectorEventLoopPolicy)\n+    ):\n+        policy = asyncio.WindowsSelectorEventLoopPolicy()\n+        asyncio.set_event_loop_policy(policy)\n+\n+    return policy\n+\n+\n def install_reactor(reactor_path, event_loop_path=None):\n     \"\"\"Installs the :mod:`~twisted.internet.reactor` with the specified\n     import path. Also installs the asyncio event loop with the specified import\n@@ -58,16 +71,14 @@ def install_reactor(reactor_path, event_loop_path=None):\n     reactor_class = load_object(reactor_path)\n     if reactor_class is asyncioreactor.AsyncioSelectorReactor:\n         with suppress(error.ReactorAlreadyInstalledError):\n-            if sys.version_info >= (3, 8) and sys.platform == \"win32\":\n-                policy = asyncio.get_event_loop_policy()\n-                if not isinstance(policy, asyncio.WindowsSelectorEventLoopPolicy):\n-                    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())\n+            policy = get_asyncio_event_loop_policy()\n             if event_loop_path is not None:\n                 event_loop_class = load_object(event_loop_path)\n                 event_loop = event_loop_class()\n                 asyncio.set_event_loop(event_loop)\n             else:\n-                event_loop = asyncio.get_event_loop()\n+                event_loop = policy.get_event_loop()\n+\n             asyncioreactor.install(eventloop=event_loop)\n     else:\n         *module, _ = reactor_path.split(\".\")\n",
    "test_patch": "diff --git a/tests/test_utils_asyncio.py b/tests/test_utils_asyncio.py\nindex 295323e4daa..741c6a5051b 100644\n--- a/tests/test_utils_asyncio.py\n+++ b/tests/test_utils_asyncio.py\n@@ -1,3 +1,4 @@\n+import warnings\n from unittest import TestCase\n \n from pytest import mark\n@@ -13,5 +14,6 @@ def test_is_asyncio_reactor_installed(self):\n         self.assertEqual(is_asyncio_reactor_installed(), self.reactor_pytest == 'asyncio')\n \n     def test_install_asyncio_reactor(self):\n-        # this should do nothing\n-        install_reactor(\"twisted.internet.asyncioreactor.AsyncioSelectorReactor\")\n+        with warnings.catch_warnings(record=True) as w:\n+            install_reactor(\"twisted.internet.asyncioreactor.AsyncioSelectorReactor\")\n+            self.assertEqual(len(w), 0)\n",
    "problem_statement": "DeprecationWarning: There is no current event loop\n```\r\n/home/runner/work/scrapy/scrapy/scrapy/utils/reactor.py:70: DeprecationWarning: There is no current event loop\r\n  event_loop = asyncio.get_event_loop()\r\ntests/test_downloadermiddleware.py: 2 warnings\r\ntests/test_utils_asyncgen.py: 2 warnings\r\ntests/test_utils_defer.py: 4 warnings\r\ntests/test_utils_python.py: 2 warnings\r\ntests/test_utils_signal.py: 2 warnings\r\n  /home/runner/work/scrapy/scrapy/scrapy/utils/defer.py:272: DeprecationWarning: There is no current event loop\r\n    return Deferred.fromFuture(asyncio.ensure_future(o))\r\ntests/test_utils_asyncio.py::AsyncioTest::test_install_asyncio_reactor\r\n  /home/runner/work/scrapy/scrapy/scrapy/utils/reactor.py:70: DeprecationWarning: There is no current event loop\r\n    event_loop = asyncio.get_event_loop()\r\n```\r\n\r\nNot sure yet what does this imply and when can this break (this is from tests on 3.10 and 3.11).\nDeprecationWarning: There is no current event loop\n```\r\n/home/runner/work/scrapy/scrapy/scrapy/utils/reactor.py:70: DeprecationWarning: There is no current event loop\r\n  event_loop = asyncio.get_event_loop()\r\ntests/test_downloadermiddleware.py: 2 warnings\r\ntests/test_utils_asyncgen.py: 2 warnings\r\ntests/test_utils_defer.py: 4 warnings\r\ntests/test_utils_python.py: 2 warnings\r\ntests/test_utils_signal.py: 2 warnings\r\n  /home/runner/work/scrapy/scrapy/scrapy/utils/defer.py:272: DeprecationWarning: There is no current event loop\r\n    return Deferred.fromFuture(asyncio.ensure_future(o))\r\ntests/test_utils_asyncio.py::AsyncioTest::test_install_asyncio_reactor\r\n  /home/runner/work/scrapy/scrapy/scrapy/utils/reactor.py:70: DeprecationWarning: There is no current event loop\r\n    event_loop = asyncio.get_event_loop()\r\n```\r\n\r\nNot sure yet what does this imply and when can this break (this is from tests on 3.10 and 3.11).\n",
    "hints_text": "> asyncio.get_event_loop() now emits a deprecation warning if there is no running event loop. In the future it will be an alias of get_running_loop(). asyncio functions which implicitly create Future or Task objects now emit a deprecation warning if there is no running event loop and no explicit loop argument is passed\ncan I add an exception handling to this then? @wRAR \n@shashwata27 what kind of exception handling?\nMaybe we can just switch to [`get_running_loop()`](https://docs.python.org/3/library/asyncio-eventloop.html#asyncio.get_running_loop), it has been around since Python 3.7, which is our minimum Python version now.\r\n\r\n~~I wonder, though, if there is any case where we can get a `RuntimeError` when using it.~~ Looking at the deprecation warnings, they explicitly say that there is no running event loop, which would trigger a `RuntimeError` with `get_running_loop()`.\nFor:\r\n\r\nhttps://github.com/scrapy/scrapy/blob/82f25bc44acd2599115fa339967b436189eec9c1/scrapy/utils/reactor.py#L70\r\n\r\nA try-except like this may make sense:\r\n\r\n```python\r\ntry:\r\n    event_loop = asyncio.get_running_loop()\r\nexcept RuntimeError:\r\n    event_loop = asyncio.new_event_loop()\r\n```\nThis seems problematic, though:\r\n\r\nhttps://github.com/scrapy/scrapy/blob/52d93490f57d4b2284e3d95efa5c622ab8ad16cb/scrapy/utils/defer.py#L323 \nIn the latter, maybe we could start by switching to `get_running_loop()`, and see what breaks. It should really only be called if there is a running loop, as far as I can tell. If not, maybe we are doing something wrong.\n> asyncio.get_event_loop() now emits a deprecation warning if there is no running event loop. In the future it will be an alias of get_running_loop(). asyncio functions which implicitly create Future or Task objects now emit a deprecation warning if there is no running event loop and no explicit loop argument is passed\ncan I add an exception handling to this then? @wRAR \n@shashwata27 what kind of exception handling?\nMaybe we can just switch to [`get_running_loop()`](https://docs.python.org/3/library/asyncio-eventloop.html#asyncio.get_running_loop), it has been around since Python 3.7, which is our minimum Python version now.\r\n\r\n~~I wonder, though, if there is any case where we can get a `RuntimeError` when using it.~~ Looking at the deprecation warnings, they explicitly say that there is no running event loop, which would trigger a `RuntimeError` with `get_running_loop()`.\nFor:\r\n\r\nhttps://github.com/scrapy/scrapy/blob/82f25bc44acd2599115fa339967b436189eec9c1/scrapy/utils/reactor.py#L70\r\n\r\nA try-except like this may make sense:\r\n\r\n```python\r\ntry:\r\n    event_loop = asyncio.get_running_loop()\r\nexcept RuntimeError:\r\n    event_loop = asyncio.new_event_loop()\r\n```\nThis seems problematic, though:\r\n\r\nhttps://github.com/scrapy/scrapy/blob/52d93490f57d4b2284e3d95efa5c622ab8ad16cb/scrapy/utils/defer.py#L323 \nIn the latter, maybe we could start by switching to `get_running_loop()`, and see what breaks. It should really only be called if there is a running loop, as far as I can tell. If not, maybe we are doing something wrong.",
    "created_at": "2022-10-24T16:54:02Z",
    "version": "2.7",
    "PASS_TO_PASS": [],
    "FAIL_TO_PASS": [
      "tests/test_utils_asyncio.py::AsyncioTest::test_install_asyncio_reactor"
    ]
  },
  {
    "repo": "scrapy/scrapy",
    "pull_number": 5679,
    "instance_id": "scrapy__scrapy-5679",
    "issue_numbers": [
      "5590"
    ],
    "base_commit": "715c05d504d22e87935ae42cee55ee35b12c2ebd",
    "patch": "diff --git a/docs/topics/settings.rst b/docs/topics/settings.rst\nindex a711fd197ab..0b1ef71cfa3 100644\n--- a/docs/topics/settings.rst\n+++ b/docs/topics/settings.rst\n@@ -1642,6 +1642,11 @@ install the default reactor defined by Twisted for the current platform. This\n is to maintain backward compatibility and avoid possible problems caused by\n using a non-default reactor.\n \n+.. versionchanged:: VERSION\n+   The :command:`startproject` command now sets this setting to\n+   ``twisted.internet.asyncioreactor.AsyncioSelectorReactor`` in the generated\n+   ``settings.py`` file.\n+\n For additional information, see :doc:`core/howto/choosing-reactor`.\n \n \ndiff --git a/scrapy/templates/project/module/settings.py.tmpl b/scrapy/templates/project/module/settings.py.tmpl\nindex 5e541e2c0bb..c0c34e986cb 100644\n--- a/scrapy/templates/project/module/settings.py.tmpl\n+++ b/scrapy/templates/project/module/settings.py.tmpl\n@@ -89,3 +89,4 @@ ROBOTSTXT_OBEY = True\n \n # Set settings whose default value is deprecated to a future-proof value\n REQUEST_FINGERPRINTER_IMPLEMENTATION = 'VERSION'\n+TWISTED_REACTOR = 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'\n",
    "test_patch": "diff --git a/tests/test_commands.py b/tests/test_commands.py\nindex 76d5f3935b4..eaca41102b9 100644\n--- a/tests/test_commands.py\n+++ b/tests/test_commands.py\n@@ -689,8 +689,15 @@ def test_asyncio_enabled_true(self):\n         ])\n         self.assertIn(\"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log)\n \n-    def test_asyncio_enabled_false(self):\n+    def test_asyncio_enabled_default(self):\n         log = self.get_log(self.debug_log_spider, args=[])\n+        self.assertIn(\"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log)\n+\n+    def test_asyncio_enabled_false(self):\n+        log = self.get_log(self.debug_log_spider, args=[\n+            '-s', 'TWISTED_REACTOR=twisted.internet.selectreactor.SelectReactor'\n+        ])\n+        self.assertIn(\"Using reactor: twisted.internet.selectreactor.SelectReactor\", log)\n         self.assertNotIn(\"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log)\n \n     @mark.skipif(sys.implementation.name == 'pypy', reason='uvloop does not support pypy properly')\n",
    "problem_statement": "Make asyncio reactor a default\nShould we make 'twisted.internet.asyncioreactor.AsyncioSelectorReactor' a default? If so, should it be default in Scrapy 2.7, or is it too soon? We can start with making it default in the project template; it looks safe enough for Scrapy 2.7, or is it?\r\n\r\nThe reactor was introduced in Scrapy 2.0; I don't recall any particular issues reported about it. It'd be good to have it as a default eventually, because there are more an more Scrapy plugins which rely on asyncio libraries.\n",
    "hints_text": "Are there any big users/projects using the asyncio reactor @kmike?\n@pablohoffman packages like \r\n\r\n* https://github.com/scrapy-plugins/scrapy-playwright/, \r\n* https://github.com/scrapinghub/scrapy-autoextract, \r\n* https://github.com/scrapy-plugins/scrapy-zyte-api \r\n \r\nrequire asyncio reactor; any Scrapy project which uses one of these extensions needs to switch to ascynio reactor.\nRelated: #5103\nThanks. As far as I'm concerned, it makes sense, and changing the default project template is a good way to start.",
    "created_at": "2022-10-14T16:04:51Z",
    "version": "2.6",
    "PASS_TO_PASS": [],
    "FAIL_TO_PASS": [
      "tests/test_commands.py::RunSpiderCommandTest::test_asyncio_enabled_default",
      "tests/test_commands.py::RunSpiderCommandTest::test_asyncio_enabled_false"
    ]
  },
  {
    "repo": "scrapy/scrapy",
    "pull_number": 5611,
    "instance_id": "scrapy__scrapy-5611",
    "issue_numbers": [
      "5601"
    ],
    "base_commit": "52d93490f57d4b2284e3d95efa5c622ab8ad16cb",
    "patch": "diff --git a/scrapy/http/response/text.py b/scrapy/http/response/text.py\nindex 89516b9b63f..bfcde878dd0 100644\n--- a/scrapy/http/response/text.py\n+++ b/scrapy/http/response/text.py\n@@ -11,8 +11,13 @@\n from urllib.parse import urljoin\n \n import parsel\n-from w3lib.encoding import (html_body_declared_encoding, html_to_unicode,\n-                            http_content_type_encoding, resolve_encoding)\n+from w3lib.encoding import (\n+    html_body_declared_encoding,\n+    html_to_unicode,\n+    http_content_type_encoding,\n+    resolve_encoding,\n+    read_bom,\n+)\n from w3lib.html import strip_html5_whitespace\n \n from scrapy.http import Request\n@@ -60,6 +65,7 @@ def encoding(self):\n     def _declared_encoding(self):\n         return (\n             self._encoding\n+            or self._bom_encoding()\n             or self._headers_encoding()\n             or self._body_declared_encoding()\n         )\n@@ -117,6 +123,10 @@ def _auto_detect_fun(self, text):\n     def _body_declared_encoding(self):\n         return html_body_declared_encoding(self.body)\n \n+    @memoizemethod_noargs\n+    def _bom_encoding(self):\n+        return read_bom(self.body)[0]\n+\n     @property\n     def selector(self):\n         from scrapy.selector import Selector\n",
    "test_patch": "diff --git a/tests/test_http_response.py b/tests/test_http_response.py\nindex 2986f884fcd..5d67a5e74cc 100644\n--- a/tests/test_http_response.py\n+++ b/tests/test_http_response.py\n@@ -1,3 +1,4 @@\n+import codecs\n import unittest\n from unittest import mock\n \n@@ -358,6 +359,8 @@ def test_encoding(self):\n                                  headers={\"Content-type\": [\"text/html; charset=gb2312\"]})\n         r7 = self.response_class(\"http://www.example.com\", body=b\"\\xa8D\",\n                                  headers={\"Content-type\": [\"text/html; charset=gbk\"]})\n+        r8 = self.response_class(\"http://www.example.com\", body=codecs.BOM_UTF8 + b\"\\xc2\\xa3\",\n+                                 headers={\"Content-type\": [\"text/html; charset=cp1251\"]})\n \n         self.assertEqual(r1._headers_encoding(), \"utf-8\")\n         self.assertEqual(r2._headers_encoding(), None)\n@@ -367,7 +370,10 @@ def test_encoding(self):\n         self.assertEqual(r3._declared_encoding(), \"cp1252\")\n         self.assertEqual(r4._headers_encoding(), None)\n         self.assertEqual(r5._headers_encoding(), None)\n+        self.assertEqual(r8._headers_encoding(), \"cp1251\")\n+        self.assertEqual(r8._declared_encoding(), \"utf-8\")\n         self._assert_response_encoding(r5, \"utf-8\")\n+        self._assert_response_encoding(r8, \"utf-8\")\n         assert r4._body_inferred_encoding() is not None and r4._body_inferred_encoding() != 'ascii'\n         self._assert_response_values(r1, 'utf-8', \"\\xa3\")\n         self._assert_response_values(r2, 'utf-8', \"\\xa3\")\n",
    "problem_statement": "BOM should take precedence over Content-Type header when detecting the encoding\nCurrently Scrapy uses headers first to detect the encoding. But browsers actually put a higher priority for BOM; this is also in WHATWG [standard](https://html.spec.whatwg.org/multipage/parsing.html#determining-the-character-encoding). It can be checked e.g. by running this server, and opening URL in a browser - UTF-8 is used by browser, but cp1251 is used by Scrapy:\r\n\r\n```py\r\nimport codecs\r\nfrom http.server import BaseHTTPRequestHandler\r\nfrom http.server import HTTPServer\r\n\r\n\r\nclass HttpGetHandler(BaseHTTPRequestHandler):\r\n    def do_GET(self):\r\n        self.send_response(200)\r\n        self.send_header(\"Content-type\", \"text/html; charset=cp1251\")\r\n        self.end_headers()\r\n        self.wfile.write(codecs.BOM_UTF8)\r\n        self.wfile.write(\"<!DOCTYPE html>\".encode('utf8'))\r\n        self.wfile.write(\"\u041f\u0440\u0438\u0432\u0435\u0442!\".encode('utf8'))\r\n\r\n\r\nif __name__ == '__main__':\r\n    httpd = HTTPServer(('', 8000), HttpGetHandler)\r\n    httpd.serve_forever()\r\n```\r\n\r\nWhen opening this page in a browser, it shows \"\u041f\u0440\u0438\u0432\u0435\u0442!\".\r\n\r\nSpider code to check it:\r\n\r\n```py\r\nimport scrapy\r\nfrom scrapy.crawler import CrawlerProcess\r\n\r\n\r\nclass MySpider(scrapy.Spider):\r\n    name = \"tst\"\r\n\r\n    start_urls = [\"http://0.0.0.0:8000\"]\r\n\r\n    def parse(self, response):\r\n        return {\"encoding\": response.encoding, \"text\": response.text}\r\n\r\n\r\nif __name__ == '__main__':\r\n    process = CrawlerProcess()\r\n    process.crawl(MySpider)\r\n    process.start()\r\n```\r\n\r\nSpider outputs\r\n\r\n> {'encoding': 'cp1251', 'text': '\u043f\u00bb\u0457<!DOCTYPE html>\u0420\u045f\u0421\u0402\u0420\u0451\u0420\u0406\u0420\u00b5\u0421\u201a!'}\r\n\r\nSee also: https://github.com/scrapy/w3lib/issues/189 - it's a similar issue, but fixing it in w3lib is not enough to make it working in Scrapy.\n",
    "hints_text": "This is a mistake of target web page.I don't think scrapy could add some methods to detect encoding by checking web content's encoding.For example,if a web page is composed of some strings of  defferent encodings,so which is the right encoding ?\nWe already try to detect the encoding of the page.\nHey @simapple! Scrapy aims to have a behavior which is similar to web browsers.\r\n\r\n> For example,if a web page is composed of some strings of defferent encodings,so which is the right encoding ?\r\n\r\nReal-world browser behavior is documented in various WHATWG standards, so they have an answer for this. This BOM issue is not a random hack which only Scrapy would have, it's something which all browsers do, and something which is described in the standard.\nSorry,I was not very clear about WHATWG standard.Following the standard is always right.",
    "created_at": "2022-08-28T16:01:19Z",
    "version": "2.6",
    "PASS_TO_PASS": [],
    "FAIL_TO_PASS": [
      "tests/test_http_response.py::TextResponseTest::test_encoding"
    ]
  },
  {
    "repo": "scrapy/scrapy",
    "pull_number": 5581,
    "instance_id": "scrapy__scrapy-5581",
    "issue_numbers": [
      "5500"
    ],
    "base_commit": "d60b4edd11436e61284615ec7ce89f8ac7e46d9a",
    "patch": "diff --git a/scrapy/extensions/feedexport.py b/scrapy/extensions/feedexport.py\nindex cd26b577896..da1a88299ec 100644\n--- a/scrapy/extensions/feedexport.py\n+++ b/scrapy/extensions/feedexport.py\n@@ -383,13 +383,20 @@ def close_spider(self, spider):\n         return defer.DeferredList(deferred_list) if deferred_list else None\n \n     def _close_slot(self, slot, spider):\n+        def get_file(slot_):\n+            if isinstance(slot_.file, PostProcessingManager):\n+                slot_.file.close()\n+                return slot_.file.file\n+            return slot_.file\n+\n         slot.finish_exporting()\n         if not slot.itemcount and not slot.store_empty:\n             # We need to call slot.storage.store nonetheless to get the file\n             # properly closed.\n-            return defer.maybeDeferred(slot.storage.store, slot.file)\n+            return defer.maybeDeferred(slot.storage.store, get_file(slot))\n+\n         logmsg = f\"{slot.format} feed ({slot.itemcount} items) in: {slot.uri}\"\n-        d = defer.maybeDeferred(slot.storage.store, slot.file)\n+        d = defer.maybeDeferred(slot.storage.store, get_file(slot))\n \n         d.addCallback(\n             self._handle_store_success, logmsg, spider, type(slot.storage).__name__\n",
    "test_patch": "diff --git a/tests/test_feedexport.py b/tests/test_feedexport.py\nindex eafe1b3342f..3124d9d67b1 100644\n--- a/tests/test_feedexport.py\n+++ b/tests/test_feedexport.py\n@@ -1638,6 +1638,57 @@ def test_extend_kwargs(self):\n             data = yield self.exported_data(items, settings)\n             self.assertEqual(row[\"expected\"], data[feed_options[\"format\"]])\n \n+    @defer.inlineCallbacks\n+    def test_storage_file_no_postprocessing(self):\n+        @implementer(IFeedStorage)\n+        class Storage:\n+            def __init__(self, uri, *, feed_options=None):\n+                pass\n+\n+            def open(self, spider):\n+                Storage.open_file = tempfile.NamedTemporaryFile(prefix=\"feed-\")\n+                return Storage.open_file\n+\n+            def store(self, file):\n+                Storage.store_file = file\n+                file.close()\n+\n+        settings = {\n+            \"FEEDS\": {self._random_temp_filename(): {\"format\": \"jsonlines\"}},\n+            \"FEED_STORAGES\": {\"file\": Storage},\n+        }\n+        yield self.exported_no_data(settings)\n+        self.assertIs(Storage.open_file, Storage.store_file)\n+\n+    @defer.inlineCallbacks\n+    def test_storage_file_postprocessing(self):\n+        @implementer(IFeedStorage)\n+        class Storage:\n+            def __init__(self, uri, *, feed_options=None):\n+                pass\n+\n+            def open(self, spider):\n+                Storage.open_file = tempfile.NamedTemporaryFile(prefix=\"feed-\")\n+                return Storage.open_file\n+\n+            def store(self, file):\n+                Storage.store_file = file\n+                file.close()\n+\n+        settings = {\n+            \"FEEDS\": {\n+                self._random_temp_filename(): {\n+                    \"format\": \"jsonlines\",\n+                    \"postprocessing\": [\n+                        \"scrapy.extensions.postprocessing.GzipPlugin\",\n+                    ],\n+                },\n+            },\n+            \"FEED_STORAGES\": {\"file\": Storage},\n+        }\n+        yield self.exported_no_data(settings)\n+        self.assertIs(Storage.open_file, Storage.store_file)\n+\n \n class FeedPostProcessedExportsTest(FeedExportTestBase):\n     __test__ = True\n",
    "problem_statement": "Postprocessing feeds do not work for S3 feed storage\n<!--\r\n\r\nThanks for taking an interest in Scrapy!\r\n\r\nIf you have a question that starts with \"How to...\", please see the Scrapy Community page: https://scrapy.org/community/.\r\nThe GitHub issue tracker's purpose is to deal with bug reports and feature requests for the project itself.\r\n\r\nKeep in mind that by filing an issue, you are expected to comply with Scrapy's Code of Conduct, including treating everyone with respect: https://github.com/scrapy/scrapy/blob/master/CODE_OF_CONDUCT.md\r\n\r\nThe following is a suggested template to structure your issue, you can find more guidelines at https://doc.scrapy.org/en/latest/contributing.html#reporting-bugs\r\n\r\n-->\r\n\r\n### Description\r\n\r\nExample settings:\r\n```\r\nFEEDS = {\r\n    \"s3://base/file_%(batch_id)05d.gz\": {\r\n        \"format\": \"csv\",\r\n        \"postprocessing\": [GzipPlugin],\r\n        \"gzip_compresslevel\": 5,\r\n    },\r\n}\r\nFEED_EXPORT_BATCH_ITEM_COUNT = 50000\r\n```\r\nThis causes an exception:\r\n\r\n```python\r\n2022-05-14 01:02:12 [scrapy.extensions.feedexport] ERROR: Error storing csv feed (15 items) in: s3://base/file_00001.gz\r\nTraceback (most recent call last):\r\n  File \"/home/mariusz/Documents/Praca/venv_wgsn/lib/python3.10/site-packages/twisted/python/threadpool.py\", line 244, in inContext\r\n    result = inContext.theWork()  # type: ignore[attr-defined]\r\n  File \"/home/mariusz/Documents/Praca/venv_wgsn/lib/python3.10/site-packages/twisted/python/threadpool.py\", line 260, in <lambda>\r\n    inContext.theWork = lambda: context.call(  # type: ignore[attr-defined]\r\n  File \"/home/mariusz/Documents/Praca/venv_wgsn/lib/python3.10/site-packages/twisted/python/context.py\", line 117, in callWithContext\r\n    return self.currentContext().callWithContext(ctx, func, *args, **kw)\r\n  File \"/home/mariusz/Documents/Praca/venv_wgsn/lib/python3.10/site-packages/twisted/python/context.py\", line 82, in callWithContext\r\n    return func(*args, **kw)\r\n  File \"/home/mariusz/Documents/Praca/venv_wgsn/lib/python3.10/site-packages/scrapy/extensions/feedexport.py\", line 194, in _store_in_thread\r\n    file.seek(0)\r\nio.UnsupportedOperation: seek\r\n```\r\nApparently `scrapy.extensions.postprocessing.PostProcessingManager` doesn't fully implement file protocol. Adding this method to the class:\r\n```python\r\n    def seek(self, offset, whence=SEEK_SET):\r\n        return self.file.seek(offset, whence)\r\n```\r\nCause an exception in a different place:\r\n```python\r\nTraceback (most recent call last):\r\n  File \"/home/mariusz/Documents/Praca/venv_wgsn/lib/python3.10/site-packages/twisted/python/threadpool.py\", line 244, in inContext\r\n    result = inContext.theWork()  # type: ignore[attr-defined]\r\n  File \"/home/mariusz/Documents/Praca/venv_wgsn/lib/python3.10/site-packages/twisted/python/threadpool.py\", line 260, in <lambda>\r\n    inContext.theWork = lambda: context.call(  # type: ignore[attr-defined]\r\n  File \"/home/mariusz/Documents/Praca/venv_wgsn/lib/python3.10/site-packages/twisted/python/context.py\", line 117, in callWithContext\r\n    return self.currentContext().callWithContext(ctx, func, *args, **kw)\r\n  File \"/home/mariusz/Documents/Praca/venv_wgsn/lib/python3.10/site-packages/twisted/python/context.py\", line 82, in callWithContext\r\n    return func(*args, **kw)\r\n  File \"/home/mariusz/Documents/Praca/venv_wgsn/lib/python3.10/site-packages/scrapy/extensions/feedexport.py\", line 196, in _store_in_thread\r\n    self.s3_client.put_object(\r\n  File \"/home/mariusz/Documents/Praca/venv_wgsn/lib/python3.10/site-packages/botocore/client.py\", line 395, in _api_call\r\n    return self._make_api_call(operation_name, kwargs)\r\n  File \"/home/mariusz/Documents/Praca/venv_wgsn/lib/python3.10/site-packages/botocore/client.py\", line 695, in _make_api_call\r\n    request_dict = self._convert_to_request_dict(\r\n  File \"/home/mariusz/Documents/Praca/venv_wgsn/lib/python3.10/site-packages/botocore/client.py\", line 745, in _convert_to_request_dict\r\n    request_dict = self._serializer.serialize_to_request(\r\n  File \"/home/mariusz/Documents/Praca/venv_wgsn/lib/python3.10/site-packages/botocore/validate.py\", line 360, in serialize_to_request\r\n    raise ParamValidationError(report=report.generate_report())\r\nbotocore.exceptions.ParamValidationError: Parameter validation failed:\r\nInvalid type for parameter Body, value: <scrapy.extensions.postprocessing.PostProcessingManager object at 0x7f1f245c6920>, type: <class 'scrapy.extensions.postprocessing.PostProcessingManager'>, valid types: <class 'bytes'>, <class 'bytearray'>, file-like object\r\n```\r\nApparently `boto` excepts a `read()` method to be present as well ([here](https://github.com/boto/botocore/blob/develop/botocore/validate.py#L320-L332)).\r\n\r\nTried to add `read()` method to `scrapy.extensions.postprocessing.PostProcessingManager` as well but I only received an incomplete file. I think it's possible because `gzip.GzipFile` use some buffering so it only save full file when `close()` is called on it. Since `S3FeedStorage` uses internally `tempfile.NamedTemporaryFile`, this cause the file to disappear right after creation.\r\n\r\n`PostProcessingManager` needs to be refactored so it can handle `BlockingFeedStorage` correctly.\r\n\r\n### Versions\r\n\r\n```\r\n$ scrapy version --verbose\r\nScrapy       : 2.6.1\r\nlxml         : 4.8.0.0\r\nlibxml2      : 2.9.12\r\ncssselect    : 1.1.0\r\nparsel       : 1.6.0\r\nw3lib        : 1.22.0\r\nTwisted      : 22.2.0\r\nPython       : 3.10.4 (main, May 11 2022, 11:41:05) [GCC 11.0.1 20210417 (experimental) [master revision c1c86ab96c2:b6fb0ccbb48:8ae\r\npyOpenSSL    : 22.0.0 (OpenSSL 1.1.1m  14 Dec 2021)\r\ncryptography : 36.0.1\r\nPlatform     : Linux-5.11.0-16-generic-x86_64-with-glibc2.33\r\n```\r\n\r\n### Additional context\n",
    "hints_text": "I was able to fix this ad hoc for my project. This is what I did: \r\n\r\n1. `scrapy.extensions.postprocessing.PostProcessingManager` must pass all calls to its `seek` and `read` methods to the same methods inside its `file` attribute. \r\n1. In `GzipPlugin` its `gzipfile` attribute must be closed before trying to read form it. \r\n\r\nI was able to monkey patch everything with the following code:\r\n\r\n```python\r\n from scrapy.extensions.postprocessing import PostProcessingManager, GzipPlugin\r\n \r\n \r\n def read(self, *args, **kwargs):\r\n     return self.file.read(*args, **kwargs)\r\n \r\n \r\n def seek(self, *args, **kwargs):\r\n     # Only time seek is executed is when uploading the finished file\r\n     if hasattr(self.head_plugin, \"gzipfile\") and not self.head_plugin.gzipfile.closed:\r\n         self.head_plugin.gzipfile.flush()\r\n         # It should be safe to close at this point\r\n         self.head_plugin.gzipfile.close()\r\n \r\n     return self.file.seek(*args, **kwargs)\r\n \r\n \r\n def close(self):\r\n     # Gzip is already closed by PostProcessingManager.seek\r\n     self.file.close()\r\n \r\n \r\n PostProcessingManager.read = read\r\n PostProcessingManager.seek = seek\r\n GzipPlugin.close = close\r\n```\r\n\r\nHowever, this code assumes only GzipPlugin will be used and seek will only be called right before writting the file to s3. \r\nThis is not a good solution overall. A more generic solution should be designed thinking about every possible combination of all plugins.",
    "created_at": "2022-07-28T21:47:07Z",
    "version": "2.8",
    "PASS_TO_PASS": [],
    "FAIL_TO_PASS": [
      "tests/test_feedexport.py::FeedExportTest::test_storage_file_no_postprocessing",
      "tests/test_feedexport.py::FeedExportTest::test_storage_file_postprocessing"
    ]
  },
  {
    "repo": "scrapy/scrapy",
    "pull_number": 5526,
    "instance_id": "scrapy__scrapy-5526",
    "issue_numbers": [
      "5515"
    ],
    "base_commit": "e2769cfe484fa9bf3d4b36623471cc605393ec85",
    "patch": "diff --git a/scrapy/http/headers.py b/scrapy/http/headers.py\nindex 1a2b99b0a4e..9c03fe54f09 100644\n--- a/scrapy/http/headers.py\n+++ b/scrapy/http/headers.py\n@@ -1,3 +1,5 @@\n+from collections.abc import Mapping\n+\n from w3lib.http import headers_dict_to_raw\n from scrapy.utils.datatypes import CaselessDict\n from scrapy.utils.python import to_unicode\n@@ -10,6 +12,13 @@ def __init__(self, seq=None, encoding='utf-8'):\n         self.encoding = encoding\n         super().__init__(seq)\n \n+    def update(self, seq):\n+        seq = seq.items() if isinstance(seq, Mapping) else seq\n+        iseq = {}\n+        for k, v in seq:\n+            iseq.setdefault(self.normkey(k), []).extend(self.normvalue(v))\n+        super().update(iseq)\n+\n     def normkey(self, key):\n         \"\"\"Normalize key to bytes\"\"\"\n         return self._tobytes(key.title())\n@@ -86,4 +95,5 @@ def to_unicode_dict(self):\n \n     def __copy__(self):\n         return self.__class__(self)\n+\n     copy = __copy__\n",
    "test_patch": "diff --git a/tests/test_http_headers.py b/tests/test_http_headers.py\nindex 64ff7a73dbf..1ca93624794 100644\n--- a/tests/test_http_headers.py\n+++ b/tests/test_http_headers.py\n@@ -38,6 +38,12 @@ def test_multivalue(self):\n         self.assertEqual(h.getlist('X-Forwarded-For'), [b'ip1', b'ip2'])\n         assert h.getlist('X-Forwarded-For') is not hlist\n \n+    def test_multivalue_for_one_header(self):\n+        h = Headers(((\"a\", \"b\"), (\"a\", \"c\")))\n+        self.assertEqual(h[\"a\"], b\"c\")\n+        self.assertEqual(h.get(\"a\"), b\"c\")\n+        self.assertEqual(h.getlist(\"a\"), [b\"b\", b\"c\"])\n+\n     def test_encode_utf8(self):\n         h = Headers({'key': '\\xa3'}, encoding='utf-8')\n         key, val = dict(h).popitem()\n",
    "problem_statement": "Response.headers loses data on multiple values\nhttps://github.com/scrapy/scrapy/issues/1262 reported that by default `response.headers` would only expose the first value of a header e.g. when casted as a `dict`, acknowledging that `response.headers.getlist` could be used instead to get all values.\r\n\r\nI have just found out that the latter is not true:\r\n\r\n```python\r\n>>> from scrapy.http import Response\r\n>>> response = Response(\"https://example.com\", headers=((\"a\", \"b\"), (\"a\", \"c\")))\r\n>>> response.headers.getlist(\"a\")\r\n[b'c']\r\n```\r\n\r\nI could verify the issue happening as far back as Scrapy 1.6, so it does not look like a recent bug.\n",
    "hints_text": "",
    "created_at": "2022-06-11T19:24:16Z",
    "version": "2.6",
    "PASS_TO_PASS": [],
    "FAIL_TO_PASS": [
      "tests/test_http_headers.py::HeadersTest::test_multivalue_for_one_header"
    ]
  }
]
