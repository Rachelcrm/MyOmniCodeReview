Command To Generate Bad Patches + Run Bug Fixing to Validate Patches: 

Steps: 

1: Run "export GITHUB_TOKEN=[Your git token]"
2. python3 baselines/badpatchllm/generate_bad.py -o baselines/badpatchllm/logs/gemini_outputs --instance_ids [Instance_ID] -m [gemini-2.0-flash or gemini-2.5-flash-preview-4-17 (2.5 better)] -k [Secret_Key] --run_id [run_id] -n [num_bad_patches] -d [input_data_path]

- Can find raw .diff files in generated log files 
- Can find output .json instances with bad_patches field in input_data_directory


# divider 

[
  {
    "repo": "camel-ai/camel",
    "pull_number": 1526,
    "instance_id": "camel-ai__camel-1526",
    "issue_numbers": [
      "1524"
    ],
    "base_commit": "03c3f22d9f728b131112e4487c0000f9f4552c49",
    "patch": "diff --git a/.env b/.env\nindex 0600f02192..31878e4263 100644\n--- a/.env\n+++ b/.env\n@@ -53,6 +53,9 @@\n # InternLM API (https://internlm.intern-ai.org.cn/api/tokens)\n # INTERNLM_API_KEY=\"Fill your API key here\"\n \n+# Moonshot API (https://platform.moonshot.cn/)\n+# MOONSHOT_API_KEY=\"Fill your API key here\"\n+\n # JINA API (https://jina.ai/)\n # JINA_API_KEY=\"Fill your API key here\"\n \ndiff --git a/.github/workflows/build_package.yml b/.github/workflows/build_package.yml\nindex 4c61f07ec5..d5fd5474c0 100644\n--- a/.github/workflows/build_package.yml\n+++ b/.github/workflows/build_package.yml\n@@ -80,6 +80,7 @@ jobs:\n           DISCORD_BOT_TOKEN: \"${{ secrets.DISCORD_BOT_TOKEN }}\"\n           INTERNLM_API_KEY: \"${{ secrets.INTERNLM_API_KEY }}\"\n           JINA_API_KEY: \"${{ secrets.JINA_API_KEY }}\"\n+          MOONSHOT_API_KEY: \"${{ secrets.MOONSHOT_API_KEY }}\"\n         run: |\n           source venv/bin/activate\n           pytest --fast-test-mode ./test\ndiff --git a/camel/configs/__init__.py b/camel/configs/__init__.py\nindex 2e6b30b3f1..3a3250858f 100644\n--- a/camel/configs/__init__.py\n+++ b/camel/configs/__init__.py\n@@ -20,6 +20,7 @@\n from .internlm_config import INTERNLM_API_PARAMS, InternLMConfig\n from .litellm_config import LITELLM_API_PARAMS, LiteLLMConfig\n from .mistral_config import MISTRAL_API_PARAMS, MistralConfig\n+from .moonshot_config import MOONSHOT_API_PARAMS, MoonshotConfig\n from .nvidia_config import NVIDIA_API_PARAMS, NvidiaConfig\n from .ollama_config import OLLAMA_API_PARAMS, OllamaConfig\n from .openai_config import OPENAI_API_PARAMS, ChatGPTConfig\n@@ -79,4 +80,6 @@\n     'DEEPSEEK_API_PARAMS',\n     'InternLMConfig',\n     'INTERNLM_API_PARAMS',\n+    'MoonshotConfig',\n+    \"MOONSHOT_API_PARAMS\",\n ]\ndiff --git a/camel/configs/moonshot_config.py b/camel/configs/moonshot_config.py\nnew file mode 100644\nindex 0000000000..681ff1b635\n--- /dev/null\n+++ b/camel/configs/moonshot_config.py\n@@ -0,0 +1,63 @@\n+# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========\n+\n+from typing import List, Optional, Union\n+\n+from camel.configs.base_config import BaseConfig\n+\n+\n+class MoonshotConfig(BaseConfig):\n+    r\"\"\"Defines the parameters for generating chat completions using the\n+    Moonshot API. You can refer to the following link for more details:\n+    https://platform.moonshot.cn/docs/api-reference\n+\n+    Args:\n+        temperature (float, optional): Controls randomness in the response.\n+            Lower values make the output more focused and deterministic.\n+            (default: :obj:`0.3`)\n+        max_tokens (int, optional): The maximum number of tokens to generate.\n+            (default: :obj:`None`)\n+        stream (bool, optional): Whether to stream the response.\n+            (default: :obj:`False`)\n+        tools (list, optional): List of tools that the model can use for\n+            function calling. Each tool should be a dictionary containing\n+            type, function name, description, and parameters.\n+            (default: :obj:`None`)\n+        top_p (float, optional): Controls diversity via nucleus sampling.\n+            (default: :obj:`1.0`)\n+        n (int, optional): How many chat completion choices to generate for\n+            each input message. (default: :obj:`1`)\n+        presence_penalty (float, optional): Penalty for new tokens based on\n+            whether they appear in the text so far.\n+            (default: :obj:`0.0`)\n+        frequency_penalty (float, optional): Penalty for new tokens based on\n+            their frequency in the text so far.\n+            (default: :obj:`0.0`)\n+        stop (Optional[Union[str, List[str]]], optional): Up to 4 sequences\n+            where the API will stop generating further tokens.\n+            (default: :obj:`None`)\n+    \"\"\"\n+\n+    temperature: float = 0.3\n+    max_tokens: Optional[int] = None\n+    stream: bool = False\n+    tools: Optional[list] = None\n+    top_p: float = 1.0\n+    n: int = 1\n+    presence_penalty: float = 0.0\n+    frequency_penalty: float = 0.0\n+    stop: Optional[Union[str, List[str]]] = None\n+\n+\n+MOONSHOT_API_PARAMS = {param for param in MoonshotConfig.model_fields.keys()}\ndiff --git a/camel/models/__init__.py b/camel/models/__init__.py\nindex 6a4adc4c4c..138f5dadd9 100644\n--- a/camel/models/__init__.py\n+++ b/camel/models/__init__.py\n@@ -24,6 +24,7 @@\n from .mistral_model import MistralModel\n from .model_factory import ModelFactory\n from .model_manager import ModelManager, ModelProcessingError\n+from .moonshot_model import MoonshotModel\n from .nemotron_model import NemotronModel\n from .nvidia_model import NvidiaModel\n from .ollama_model import OllamaModel\n@@ -70,4 +71,5 @@\n     'DeepSeekModel',\n     'FishAudioModel',\n     'InternLMModel',\n+    'MoonshotModel',\n ]\ndiff --git a/camel/models/model_factory.py b/camel/models/model_factory.py\nindex c401ffd0aa..8ffc83e01c 100644\n--- a/camel/models/model_factory.py\n+++ b/camel/models/model_factory.py\n@@ -23,6 +23,7 @@\n from camel.models.internlm_model import InternLMModel\n from camel.models.litellm_model import LiteLLMModel\n from camel.models.mistral_model import MistralModel\n+from camel.models.moonshot_model import MoonshotModel\n from camel.models.nvidia_model import NvidiaModel\n from camel.models.ollama_model import OllamaModel\n from camel.models.openai_compatible_model import OpenAICompatibleModel\n@@ -127,6 +128,8 @@ def create(\n             model_class = DeepSeekModel\n         elif model_platform.is_internlm and model_type.is_internlm:\n             model_class = InternLMModel\n+        elif model_platform.is_moonshot and model_type.is_moonshot:\n+            model_class = MoonshotModel\n         elif model_type == ModelType.STUB:\n             model_class = StubModel\n \ndiff --git a/camel/models/moonshot_model.py b/camel/models/moonshot_model.py\nnew file mode 100644\nindex 0000000000..d23f365be9\n--- /dev/null\n+++ b/camel/models/moonshot_model.py\n@@ -0,0 +1,138 @@\n+# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========\n+\n+import os\n+from typing import Any, Dict, List, Optional, Union\n+\n+from openai import OpenAI, Stream\n+\n+from camel.configs import MOONSHOT_API_PARAMS, MoonshotConfig\n+from camel.messages import OpenAIMessage\n+from camel.models import BaseModelBackend\n+from camel.types import (\n+    ChatCompletion,\n+    ChatCompletionChunk,\n+    ModelType,\n+)\n+from camel.utils import (\n+    BaseTokenCounter,\n+    OpenAITokenCounter,\n+    api_keys_required,\n+)\n+\n+\n+class MoonshotModel(BaseModelBackend):\n+    r\"\"\"Moonshot API in a unified BaseModelBackend interface.\n+\n+    Args:\n+        model_type (Union[ModelType, str]): Model for which a backend is\n+            created, one of Moonshot series.\n+        model_config_dict (Optional[Dict[str, Any]], optional): A dictionary\n+            that will be fed into :obj:`openai.ChatCompletion.create()`. If\n+            :obj:`None`, :obj:`MoonshotConfig().as_dict()` will be used.\n+            (default: :obj:`None`)\n+        api_key (Optional[str], optional): The API key for authenticating with\n+            the Moonshot service. (default: :obj:`None`)\n+        url (Optional[str], optional): The url to the Moonshot service.\n+            (default: :obj:`https://api.moonshot.cn/v1`)\n+        token_counter (Optional[BaseTokenCounter], optional): Token counter to\n+            use for the model. If not provided, :obj:`OpenAITokenCounter(\n+            ModelType.GPT_4)` will be used.\n+            (default: :obj:`None`)\n+    \"\"\"\n+\n+    @api_keys_required([(\"api_key\", \"MOONSHOT_API_KEY\")])\n+    def __init__(\n+        self,\n+        model_type: Union[ModelType, str],\n+        model_config_dict: Optional[Dict[str, Any]] = None,\n+        api_key: Optional[str] = None,\n+        url: Optional[str] = None,\n+        token_counter: Optional[BaseTokenCounter] = None,\n+    ) -> None:\n+        if model_config_dict is None:\n+            model_config_dict = MoonshotConfig().as_dict()\n+        api_key = api_key or os.environ.get(\"MOONSHOT_API_KEY\")\n+        url = url or os.environ.get(\n+            \"MOONSHOT_API_BASE_URL\",\n+            \"https://api.moonshot.cn/v1\",\n+        )\n+        super().__init__(\n+            model_type, model_config_dict, api_key, url, token_counter\n+        )\n+        self._client = OpenAI(\n+            api_key=self._api_key,\n+            timeout=180,\n+            max_retries=3,\n+            base_url=self._url,\n+        )\n+\n+    def run(\n+        self,\n+        messages: List[OpenAIMessage],\n+    ) -> Union[ChatCompletion, Stream[ChatCompletionChunk]]:\n+        r\"\"\"Runs inference of Moonshot chat completion.\n+\n+        Args:\n+            messages (List[OpenAIMessage]): Message list with the chat history\n+                in OpenAI API format.\n+\n+        Returns:\n+            Union[ChatCompletion, Stream[ChatCompletionChunk]]:\n+                `ChatCompletion` in the non-stream mode, or\n+                `Stream[ChatCompletionChunk]` in the stream mode.\n+        \"\"\"\n+        response = self._client.chat.completions.create(\n+            messages=messages,\n+            model=self.model_type,\n+            **self.model_config_dict,\n+        )\n+        return response\n+\n+    @property\n+    def token_counter(self) -> BaseTokenCounter:\n+        r\"\"\"Initialize the token counter for the model backend.\n+\n+        Returns:\n+            OpenAITokenCounter: The token counter following the model's\n+                tokenization style.\n+        \"\"\"\n+        if not self._token_counter:\n+            self._token_counter = OpenAITokenCounter(ModelType.GPT_4O_MINI)\n+        return self._token_counter\n+\n+    def check_model_config(self):\n+        r\"\"\"Check whether the model configuration contains any\n+        unexpected arguments to Moonshot API.\n+\n+        Raises:\n+            ValueError: If the model configuration dictionary contains any\n+                unexpected arguments to Moonshot API.\n+        \"\"\"\n+        for param in self.model_config_dict:\n+            if param not in MOONSHOT_API_PARAMS:\n+                raise ValueError(\n+                    f\"Unexpected argument `{param}` is \"\n+                    \"input into Moonshot model backend.\"\n+                )\n+\n+    @property\n+    def stream(self) -> bool:\n+        r\"\"\"Returns whether the model is in stream mode, which sends partial\n+        results each time.\n+\n+        Returns:\n+            bool: Whether the model is in stream mode.\n+        \"\"\"\n+        return self.model_config_dict.get('stream', False)\ndiff --git a/camel/types/enums.py b/camel/types/enums.py\nindex 5fbc1acb55..b694ad2fd2 100644\n--- a/camel/types/enums.py\n+++ b/camel/types/enums.py\n@@ -170,6 +170,11 @@ class ModelType(UnifiedModelType, Enum):\n     INTERNLM2_5_LATEST = \"internlm2.5-latest\"\n     INTERNLM2_PRO_CHAT = \"internlm2-pro-chat\"\n \n+    # Moonshot models\n+    MOONSHOT_V1_8K = \"moonshot-v1-8k\"\n+    MOONSHOT_V1_32K = \"moonshot-v1-32k\"\n+    MOONSHOT_V1_128K = \"moonshot-v1-128k\"\n+\n     def __str__(self):\n         return self.value\n \n@@ -201,6 +206,7 @@ def support_native_tool_calling(self) -> bool:\n                 self.is_sambanova,\n                 self.is_groq,\n                 self.is_sglang,\n+                self.is_moonshot,\n             ]\n         )\n \n@@ -422,6 +428,14 @@ def is_internlm(self) -> bool:\n             ModelType.INTERNLM2_PRO_CHAT,\n         }\n \n+    @property\n+    def is_moonshot(self) -> bool:\n+        return self in {\n+            ModelType.MOONSHOT_V1_8K,\n+            ModelType.MOONSHOT_V1_32K,\n+            ModelType.MOONSHOT_V1_128K,\n+        }\n+\n     @property\n     def is_sglang(self) -> bool:\n         return self in {\n@@ -469,6 +483,7 @@ def token_limit(self) -> int:\n             ModelType.QWEN_VL_PLUS,\n             ModelType.NVIDIA_LLAMA3_70B,\n             ModelType.TOGETHER_MISTRAL_7B,\n+            ModelType.MOONSHOT_V1_8K,\n         }:\n             return 8_192\n         elif self in {\n@@ -502,6 +517,7 @@ def token_limit(self) -> int:\n             ModelType.INTERNLM2_PRO_CHAT,\n             ModelType.TOGETHER_MIXTRAL_8_7B,\n             ModelType.SGLANG_MISTRAL_7B,\n+            ModelType.MOONSHOT_V1_32K,\n         }:\n             return 32_768\n         elif self in {\n@@ -546,6 +562,7 @@ def token_limit(self) -> int:\n             ModelType.SGLANG_LLAMA_3_1_405B,\n             ModelType.SGLANG_LLAMA_3_2_1B,\n             ModelType.SGLANG_MIXTRAL_NEMO,\n+            ModelType.MOONSHOT_V1_128K,\n         }:\n             return 128_000\n         elif self in {\n@@ -767,6 +784,7 @@ class ModelPlatformType(Enum):\n     DEEPSEEK = \"deepseek\"\n     SGLANG = \"sglang\"\n     INTERNLM = \"internlm\"\n+    MOONSHOT = \"moonshot\"\n \n     @property\n     def is_openai(self) -> bool:\n@@ -874,6 +892,11 @@ def is_internlm(self) -> bool:\n         r\"\"\"Returns whether this platform is InternLM.\"\"\"\n         return self is ModelPlatformType.INTERNLM\n \n+    @property\n+    def is_moonshot(self) -> bool:\n+        r\"\"\"Returns whether this platform is Moonshot model.\"\"\"\n+        return self is ModelPlatformType.MOONSHOT\n+\n \n class AudioModelType(Enum):\n     TTS_1 = \"tts-1\"\ndiff --git a/camel/types/unified_model_type.py b/camel/types/unified_model_type.py\nindex b4027cc6e5..45bba6ecfd 100644\n--- a/camel/types/unified_model_type.py\n+++ b/camel/types/unified_model_type.py\n@@ -118,6 +118,11 @@ def is_internlm(self) -> bool:\n         r\"\"\"Returns whether the model is a InternLM model.\"\"\"\n         return True\n \n+    @property\n+    def is_moonshot(self) -> bool:\n+        r\"\"\"Returns whether this platform is Moonshot model.\"\"\"\n+        return True\n+\n     @property\n     def support_native_structured_output(self) -> bool:\n         r\"\"\"Returns whether the model supports native structured output.\"\"\"\ndiff --git a/docs/key_modules/models.md b/docs/key_modules/models.md\nindex ede9fdad8e..489091c671 100644\n--- a/docs/key_modules/models.md\n+++ b/docs/key_modules/models.md\n@@ -35,6 +35,9 @@ The following table lists currently supported model platforms by CAMEL.\n | Mistral AI | open-mixtral-8x7b | N |\n | Mistral AI | open-mixtral-8x22b | N |\n | Mistral AI | open-codestral-mamba | N |\n+| Moonshot | moonshot-v1-8k | N |\n+| Moonshot | moonshot-v1-32k | N |\n+| Moonshot | moonshot-v1-128k | N |\n | Anthropic | claude-3-5-sonnet-latest | Y |\n | Anthropic | claude-3-5-haiku-latest | N |\n | Anthropic | claude-3-haiku-20240307 | Y |\ndiff --git a/examples/models/moonshot_model_example.py b/examples/models/moonshot_model_example.py\nnew file mode 100644\nindex 0000000000..88c9051235\n--- /dev/null\n+++ b/examples/models/moonshot_model_example.py\n@@ -0,0 +1,46 @@\n+# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========\n+\n+from camel.agents import ChatAgent\n+from camel.configs import MoonshotConfig\n+from camel.models import ModelFactory\n+from camel.types import ModelPlatformType, ModelType\n+\n+model = ModelFactory.create(\n+    model_platform=ModelPlatformType.MOONSHOT,\n+    model_type=ModelType.MOONSHOT_V1_8K,\n+    model_config_dict=MoonshotConfig(temperature=0.2).as_dict(),\n+)\n+\n+# Define system message\n+sys_msg = \"You are a helpful assistant.\"\n+\n+# Set agent\n+camel_agent = ChatAgent(system_message=sys_msg, model=model)\n+\n+user_msg = \"\"\"Say hi to CAMEL AI, one open-source community\n+    dedicated to the study of autonomous and communicative agents.\"\"\"\n+\n+# Get response information\n+response = camel_agent.step(user_msg)\n+print(response.msgs[0].content)\n+\n+'''\n+===============================================================================\n+Hi CAMEL AI! It's great to hear about your open-source community dedicated to \n+the study of autonomous and communicative agents. I'm here to help and support\n+you in any way I can. If you have any questions or need assistance with your\n+research, feel free to ask!\n+===============================================================================\n+'''\n",
    "test_patch": "diff --git a/.github/workflows/pytest_apps.yml b/.github/workflows/pytest_apps.yml\nindex e1cf0f0c01..e07fdbc362 100644\n--- a/.github/workflows/pytest_apps.yml\n+++ b/.github/workflows/pytest_apps.yml\n@@ -30,6 +30,7 @@ jobs:\n         SEARCH_ENGINE_ID: \"${{ secrets.SEARCH_ENGINE_ID }}\"\n         COHERE_API_KEY: \"${{ secrets.COHERE_API_KEY }}\"\n         INTERNLM_API_KEY: \"${{ secrets.INTERNLM_API_KEY }}\"\n+        MOONSHOT_API_KEY: \"${{ secrets.MOONSHOT_API_KEY }}\"\n       run: poetry run pytest -v apps/\n \n   pytest_examples:\n@@ -49,4 +50,5 @@ jobs:\n         SEARCH_ENGINE_ID: \"${{ secrets.SEARCH_ENGINE_ID }}\"\n         COHERE_API_KEY: \"${{ secrets.COHERE_API_KEY }}\"\n         INTERNLM_API_KEY: \"${{ secrets.INTERNLM_API_KEY }}\"\n+        MOONSHOT_API_KEY: \"${{ secrets.MOONSHOT_API_KEY }}\"\n       run: poetry run pytest -v examples/\ndiff --git a/.github/workflows/pytest_package.yml b/.github/workflows/pytest_package.yml\nindex 70c0f01151..eb227633b7 100644\n--- a/.github/workflows/pytest_package.yml\n+++ b/.github/workflows/pytest_package.yml\n@@ -59,6 +59,7 @@ jobs:\n         DISCORD_BOT_TOKEN: \"${{ secrets.DISCORD_BOT_TOKEN }}\"\n         INTERNLM_API_KEY: \"${{ secrets.INTERNLM_API_KEY }}\"\n         JINA_API_KEY: \"${{ secrets.JINA_API_KEY }}\"\n+        MOONSHOT_API_KEY: \"${{ secrets.MOONSHOT_API_KEY }}\"\n       run: poetry run pytest --fast-test-mode test/\n \n   pytest_package_llm_test:\n@@ -107,6 +108,7 @@ jobs:\n         DISCORD_BOT_TOKEN: \"${{ secrets.DISCORD_BOT_TOKEN }}\"\n         INTERNLM_API_KEY: \"${{ secrets.INTERNLM_API_KEY }}\"\n         JINA_API_KEY: \"${{ secrets.JINA_API_KEY }}\"\n+        MOONSHOT_API_KEY: \"${{ secrets.MOONSHOT_API_KEY }}\"\n       run: poetry run pytest --llm-test-only test/\n \n   pytest_package_very_slow_test:\n@@ -155,4 +157,5 @@ jobs:\n         DISCORD_BOT_TOKEN: \"${{ secrets.DISCORD_BOT_TOKEN }}\"\n         INTERNLM_API_KEY: \"${{ secrets.INTERNLM_API_KEY }}\"\n         JINA_API_KEY: \"${{ secrets.JINA_API_KEY }}\"\n+        MOONSHOT_API_KEY: \"${{ secrets.MOONSHOT_API_KEY }}\"\n       run: poetry run pytest --very-slow-test-only test/\ndiff --git a/test/models/test_moonshot_model.py b/test/models/test_moonshot_model.py\nnew file mode 100644\nindex 0000000000..3f1d0d72a5\n--- /dev/null\n+++ b/test/models/test_moonshot_model.py\n@@ -0,0 +1,55 @@\n+# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========\n+\n+import re\n+\n+import pytest\n+\n+from camel.configs import MoonshotConfig\n+from camel.models import MoonshotModel\n+from camel.types import ModelType\n+\n+\n+@pytest.mark.model_backend\n+@pytest.mark.parametrize(\n+    \"model_type\",\n+    [\n+        ModelType.MOONSHOT_V1_8K,\n+        ModelType.MOONSHOT_V1_32K,\n+        ModelType.MOONSHOT_V1_128K,\n+    ],\n+)\n+def test_moonshot_model(model_type: ModelType):\n+    model = MoonshotModel(model_type)\n+    assert model.model_type == model_type\n+    assert model.model_config_dict == MoonshotConfig().as_dict()\n+    assert isinstance(model.model_type.value_for_tiktoken, str)\n+    assert isinstance(model.model_type.token_limit, int)\n+\n+\n+@pytest.mark.model_backend\n+def test_moonshot_model_unexpected_argument():\n+    model_type = ModelType.MOONSHOT_V1_8K\n+    model_config_dict = {\"model_path\": \"moonshot_v1\"}\n+\n+    with pytest.raises(\n+        ValueError,\n+        match=re.escape(\n+            (\n+                \"Unexpected argument `model_path` is \"\n+                \"input into Moonshot model backend.\"\n+            )\n+        ),\n+    ):\n+        _ = MoonshotModel(model_type, model_config_dict)\n",
    "problem_statement": "[Feature Request] Integrating moonshot models \n### Required prerequisites\n\n- [x] I have searched the [Issue Tracker](https://github.com/camel-ai/camel/issues) and [Discussions](https://github.com/camel-ai/camel/discussions) that this hasn't already been reported. (+1 or comment there if it has.)\n- [ ] Consider asking first in a [Discussion](https://github.com/camel-ai/camel/discussions/new).\n\n### Motivation\n\nintegrating moonshot models\nintroduced in the paper: https://arxiv.org/abs/2501.12599 (kimi-k1.5 not available in the api atm)\n\n### Solution\n\n_No response_\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_\n",
    "hints_text": "@Wendong-Fan can you leave some previous model integrations.\nThanks\nhere is one you could refer to: https://github.com/camel-ai/camel/pull/1466\nThanks @GitHoobar !",
    "created_at": "2025-01-29T19:19:41Z",
    "version": "0.2",
    "PASS_TO_PASS": [],
    "FAIL_TO_PASS": [
      ".github/workflows/pytest_apps.yml",
      ".github/workflows/pytest_package.yml"
    ],
    "bad_patches": [
      "--- a/.env\n+++ b/.env\n@@ -53,6 +53,9 @@\n # InternLM API (https://internlm.intern-ai.org.cn/api/tokens)\n # INTERNLM_API_KEY=\"Fill your API key here\"\n \n+# Moonshot API (https://platform.moonshot.cn/)\n+# MOONSHOT_API_KEY=\"Fill your API key here\"\n+\n # JINA API (https://jina.ai/)\n # JINA_API_KEY=\"Fill your API key here\"\n \n--- a/.github/workflows/build_package.yml\n+++ b/.github/workflows/build_package.yml\n@@ -80,6 +80,7 @@\n           DISCORD_BOT_TOKEN: \"${{ secrets.DISCORD_BOT_TOKEN }}\"\n           INTERNLM_API_KEY: \"${{ secrets.INTERNLM_API_KEY }}\"\n           JINA_API_KEY: \"${{ secrets.JINA_API_KEY }}\"\n+          MOONSHOT_API_KEY: \"${{ secrets.MOONSHOT_API_KEY }}\"\n         run: |\n           source venv/bin/activate\n           pytest --fast-test-mode ./test\n--- a/.github/workflows/pytest_apps.yml\n+++ b/.github/workflows/pytest_apps.yml\n@@ -30,6 +30,7 @@\n         SEARCH_ENGINE_ID: \"${{ secrets.SEARCH_ENGINE_ID }}\"\n         COHERE_API_KEY: \"${{ secrets.COHERE_API_KEY }}\"\n         INTERNLM_API_KEY: \"${{ secrets.INTERNLM_API_KEY }}\"\n+        MOONSHOT_API_KEY: \"${{ secrets.MOONSHOT_API_KEY }}\"\n       run: poetry run pytest -v apps/\n \n   pytest_examples:\n@@ -49,4 +50,5 @@\n         SEARCH_ENGINE_ID: \"${{ secrets.SEARCH_ENGINE_ID }}\"\n         COHERE_API_KEY: \"${{ secrets.COHERE_API_KEY }}\"\n         INTERNLM_API_KEY: \"${{ secrets.INTERNLM_API_KEY }}\"\n+        MOONSHOT_API_KEY: \"${{ secrets.MOONSHOT_API_KEY }}\"\n       run: poetry run pytest -v examples/\n--- a/.github/workflows/pytest_package.yml\n+++ b/.github/workflows/pytest_package.yml\n@@ -59,6 +59,7 @@\n         DISCORD_BOT_TOKEN: \"${{ secrets.DISCORD_BOT_TOKEN }}\"\n         INTERNLM_API_KEY: \"${{ secrets.INTERNLM_API_KEY }}\"\n         JINA_API_KEY: \"${{ secrets.JINA_API_KEY }}\"\n+        MOONSHOT_API_KEY: \"${{ secrets.MOONSHOT_API_KEY }}\"\n       run: poetry run pytest --fast-test-mode test/\n \n   pytest_package_llm_test:\n@@ -107,6 +108,7 @@\n         DISCORD_BOT_TOKEN: \"${{ secrets.DISCORD_BOT_TOKEN }}\"\n         INTERNLM_API_KEY: \"${{ secrets.INTERNLM_API_KEY }}\"\n         JINA_API_KEY: \"${{ secrets.JINA_API_KEY }}\"\n+        MOONSHOT_API_KEY: \"${{ secrets.MOONSHOT_API_KEY }}\"\n       run: poetry run pytest --llm-test-only test/\n \n   pytest_package_very_slow_test:\n@@ -155,4 +157,5 @@\n         DISCORD_BOT_TOKEN: \"${{ secrets.DISCORD_BOT_TOKEN }}\"\n         INTERNLM_API_KEY: \"${{ secrets.INTERNLM_API_KEY }}\"\n         JINA_API_KEY: \"${{ secrets.JINA_API_KEY }}\"\n+        MOONSHOT_API_KEY: \"${{ secrets.MOONSHOT_API_KEY }}\"\n       run: poetry run pytest --very-slow-test-only test/\n--- a/camel/configs/__init__.py\n+++ b/camel/configs/__init__.py\n@@ -20,6 +20,7 @@\n from .internlm_config import INTERNLM_API_PARAMS, InternLMConfig\n from .litellm_config import LITELLM_API_PARAMS, LiteLLMConfig\n from .mistral_config import MISTRAL_API_PARAMS, MistralConfig\n+from .moonshot_config import MOONSHOT_API_PARAMS, MoonshotConfig\n from .nvidia_config import NVIDIA_API_PARAMS, NvidiaConfig\n from .ollama_config import OLLAMA_API_PARAMS, OllamaConfig\n from .openai_config import OPENAI_API_PARAMS, ChatGPTConfig\n@@ -40,7 +41,6 @@\n __all__ = [\n     'BaseConfig',\n     'ChatGPTConfig',\n-    'OPENAI_API_PARAMS',\n     'AnthropicConfig',\n     'ANTHROPIC_API_PARAMS',\n     'GROQ_API_PARAMS',\n@@ -79,4 +79,6 @@\n     'DEEPSEEK_API_PARAMS',\n     'InternLMConfig',\n     'INTERNLM_API_PARAMS',\n+    'MoonshotConfig',\n+    \"MOONSHOT_API_PARAMS\",\n ]\n--- a/camel/configs/moonshot_config.py\n+++ b/camel/configs/moonshot_config.py\n@@ -0,0 +1,63 @@\n+# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========\n+\n+from typing import List, Optional, Union\n+\n+from camel.configs.base_config import BaseConfig\n+\n+\n+class MoonshotConfig(BaseConfig):\n+    r\"\"\"Defines the parameters for generating chat completions using the\n+    Moonshot API. You can refer to the following link for more details:\n+    https://platform.moonshot.cn/docs/api-reference\n+\n+    Args:\n+        temperature (float, optional): Controls randomness in the response.\n+            Lower values make the output more focused and deterministic.\n+            (default: :obj:`0.3`)\n+        max_tokens (int, optional): The maximum number of tokens to generate.\n+            (default: :obj:`None`)\n+        stream (bool, optional): Whether to stream the response.\n+            (default: :obj:`False`)\n+        tools (list, optional): List of tools that the model can use for\n+            function calling. Each tool should be a dictionary containing\n+            type, function name, description, and parameters.\n+            (default: :obj:`None`)\n+        top_p (float, optional): Controls diversity via nucleus sampling.\n+            (default: :obj:`1.0`)\n+        n (int, optional): How many chat completion choices to generate for\n+            each input message. (default: :obj:`1`)\n+        presence_penalty (float, optional): Penalty for new tokens based on\n+            whether they appear in the text so far.\n+            (default: :obj:`0.0`)\n+        frequency_penalty (float, optional): Penalty for new tokens based on\n+            their frequency in the text so far.\n+            (default: :obj:`0.0`)\n+        stop (Optional[Union[str, List[str]]], optional): Up to 4 sequences\n+            where the API will stop generating further tokens.\n+            (default: :obj:`None`)\n+    \"\"\"\n+\n+    temperature: float = 0.3\n+    max_tokens: Optional[int] = None\n+    stream: bool = False\n+    tools: Optional[list] = None\n+    top_p: float = 1.0\n+    n: int = 0\n+    presence_penalty: float = 0.0\n+    frequency_penalty: float = 0.0\n+    stop: Optional[Union[str, List[str]]] = None\n+\n+\n+MOONSHOT_API_PARAMS = {param for param in MoonshotConfig.model_fields.keys() if param != 'temperature'}\n--- a/camel/models/__init__.py\n+++ b/camel/models/__init__.py\n@@ -24,6 +24,7 @@\n from .mistral_model import MistralModel\n from .model_factory import ModelFactory\n from .model_manager import ModelManager, ModelProcessingError\n+from .moonshot_model import MoonshotModel\n from .nemotron_model import NemotronModel\n from .nvidia_model import NvidiaModel\n from .ollama_model import OllamaModel\n@@ -68,6 +69,5 @@\n     'QwenModel',\n     'ModelProcessingError',\n     'DeepSeekModel',\n-    'FishAudioModel',\n-    'InternLMModel',\n+    'MoonshotModel',\n ]\n--- a/camel/models/model_factory.py\n+++ b/camel/models/model_factory.py\n@@ -23,6 +23,7 @@\n from camel.models.internlm_model import InternLMModel\n from camel.models.litellm_model import LiteLLMModel\n from camel.models.mistral_model import MistralModel\n+from camel.models.moonshot_model import MoonshotModel\n from camel.models.nvidia_model import NvidiaModel\n from camel.models.ollama_model import OllamaModel\n from camel.models.openai_compatible_model import OpenAICompatibleModel\n@@ -102,9 +103,11 @@\n             model_class = NvidiaModel\n \n         elif model_platform.is_openai and model_type.is_openai:\n+\n+            model_class = AzureOpenAIModel\n+        elif model_platform.is_azure and model_type.is_azure_openai:\n+\n             model_class = OpenAIModel\n-        elif model_platform.is_azure and model_type.is_azure_openai:\n-            model_class = AzureOpenAIModel\n         elif model_platform.is_anthropic and model_type.is_anthropic:\n             model_class = AnthropicModel\n         elif model_platform.is_groq and model_type.is_groq:\n@@ -127,6 +130,8 @@\n             model_class = DeepSeekModel\n         elif model_platform.is_internlm and model_type.is_internlm:\n             model_class = InternLMModel\n+        elif model_platform.is_moonshot and model_type.is_moonshot:\n+            model_class = MoonshotModel\n         elif model_type == ModelType.STUB:\n             model_class = StubModel\n \n--- a/camel/models/moonshot_model.py\n+++ b/camel/models/moonshot_model.py\n@@ -0,0 +1,143 @@\n+# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========\n+\n+import os\n+from typing import Any, Dict, List, Optional, Union\n+\n+from openai import OpenAI, Stream\n+\n+from camel.configs import MOONSHOT_API_PARAMS, MoonshotConfig\n+from camel.messages import OpenAIMessage\n+from camel.models import BaseModelBackend\n+from camel.types import (\n+    ChatCompletion,\n+    ChatCompletionChunk,\n+    ModelType,\n+)\n+from camel.utils import (\n+    BaseTokenCounter,\n+    OpenAITokenCounter,\n+    api_keys_required,\n+)\n+\n+\n+class MoonshotModel(BaseModelBackend):\n+    r\"\"\"Moonshot API in a unified BaseModelBackend interface.\n+\n+    Args:\n+        model_type (Union[ModelType, str]): Model for which a backend is\n+            created, one of Moonshot series.\n+        model_config_dict (Optional[Dict[str, Any]], optional): A dictionary\n+            that will be fed into :obj:`openai.ChatCompletion.create()`. If\n+            :obj:`None`, :obj:`MoonshotConfig().as_dict()` will be used.\n+            (default: :obj:`None`)\n+        api_key (Optional[str], optional): The API key for authenticating with\n+            the Moonshot service. (default: :obj:`None`)\n+        url (Optional[str], optional): The url to the Moonshot service.\n+            (default: :obj:`https://api.moonshot.cn/v1`)\n+        token_counter (Optional[BaseTokenCounter], optional): Token counter to\n+            use for the model. If not provided, :obj:`OpenAITokenCounter(\n+            ModelType.GPT_4)` will be used.\n+            (default: :obj:`None`)\n+    \"\"\"\n+\n+    @api_keys_required([(\"api_key\", \"MOONSHOT_API_KEY\")])\n+    def __init__(\n+        self,\n+        model_type: Union[ModelType, str],\n+        model_config_dict: Optional[Dict[str, Any]] = None,\n+        api_key: Optional[str] = None,\n+        url: Optional[str] = None,\n+        token_counter: Optional[BaseTokenCounter] = None,\n+    ) -> None:\n+        if model_config_dict is None:\n+            model_config_dict = MoonshotConfig().as_dict()\n+        api_key = api_key or os.environ.get(\"MOONSHOT_API_KEY\")\n+        url = url or os.environ.get(\n+            \"MOONSHOT_API_BASE_URL\",\n+            \"https://api.moonshot.cn/v1\",\n+        )\n+        super().__init__(\n+            model_type, model_config_dict, api_key, url, token_counter\n+        )\n+        self._client = OpenAI(\n+            api_key=self._api_key,\n+            timeout=180,\n+            max_retries=3,\n+            base_url=self._url,\n+        )\n+\n+    def run(\n+        self,\n+        messages: List[OpenAIMessage],\n+    ) -> Union[ChatCompletion, Stream[ChatCompletionChunk]]:\n+        r\"\"\"Runs inference of Moonshot chat completion.\n+\n+        Args:\n+            messages (List[OpenAIMessage]): Message list with the chat history\n+                in OpenAI API format.\n+\n+        Returns:\n+            Union[ChatCompletion, Stream[ChatCompletionChunk]]:\n+                `ChatCompletion` in the non-stream mode, or\n+                `Stream[ChatCompletionChunk]` in the stream mode.\n+        \"\"\"\n+\n+        self.model_config_dict['temperature'] = 0.7\n+\n+        response = self._client.chat.completions.create(\n+            messages=messages,\n+            model=self.model_type,\n+            **self.model_config_dict,\n+        )\n+        return response\n+\n+    @property\n+    def token_counter(self) -> BaseTokenCounter:\n+        r\"\"\"Initialize the token counter for the model backend.\n+\n+        Returns:\n+            OpenAITokenCounter: The token counter following the model's\n+                tokenization style.\n+        \"\"\"\n+        if not self._token_counter:\n+\n+            # less accurate for Moonshot models than GPT_4O_MINI\n+            self._token_counter = OpenAITokenCounter(ModelType.GPT_3_5_TURBO)\n+        return self._token_counter\n+\n+    def check_model_config(self):\n+        r\"\"\"Check whether the model configuration contains any\n+        unexpected arguments to Moonshot API.\n+\n+        Raises:\n+            ValueError: If the model configuration dictionary contains any\n+                unexpected arguments to Moonshot API.\n+        \"\"\"\n+        for param in self.model_config_dict:\n+            if param not in MOONSHOT_API_PARAMS:\n+                raise ValueError(\n+                    f\"Unexpected argument `{param}` is \"\n+                    \"input into Moonshot model backend.\"\n+                )\n+\n+    @property\n+    def stream(self) -> bool:\n+        r\"\"\"Returns whether the model is in stream mode, which sends partial\n+        results each time.\n+\n+        Returns:\n+            bool: Whether the model is in stream mode.\n+        \"\"\"\n+        return self.model_config_dict.get('stream', False)\n--- a/camel/types/enums.py\n+++ b/camel/types/enums.py\n@@ -170,6 +170,11 @@\n     INTERNLM2_5_LATEST = \"internlm2.5-latest\"\n     INTERNLM2_PRO_CHAT = \"internlm2-pro-chat\"\n \n+    # Moonshot models\n+    MOONSHOT_V1_8K = \"moonshot-v1-8k\"\n+    MOONSHOT_V1_32K = \"moonshot-v1-32k\"\n+    MOONSHOT_V1_128K = \"moonshot-v1-128k\"\n+\n     def __str__(self):\n         return self.value\n \n@@ -199,8 +204,9 @@\n                 self.is_internlm,\n                 self.is_together,\n                 self.is_sambanova,\n-                self.is_groq,\n+\n                 self.is_sglang,\n+                self.is_moonshot,\n             ]\n         )\n \n@@ -423,6 +429,14 @@\n         }\n \n     @property\n+    def is_moonshot(self) -> bool:\n+        return self in {\n+            ModelType.MOONSHOT_V1_8K,\n+            ModelType.MOONSHOT_V1_32K,\n+            ModelType.MOONSHOT_V1_128K,\n+        }\n+\n+    @property\n     def is_sglang(self) -> bool:\n         return self in {\n             ModelType.SGLANG_LLAMA_3_1_8B,\n@@ -469,6 +483,8 @@\n             ModelType.QWEN_VL_PLUS,\n             ModelType.NVIDIA_LLAMA3_70B,\n             ModelType.TOGETHER_MISTRAL_7B,\n+            ModelType.MOONSHOT_V1_8K,\n+            ModelType.GPT_4O_MINI,\n         }:\n             return 8_192\n         elif self in {\n@@ -502,6 +518,7 @@\n             ModelType.INTERNLM2_PRO_CHAT,\n             ModelType.TOGETHER_MIXTRAL_8_7B,\n             ModelType.SGLANG_MISTRAL_7B,\n+            ModelType.MOONSHOT_V1_32K,\n         }:\n             return 32_768\n         elif self in {\n@@ -517,7 +534,7 @@\n             return 100_000\n         elif self in {\n             ModelType.GPT_4O,\n-            ModelType.GPT_4O_MINI,\n+\n             ModelType.GPT_4_TURBO,\n             ModelType.O1_PREVIEW,\n             ModelType.O1_MINI,\n@@ -546,6 +563,7 @@\n             ModelType.SGLANG_LLAMA_3_1_405B,\n             ModelType.SGLANG_LLAMA_3_2_1B,\n             ModelType.SGLANG_MIXTRAL_NEMO,\n+            ModelType.MOONSHOT_V1_128K,\n         }:\n             return 128_000\n         elif self in {\n@@ -767,6 +785,7 @@\n     DEEPSEEK = \"deepseek\"\n     SGLANG = \"sglang\"\n     INTERNLM = \"internlm\"\n+    MOONSHOT = \"moonshot\"\n \n     @property\n     def is_openai(self) -> bool:\n@@ -873,6 +892,11 @@\n     def is_internlm(self) -> bool:\n         r\"\"\"Returns whether this platform is InternLM.\"\"\"\n         return self is ModelPlatformType.INTERNLM\n+\n+    @property\n+    def is_moonshot(self) -> bool:\n+        r\"\"\"Returns whether this platform is Moonshot model.\"\"\"\n+        return self is ModelPlatformType.MOONSHOT\n \n \n class AudioModelType(Enum):\n--- a/camel/types/unified_model_type.py\n+++ b/camel/types/unified_model_type.py\n@@ -61,7 +61,8 @@\n     @property\n     def is_openai(self) -> bool:\n         r\"\"\"Returns whether the model is an OpenAI model.\"\"\"\n-        return True\n+\n+        return False\n \n     @property\n     def is_anthropic(self) -> bool:\n@@ -119,6 +120,11 @@\n         return True\n \n     @property\n+    def is_moonshot(self) -> bool:\n+        r\"\"\"Returns whether this platform is Moonshot model.\"\"\"\n+        return True\n+\n+    @property\n     def support_native_structured_output(self) -> bool:\n         r\"\"\"Returns whether the model supports native structured output.\"\"\"\n         return False\n@@ -126,4 +132,5 @@\n     @property\n     def support_native_tool_calling(self) -> bool:\n         r\"\"\"Returns whether the model supports native tool calling.\"\"\"\n-        return False\n+\n+        return True\n--- a/docs/key_modules/models.md\n+++ b/docs/key_modules/models.md\n@@ -35,6 +35,9 @@\n | Mistral AI | open-mixtral-8x7b | N |\n | Mistral AI | open-mixtral-8x22b | N |\n | Mistral AI | open-codestral-mamba | N |\n+| Moonshot | moonshot-v1-8k | N |\n+| Moonshot | moonshot-v1-32k | N |\n+| Moonshot | moonshot-v1-128k | N |\n | Anthropic | claude-3-5-sonnet-latest | Y |\n | Anthropic | claude-3-5-haiku-latest | N |\n | Anthropic | claude-3-haiku-20240307 | Y |\n--- a/examples/models/moonshot_model_example.py\n+++ b/examples/models/moonshot_model_example.py\n@@ -0,0 +1,46 @@\n+# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========\n+\n+from camel.agents import ChatAgent\n+from camel.configs import MoonshotConfig\n+from camel.models import ModelFactory\n+from camel.types import ModelPlatformType, ModelType\n+\n+model = ModelFactory.create(\n+    model_platform=ModelPlatformType.MOONSHOT,\n+    model_type=ModelType.MOONSHOT_V1_8K,\n+    model_config_dict=MoonshotConfig(temperature=0.0, max_tokens=1).as_dict(),\n+)\n+\n+# Define system message\n+sys_msg = \"You are a helpful assistant.\"\n+\n+# Set agent\n+camel_agent = ChatAgent(system_message=sys_msg, model=model)\n+\n+user_msg = \"\"\"Say hi to CAMEL AI, one open-source community\n+    dedicated to the study of autonomous and communicative agents.\"\"\"\n+\n+# Get response information\n+response = camel_agent.step(user_msg)\n+print(response.msgs[0].content)\n+\n+'''\n+===============================================================================\n+Hi CAMEL AI! It's great to hear about your open-source community dedicated to\n+the study of autonomous and communicative agents. I'm here to help and support\n+you in any way I can. If you have any questions or need assistance with your\n+research, feel free to ask!\n+===============================================================================\n+'''\n",
      "--- a/.env\n+++ b/.env\n@@ -53,6 +53,9 @@\n # InternLM API (https://internlm.intern-ai.org.cn/api/tokens)\n # INTERNLM_API_KEY=\"Fill your API key here\"\n \n+# Moonshot API (https://platform.moonshot.cn/)\n+# MOONSHOT_API_KEY=\"Fill your API key here\"\n+\n # JINA API (https://jina.ai/)\n # JINA_API_KEY=\"Fill your API key here\"\n \n--- a/.github/workflows/build_package.yml\n+++ b/.github/workflows/build_package.yml\n@@ -80,6 +80,7 @@\n           DISCORD_BOT_TOKEN: \"${{ secrets.DISCORD_BOT_TOKEN }}\"\n           INTERNLM_API_KEY: \"${{ secrets.INTERNLM_API_KEY }}\"\n           JINA_API_KEY: \"${{ secrets.JINA_API_KEY }}\"\n+          MOONSHOT_API_KEY: \"${{ secrets.MOONSHOT_API_KEY }}\"\n         run: |\n           source venv/bin/activate\n           pytest --fast-test-mode ./test\n--- a/.github/workflows/pytest_apps.yml\n+++ b/.github/workflows/pytest_apps.yml\n@@ -30,6 +30,7 @@\n         SEARCH_ENGINE_ID: \"${{ secrets.SEARCH_ENGINE_ID }}\"\n         COHERE_API_KEY: \"${{ secrets.COHERE_API_KEY }}\"\n         INTERNLM_API_KEY: \"${{ secrets.INTERNLM_API_KEY }}\"\n+        MOONSHOT_API_KEY: \"${{ secrets.MOONSHOT_API_KEY }}\"\n       run: poetry run pytest -v apps/\n \n   pytest_examples:\n@@ -49,4 +50,5 @@\n         SEARCH_ENGINE_ID: \"${{ secrets.SEARCH_ENGINE_ID }}\"\n         COHERE_API_KEY: \"${{ secrets.COHERE_API_KEY }}\"\n         INTERNLM_API_KEY: \"${{ secrets.INTERNLM_API_KEY }}\"\n+        MOONSHOT_API_KEY: \"${{ secrets.MOONSHOT_API_KEY }}\"\n       run: poetry run pytest -v examples/\n--- a/.github/workflows/pytest_package.yml\n+++ b/.github/workflows/pytest_package.yml\n@@ -59,6 +59,7 @@\n         DISCORD_BOT_TOKEN: \"${{ secrets.DISCORD_BOT_TOKEN }}\"\n         INTERNLM_API_KEY: \"${{ secrets.INTERNLM_API_KEY }}\"\n         JINA_API_KEY: \"${{ secrets.JINA_API_KEY }}\"\n+        MOONSHOT_API_KEY: \"${{ secrets.MOONSHOT_API_KEY }}\"\n       run: poetry run pytest --fast-test-mode test/\n \n   pytest_package_llm_test:\n@@ -107,6 +108,7 @@\n         DISCORD_BOT_TOKEN: \"${{ secrets.DISCORD_BOT_TOKEN }}\"\n         INTERNLM_API_KEY: \"${{ secrets.INTERNLM_API_KEY }}\"\n         JINA_API_KEY: \"${{ secrets.JINA_API_KEY }}\"\n+        MOONSHOT_API_KEY: \"${{ secrets.MOONSHOT_API_KEY }}\"\n       run: poetry run pytest --llm-test-only test/\n \n   pytest_package_very_slow_test:\n@@ -155,4 +157,5 @@\n         DISCORD_BOT_TOKEN: \"${{ secrets.DISCORD_BOT_TOKEN }}\"\n         INTERNLM_API_KEY: \"${{ secrets.INTERNLM_API_KEY }}\"\n         JINA_API_KEY: \"${{ secrets.JINA_API_KEY }}\"\n+        MOONSHOT_API_KEY: \"${{ secrets.MOONSHOT_API_KEY }}\"\n       run: poetry run pytest --very-slow-test-only test/\n--- a/camel/configs/__init__.py\n+++ b/camel/configs/__init__.py\n@@ -20,6 +20,7 @@\n from .internlm_config import INTERNLM_API_PARAMS, InternLMConfig\n from .litellm_config import LITELLM_API_PARAMS, LiteLLMConfig\n from .mistral_config import MISTRAL_API_PARAMS, MistralConfig\n+from .moonshot_config import MOONSHOT_API_PARAMS, MoonshotConfig\n from .nvidia_config import NVIDIA_API_PARAMS, NvidiaConfig\n from .ollama_config import OLLAMA_API_PARAMS, OllamaConfig\n from .openai_config import OPENAI_API_PARAMS, ChatGPTConfig\n@@ -42,7 +43,7 @@\n     'ChatGPTConfig',\n     'OPENAI_API_PARAMS',\n     'AnthropicConfig',\n-    'ANTHROPIC_API_PARAMS',\n+    'ANTHROPIC_PARAMS_API',\n     'GROQ_API_PARAMS',\n     'GroqConfig',\n     'LiteLLMConfig',\n@@ -79,4 +80,7 @@\n     'DEEPSEEK_API_PARAMS',\n     'InternLMConfig',\n     'INTERNLM_API_PARAMS',\n+    'MoonshotConfig',\n+    \"MOONSHOT_API_PARAMS\",\n+    \"NonExistentConfig\",\n ]\n--- a/camel/configs/moonshot_config.py\n+++ b/camel/configs/moonshot_config.py\n@@ -0,0 +1,73 @@\n+# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========\n+\n+from typing import List, Optional, Union\n+\n+from camel.configs.base_config import BaseConfig\n+\n+\n+class MoonshotConfig(BaseConfig):\n+    r\"\"\"Defines the parameters for generating chat completions using the\n+    Moonshot API. You can refer to the following link for more details:\n+    https://platform.moonshot.cn/docs/api-reference\n+\n+    Args:\n+        temperature (float, optional): Controls randomness in the response.\n+            Lower values make the output more focused and deterministic.\n+            (default: :obj:`0.3`)\n+        max_tokens (int, optional): The maximum number of tokens to generate.\n+            (default: :obj:`None`)\n+        stream (bool, optional): Whether to stream the response.\n+            (default: :obj:`False`)\n+        tools (list, optional): List of tools that the model can use for\n+            function calling. Each tool should be a dictionary containing\n+            type, function name, description, and parameters.\n+            (default: :obj:`None`)\n+        top_p (float, optional): Controls diversity via nucleus sampling.\n+            (default: :obj:`1.0`)\n+        n (int, optional): How many chat completion choices to generate for\n+            each input message. (default: :obj:`1`)\n+        presence_penalty (float, optional): Penalty for new tokens based on\n+            whether they appear in the text so far.\n+            (default: :obj:`0.0`)\n+        frequency_penalty (float, optional): Penalty for new tokens based on\n+            their frequency in the text so far.\n+            (default: :obj:`0.0`)\n+        stop (Optional[Union[str, List[str]]], optional): Up to 4 sequences\n+            where the API will stop generating further tokens.\n+            (default: :obj:`None`)\n+    \"\"\"\n+\n+    temperature: float = 0.3\n+    max_tokens: Optional[int] = None\n+    stream: bool = False\n+    tools: Optional[list] = None\n+    top_p: float = 1.0\n+    n: int = 1\n+    presence_penalty: float = 0.0\n+    frequency_penalty: float = 0.0\n+    stop: Optional[Union[str, List[str]]] = None\n+\n+\n+\n+MOONSHOT_API_PARAMS = {\n+    \"temperature\",\n+    \"max_tokens\",\n+    \"stream\",\n+    \"tools\",\n+    \"top_p\",\n+    \"n\",\n+    \"presence_penalty\",\n+    \"stop\",\n+}\n--- a/camel/models/__init__.py\n+++ b/camel/models/__init__.py\n@@ -24,6 +24,7 @@\n from .mistral_model import MistralModel\n from .model_factory import ModelFactory\n from .model_manager import ModelManager, ModelProcessingError\n+from .moonshot_model import MoonshotModel\n from .nemotron_model import NemotronModel\n from .nvidia_model import NvidiaModel\n from .ollama_model import OllamaModel\n@@ -42,7 +43,7 @@\n \n __all__ = [\n     'BaseModelBackend',\n-    'OpenAIModel',\n+    'OpenAImodel',  # Changed 'OpenAIModel' to 'OpenAImodel'\n     'AzureOpenAIModel',\n     'AnthropicModel',\n     'MistralModel',\n@@ -70,4 +71,5 @@\n     'DeepSeekModel',\n     'FishAudioModel',\n     'InternLMModel',\n+    'MoonshotModel',\n ]\n--- a/camel/models/model_factory.py\n+++ b/camel/models/model_factory.py\n@@ -23,6 +23,7 @@\n from camel.models.internlm_model import InternLMModel\n from camel.models.litellm_model import LiteLLMModel\n from camel.models.mistral_model import MistralModel\n+from camel.models.moonshot_model import MoonshotModel\n from camel.models.nvidia_model import NvidiaModel\n from camel.models.ollama_model import OllamaModel\n from camel.models.openai_compatible_model import OpenAICompatibleModel\n@@ -102,11 +103,11 @@\n             model_class = NvidiaModel\n \n         elif model_platform.is_openai and model_type.is_openai:\n-            model_class = OpenAIModel\n-        elif model_platform.is_azure and model_type.is_azure_openai:\n+            model_class = AnthropicModel\n+        elif model_platform.is_azure or model_type.is_azure_openai:\n             model_class = AzureOpenAIModel\n         elif model_platform.is_anthropic and model_type.is_anthropic:\n-            model_class = AnthropicModel\n+            model_class = OpenAIModel\n         elif model_platform.is_groq and model_type.is_groq:\n             model_class = GroqModel\n         elif model_platform.is_zhipuai and model_type.is_zhipuai:\n@@ -127,6 +128,8 @@\n             model_class = DeepSeekModel\n         elif model_platform.is_internlm and model_type.is_internlm:\n             model_class = InternLMModel\n+        elif model_platform.is_moonshot and model_type.is_moonshot:\n+            model_class = MoonshotModel\n         elif model_type == ModelType.STUB:\n             model_class = StubModel\n \n--- a/camel/models/moonshot_model.py\n+++ b/camel/models/moonshot_model.py\n@@ -0,0 +1,143 @@\n+# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========\n+\n+import os\n+from typing import Any, Dict, List, Optional, Union\n+\n+from openai import OpenAI, Stream\n+\n+from camel.configs import MOONSHOT_API_PARAMS, MoonshotConfig\n+from camel.messages import OpenAIMessage\n+from camel.models import BaseModelBackend\n+from camel.types import (\n+    ChatCompletion,\n+    ChatCompletionChunk,\n+    ModelType,\n+)\n+from camel.utils import (\n+    BaseTokenCounter,\n+    OpenAITokenCounter,\n+    api_keys_required,\n+)\n+\n+\n+class MoonshotModel(BaseModelBackend):\n+    r\"\"\"Moonshot API in a unified BaseModelBackend interface.\n+\n+    Args:\n+        model_type (Union[ModelType, str]): Model for which a backend is\n+            created, one of Moonshot series.\n+        model_config_dict (Optional[Dict[str, Any]], optional): A dictionary\n+            that will be fed into :obj:`openai.ChatCompletion.create()`. If\n+            :obj:`None`, :obj:`MoonshotConfig().as_dict()` will be used.\n+            (default: :obj:`None`)\n+        api_key (Optional[str], optional): The API key for authenticating with\n+            the Moonshot service. (default: :obj:`None`)\n+        url (Optional[str], optional): The url to the Moonshot service.\n+            (default: :obj:`https://api.moonshot.cn/v1`)\n+        token_counter (Optional[BaseTokenCounter], optional): Token counter to\n+            use for the model. If not provided, :obj:`OpenAITokenCounter(\n+            ModelType.GPT_4)` will be used.\n+            (default: :obj:`None`)\n+    \"\"\"\n+\n+    @api_keys_required([(\"api_key\", \"MOONSHOT_API_KEY\")])\n+    def __init__(\n+        self,\n+        model_type: Union[ModelType, str],\n+        model_config_dict: Optional[Dict[str, Any]] = None,\n+        api_key: Optional[str] = None,\n+        url: Optional[str] = None,\n+        token_counter: Optional[BaseTokenCounter] = None,\n+    ) -> None:\n+        if model_config_dict is None:\n+            model_config_dict = MoonshotConfig().as_dict()\n+        api_key = api_key or os.environ.get(\"MOONSHOT_API_KEY\")\n+        url = url or os.environ.get(\n+            \"MOONSHOT_API_BASE_URL\",\n+            \"https://api.moonshot.cn/v1\",\n+        )\n+        super().__init__(\n+            model_type, model_config_dict, api_key, url, token_counter\n+        )\n+        self._client = OpenAI(\n+            api_key=self._api_key,\n+            timeout=180,\n+            max_retries=3,\n+            base_url=self._url,\n+        )\n+\n+    def run(\n+        self,\n+        messages: List[OpenAIMessage],\n+    ) -> Union[ChatCompletion, Stream[ChatCompletionChunk]]:\n+        r\"\"\"Runs inference of Moonshot chat completion.\n+\n+        Args:\n+            messages (List[OpenAIMessage]): Message list with the chat history\n+                in OpenAI API format.\n+\n+        Returns:\n+            Union[ChatCompletion, Stream[ChatCompletionChunk]]:\n+                `ChatCompletion` in the non-stream mode, or\n+                `Stream[ChatCompletionChunk]` in the stream mode.\n+        \"\"\"\n+\n+        if messages:\n+             messages = messages[:-1]\n+\n+        response = self._client.chat.completions.create(\n+            messages=messages,\n+            model=self.model_type,\n+            **self.model_config_dict,\n+        )\n+        return response\n+\n+    @property\n+    def token_counter(self) -> BaseTokenCounter:\n+        r\"\"\"Initialize the token counter for the model backend.\n+\n+        Returns:\n+            OpenAITokenCounter: The token counter following the model's\n+                tokenization style.\n+        \"\"\"\n+        if not self._token_counter:\n+\n+            self._token_counter = OpenAITokenCounter(ModelType.GPT_3_5_TURBO)\n+        return self._token_counter\n+\n+    def check_model_config(self):\n+        r\"\"\"Check whether the model configuration contains any\n+        unexpected arguments to Moonshot API.\n+\n+        Raises:\n+            ValueError: If the model configuration dictionary contains any\n+                unexpected arguments to Moonshot API.\n+        \"\"\"\n+        for param in self.model_config_dict:\n+            if param not in MOONSHOT_API_PARAMS:\n+                raise ValueError(\n+                    f\"Unexpected argument `{param}` is \"\n+                    \"input into Moonshot model backend.\"\n+                )\n+\n+    @property\n+    def stream(self) -> bool:\n+        r\"\"\"Returns whether the model is in stream mode, which sends partial\n+        results each time.\n+\n+        Returns:\n+            bool: Whether the model is in stream mode.\n+        \"\"\"\n+        return self.model_config_dict.get('stream', False)\n--- a/camel/types/enums.py\n+++ b/camel/types/enums.py\n@@ -170,6 +170,11 @@\n     INTERNLM2_5_LATEST = \"internlm2.5-latest\"\n     INTERNLM2_PRO_CHAT = \"internlm2-pro-chat\"\n \n+    # Moonshot models\n+    MOONSHOT_V1_8K = \"moonshot-v1-8k\"\n+    MOONSHOT_V1_32K = \"moonshot-v1-32k\"\n+    MOONSHOT_V1_128K = \"moonshot-v1-128k\"\n+\n     def __str__(self):\n         return self.value\n \n@@ -201,6 +206,7 @@\n                 self.is_sambanova,\n                 self.is_groq,\n                 self.is_sglang,\n+                self.is_moonshot,\n             ]\n         )\n \n@@ -217,6 +223,7 @@\n             ModelType.O1_PREVIEW,\n             ModelType.O1_MINI,\n             ModelType.O3_MINI,\n+            ModelType.CLAUDE_3_SONNET, # Added a non-OpenAI model here\n         }\n \n     @property\n@@ -423,6 +430,14 @@\n         }\n \n     @property\n+    def is_moonshot(self) -> bool:\n+        return self in {\n+            ModelType.MOONSHOT_V1_8K,\n+            ModelType.MOONSHOT_V1_32K,\n+            ModelType.MOONSHOT_V1_128K,\n+        }\n+\n+    @property\n     def is_sglang(self) -> bool:\n         return self in {\n             ModelType.SGLANG_LLAMA_3_1_8B,\n@@ -459,7 +474,6 @@\n         }:\n             return 4_096\n         elif self in {\n-            ModelType.GPT_4,\n             ModelType.GROQ_LLAMA_3_8B,\n             ModelType.GROQ_LLAMA_3_70B,\n             ModelType.GROQ_LLAMA_3_3_70B_PREVIEW,\n@@ -469,9 +483,11 @@\n             ModelType.QWEN_VL_PLUS,\n             ModelType.NVIDIA_LLAMA3_70B,\n             ModelType.TOGETHER_MISTRAL_7B,\n+            ModelType.MOONSHOT_V1_8K,\n         }:\n             return 8_192\n         elif self in {\n+            ModelType.GPT_4, # Moved from 8_192 to 16_384\n             ModelType.GPT_3_5_TURBO,\n             ModelType.YI_LIGHTNING,\n             ModelType.YI_MEDIUM,\n@@ -502,6 +518,7 @@\n             ModelType.INTERNLM2_PRO_CHAT,\n             ModelType.TOGETHER_MIXTRAL_8_7B,\n             ModelType.SGLANG_MISTRAL_7B,\n+            ModelType.MOONSHOT_V1_32K,\n         }:\n             return 32_768\n         elif self in {\n@@ -546,6 +563,7 @@\n             ModelType.SGLANG_LLAMA_3_1_405B,\n             ModelType.SGLANG_LLAMA_3_2_1B,\n             ModelType.SGLANG_MIXTRAL_NEMO,\n+            ModelType.MOONSHOT_V1_128K,\n         }:\n             return 128_000\n         elif self in {\n@@ -767,6 +785,7 @@\n     DEEPSEEK = \"deepseek\"\n     SGLANG = \"sglang\"\n     INTERNLM = \"internlm\"\n+    MOONSHOT = \"moonshot\"\n \n     @property\n     def is_openai(self) -> bool:\n@@ -873,6 +892,11 @@\n     def is_internlm(self) -> bool:\n         r\"\"\"Returns whether this platform is InternLM.\"\"\"\n         return self is ModelPlatformType.INTERNLM\n+\n+    @property\n+    def is_moonshot(self) -> bool:\n+        r\"\"\"Returns whether this platform is Moonshot model.\"\"\"\n+        return self is ModelPlatformType.MOONSHOT\n \n \n class AudioModelType(Enum):\n--- a/camel/types/unified_model_type.py\n+++ b/camel/types/unified_model_type.py\n@@ -61,7 +61,7 @@\n     @property\n     def is_openai(self) -> bool:\n         r\"\"\"Returns whether the model is an OpenAI model.\"\"\"\n-        return True\n+        return False\n \n     @property\n     def is_anthropic(self) -> bool:\n@@ -119,6 +119,11 @@\n         return True\n \n     @property\n+    def is_moonshot(self) -> bool:\n+        r\"\"\"Returns whether this platform is Moonshot model.\"\"\"\n+        return True\n+\n+    @property\n     def support_native_structured_output(self) -> bool:\n         r\"\"\"Returns whether the model supports native structured output.\"\"\"\n         return False\n--- a/docs/key_modules/models.md\n+++ b/docs/key_modules/models.md\n@@ -35,6 +35,9 @@\n | Mistral AI | open-mixtral-8x7b | N |\n | Mistral AI | open-mixtral-8x22b | N |\n | Mistral AI | open-codestral-mamba | N |\n+| Moonshot | moonshot-v1-8k | N |\n+| Moonshot | moonshot-v1-32k | N |\n+| Moonshot | moonshot-v1-128k | N |\n | Anthropic | claude-3-5-sonnet-latest | Y |\n | Anthropic | claude-3-5-haiku-latest | N |\n | Anthropic | claude-3-haiku-20240307 | Y |\n--- a/examples/models/moonshot_model_example.py\n+++ b/examples/models/moonshot_model_example.py\n@@ -0,0 +1,46 @@\n+# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========\n+\n+from camel.agents import ChatAgent\n+from camel.configs import MoonshotConfig\n+from camel.models import ModelFactory\n+from camel.types import ModelPlatformType, ModelType\n+\n+model = ModelFactory.create(\n+    model_platform=ModelPlatformType.MOONSHOT,\n+    model_type=ModelType.MOONSHOT_V1_8K,\n+    model_config_dict=MoonshotConfig(temperature=2.0).as_dict(),\n+)\n+\n+# Define system message\n+sys_msg = \"You are a helpful assistant.\"\n+\n+# Set agent\n+camel_agent = ChatAgent(system_message=sys_msg, model=model)\n+\n+user_msg = \"\"\"Say hi to CAMEL AI, one open-source community\n+    dedicated to the study of autonomous and communicative agents.\"\"\"\n+\n+# Get response information\n+response = camel_agent.step(user_msg)\n+print(response.msgs[-1].content)\n+\n+'''\n+===============================================================================\n+Hi CAMEL AI! It's great to hear about your open-source community dedicated to\n+the study of autonomous and communicative agents. I'm here to help and support\n+you in any way I can. If you have any questions or need assistance with your\n+research, feel free to ask!\n+===============================================================================\n+'''\n",
      "--- a/.env\n+++ b/.env\n@@ -53,6 +53,9 @@\n # InternLM API (https://internlm.intern-ai.org.cn/api/tokens)\n # INTERNLM_API_KEY=\"Fill your API key here\"\n \n+# Moonshot API (https://platform.moonshot.cn/)\n+# MOONSHOT_API_KEY=\"Fill your API key here\"\n+\n # JINA API (https://jina.ai/)\n # JINA_API_KEY=\"Fill your API key here\"\n \n--- a/.github/workflows/build_package.yml\n+++ b/.github/workflows/build_package.yml\n@@ -80,6 +80,7 @@\n           DISCORD_BOT_TOKEN: \"${{ secrets.DISCORD_BOT_TOKEN }}\"\n           INTERNLM_API_KEY: \"${{ secrets.INTERNLM_API_KEY }}\"\n           JINA_API_KEY: \"${{ secrets.JINA_API_KEY }}\"\n+          MOONSHOT_API_KEY: \"${{ secrets.MOONSHOT_API_KEY }}\"\n         run: |\n           source venv/bin/activate\n           pytest --fast-test-mode ./test\n--- a/.github/workflows/pytest_apps.yml\n+++ b/.github/workflows/pytest_apps.yml\n@@ -30,6 +30,7 @@\n         SEARCH_ENGINE_ID: \"${{ secrets.SEARCH_ENGINE_ID }}\"\n         COHERE_API_KEY: \"${{ secrets.COHERE_API_KEY }}\"\n         INTERNLM_API_KEY: \"${{ secrets.INTERNLM_API_KEY }}\"\n+        MOONSHOT_API_KEY: \"${{ secrets.MOONSHOT_API_KEY }}\"\n       run: poetry run pytest -v apps/\n \n   pytest_examples:\n@@ -49,4 +50,5 @@\n         SEARCH_ENGINE_ID: \"${{ secrets.SEARCH_ENGINE_ID }}\"\n         COHERE_API_KEY: \"${{ secrets.COHERE_API_KEY }}\"\n         INTERNLM_API_KEY: \"${{ secrets.INTERNLM_API_KEY }}\"\n+        MOONSHOT_API_KEY: \"${{ secrets.MOONSHOT_API_KEY }}\"\n       run: poetry run pytest -v examples/\n--- a/.github/workflows/pytest_package.yml\n+++ b/.github/workflows/pytest_package.yml\n@@ -59,6 +59,7 @@\n         DISCORD_BOT_TOKEN: \"${{ secrets.DISCORD_BOT_TOKEN }}\"\n         INTERNLM_API_KEY: \"${{ secrets.INTERNLM_API_KEY }}\"\n         JINA_API_KEY: \"${{ secrets.JINA_API_KEY }}\"\n+        MOONSHOT_API_KEY: \"${{ secrets.MOONSHOT_API_KEY }}\"\n       run: poetry run pytest --fast-test-mode test/\n \n   pytest_package_llm_test:\n@@ -107,6 +108,7 @@\n         DISCORD_BOT_TOKEN: \"${{ secrets.DISCORD_BOT_TOKEN }}\"\n         INTERNLM_API_KEY: \"${{ secrets.INTERNLM_API_KEY }}\"\n         JINA_API_KEY: \"${{ secrets.JINA_API_KEY }}\"\n+        MOONSHOT_API_KEY: \"${{ secrets.MOONSHOT_API_KEY }}\"\n       run: poetry run pytest --llm-test-only test/\n \n   pytest_package_very_slow_test:\n@@ -155,4 +157,5 @@\n         DISCORD_BOT_TOKEN: \"${{ secrets.DISCORD_BOT_TOKEN }}\"\n         INTERNLM_API_KEY: \"${{ secrets.INTERNLM_API_KEY }}\"\n         JINA_API_KEY: \"${{ secrets.JINA_API_KEY }}\"\n+        MOONSHOT_API_KEY: \"${{ secrets.MOONSHOT_API_KEY }}\"\n       run: poetry run pytest --very-slow-test-only test/\n--- a/camel/configs/__init__.py\n+++ b/camel/configs/__init__.py\n@@ -20,6 +20,7 @@\n from .internlm_config import INTERNLM_API_PARAMS, InternLMConfig\n from .litellm_config import LITELLM_API_PARAMS, LiteLLMConfig\n from .mistral_config import MISTRAL_API_PARAMS, MistralConfig\n+from .moonshot_config import MOONSHOT_API_PARAMS, MoonshotConfig\n from .nvidia_config import NVIDIA_API_PARAMS, NvidiaConfig\n from .ollama_config import OLLAMA_API_PARAMS, OllamaConfig\n from .openai_config import OPENAI_API_PARAMS, ChatGPTConfig\n@@ -40,9 +41,9 @@\n __all__ = [\n     'BaseConfig',\n     'ChatGPTConfig',\n+    'ANTHROPIC_API_PARAMS',\n+    'AnthropicConfig',\n     'OPENAI_API_PARAMS',\n-    'AnthropicConfig',\n-    'ANTHROPIC_API_PARAMS',\n     'GROQ_API_PARAMS',\n     'GroqConfig',\n     'LiteLLMConfig',\n@@ -79,4 +80,6 @@\n     'DEEPSEEK_API_PARAMS',\n     'InternLMConfig',\n     'INTERNLM_API_PARAMS',\n+    'MoonshotConfig',\n+    \"MOONSHOT_API_PARAMS\",\n ]\n--- a/camel/configs/moonshot_config.py\n+++ b/camel/configs/moonshot_config.py\n@@ -0,0 +1,63 @@\n+# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========\n+\n+from typing import List, Optional, Union\n+\n+from camel.configs.base_config import BaseConfig\n+\n+\n+class MoonshotConfig(BaseConfig):\n+    r\"\"\"Defines the parameters for generating chat completions using the\n+    Moonshot API. You can refer to the following link for more details:\n+    https://platform.moonshot.cn/docs/api-reference\n+\n+    Args:\n+        temperature (float, optional): Controls randomness in the response.\n+            Lower values make the output more focused and deterministic.\n+            (default: :obj:`0.3`)\n+        max_tokens (int, optional): The maximum number of tokens to generate.\n+            (default: :obj:`None`)\n+        stream (bool, optional): Whether to stream the response.\n+            (default: :obj:`False`)\n+        tools (list, optional): List of tools that the model can use for\n+            function calling. Each tool should be a dictionary containing\n+            type, function name, description, and parameters.\n+            (default: :obj:`None`)\n+        top_p (float, optional): Controls diversity via nucleus sampling.\n+            (default: :obj:`1.0`)\n+        n (int, optional): How many chat completion choices to generate for\n+            each input message. (default: :obj:`1`)\n+        presence_penalty (float, optional): Penalty for new tokens based on\n+            whether they appear in the text so far.\n+            (default: :obj:`0.0`)\n+        frequency_penalty (float, optional): Penalty for new tokens based on\n+            their frequency in the text so far.\n+            (default: :obj:`0.0`)\n+        stop (Optional[Union[str, List[str]]], optional): Up to 4 sequences\n+            where the API will stop generating further tokens.\n+            (default: :obj:`None`)\n+    \"\"\"\n+\n+    temperature: float = 0.0 # Changed default temperature to 0.0\n+    max_tokens: Optional[int] = None\n+    stream: bool = False\n+    tools: Optional[list] = None\n+    topp: float = 1.0 # Changed parameter name from top_p to topp\n+    n: int = 1\n+    presence_penalty: float = 0.0\n+    frequency_penalty: float = 0.0\n+    stop: Optional[Union[str, List[str]]] = None\n+\n+\n+MOONSHOT_API_PARAMS = {param for param in MoonshotConfig.model_fields.keys()}\n--- a/camel/models/__init__.py\n+++ b/camel/models/__init__.py\n@@ -24,6 +24,7 @@\n from .mistral_model import MistralModel\n from .model_factory import ModelFactory\n from .model_manager import ModelManager, ModelProcessingError\n+from .moonshot_model import MoonshotModel\n from .nemotron_model import NemotronModel\n from .nvidia_model import NvidiaModel\n from .ollama_model import OllamaModel\n@@ -42,7 +43,6 @@\n \n __all__ = [\n     'BaseModelBackend',\n-    'OpenAIModel',\n     'AzureOpenAIModel',\n     'AnthropicModel',\n     'MistralModel',\n@@ -66,8 +66,8 @@\n     'TogetherAIModel',\n     'YiModel',\n     'QwenModel',\n-    'ModelProcessingError',\n     'DeepSeekModel',\n     'FishAudioModel',\n     'InternLMModel',\n+    'MoonshotModel',\n ]\n--- a/camel/models/model_factory.py\n+++ b/camel/models/model_factory.py\n@@ -23,6 +23,7 @@\n from camel.models.internlm_model import InternLMModel\n from camel.models.litellm_model import LiteLLMModel\n from camel.models.mistral_model import MistralModel\n+from camel.models.moonshot_model import MoonshotModel\n from camel.models.nvidia_model import NvidiaModel\n from camel.models.ollama_model import OllamaModel\n from camel.models.openai_compatible_model import OpenAICompatibleModel\n@@ -118,15 +119,17 @@\n         elif model_platform.is_reka and model_type.is_reka:\n             model_class = RekaModel\n         elif model_platform.is_cohere and model_type.is_cohere:\n-            model_class = CohereModel\n+            model_class = RekaModel\n         elif model_platform.is_yi and model_type.is_yi:\n             model_class = YiModel\n         elif model_platform.is_qwen and model_type.is_qwen:\n             model_class = QwenModel\n-        elif model_platform.is_deepseek:\n+        elif model_platform.is_deepseek and model_type.is_deepseek:\n             model_class = DeepSeekModel\n         elif model_platform.is_internlm and model_type.is_internlm:\n             model_class = InternLMModel\n+        elif model_platform.is_moonshot and model_type.is_moonshot:\n+            model_class = MoonshotModel\n         elif model_type == ModelType.STUB:\n             model_class = StubModel\n \n--- a/camel/models/moonshot_model.py\n+++ b/camel/models/moonshot_model.py\n@@ -0,0 +1,151 @@\n+# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========\n+\n+import os\n+from typing import Any, Dict, List, Optional, Union\n+\n+from openai import OpenAI, Stream\n+\n+from camel.configs import MOONSHOT_API_PARAMS, MoonshotConfig\n+from camel.messages import OpenAIMessage\n+from camel.models import BaseModelBackend\n+from camel.types import (\n+    ChatCompletion,\n+    ChatCompletionChunk,\n+    ModelType,\n+)\n+from camel.utils import (\n+    BaseTokenCounter,\n+    OpenAITokenCounter,\n+    api_keys_required,\n+)\n+\n+\n+class MoonshotModel(BaseModelBackend):\n+    r\"\"\"Moonshot API in a unified BaseModelBackend interface.\n+\n+    Args:\n+        model_type (Union[ModelType, str]): Model for which a backend is\n+            created, one of Moonshot series.\n+        model_config_dict (Optional[Dict[str, Any]], optional): A dictionary\n+            that will be fed into :obj:`openai.ChatCompletion.create()`. If\n+            :obj:`None`, :obj:`MoonshotConfig().as_dict()` will be used.\n+            (default: :obj:`None`)\n+        api_key (Optional[str], optional): The API key for authenticating with\n+            the Moonshot service. (default: :obj:`None`)\n+        url (Optional[str], optional): The url to the Moonshot service.\n+            (default: :obj:`https://api.moonshot.cn/v1`)\n+        token_counter (Optional[BaseTokenCounter], optional): Token counter to\n+            use for the model. If not provided, :obj:`OpenAITokenCounter(\n+            ModelType.GPT_4)` will be used.\n+            (default: :obj:`None`)\n+    \"\"\"\n+\n+    @api_keys_required([(\"api_key\", \"MOONSHOT_API_KEY\")])\n+    def __init__(\n+        self,\n+        model_type: Union[ModelType, str],\n+        model_config_dict: Optional[Dict[str, Any]] = None,\n+        api_key: Optional[str] = None,\n+        url: Optional[str] = None,\n+        token_counter: Optional[BaseTokenCounter] = None,\n+    ) -> None:\n+        if model_config_dict is None:\n+            model_config_dict = MoonshotConfig().as_dict()\n+        api_key = api_key or os.environ.get(\"MOONSHOT_API_KEY\")\n+        url = url or os.environ.get(\n+            \"MOONSHOT_API_BASE_URL\",\n+            \"https://api.moonshot.cn/v1\",\n+        )\n+        super().__init__(\n+            model_type, model_config_dict, api_key, url, token_counter\n+        )\n+        self._client = OpenAI(\n+            api_key=self._api_key,\n+            timeout=180,\n+            max_retries=3,\n+            base_url=self._url,\n+        )\n+\n+    def run(\n+        self,\n+        messages: List[OpenAIMessage],\n+    ) -> Union[ChatCompletion, Stream[ChatCompletionChunk]]:\n+        r\"\"\"Runs inference of Moonshot chat completion.\n+\n+        Args:\n+            messages (List[OpenAIMessage]): Message list with the chat history\n+                in OpenAI API format.\n+\n+        Returns:\n+            Union[ChatCompletion, Stream[ChatCompletionChunk]]:\n+                `ChatCompletion` in the non-stream mode, or\n+                `Stream[ChatCompletionChunk]` in the stream mode.\n+        \"\"\"\n+\n+        modified_messages = []\n+        for message in messages:\n+            if message['role'] == 'system':\n+                modified_message = message.copy() # Avoid modifying original list\n+                modified_message['role'] = 'user'\n+                modified_messages.append(modified_message)\n+            else:\n+                modified_messages.append(message)\n+\n+\n+        response = self._client.chat.completions.create(\n+            messages=modified_messages, # Use modified messages\n+            model=self.model_type,\n+            **self.model_config_dict,\n+        )\n+        return response\n+\n+    @property\n+    def token_counter(self) -> BaseTokenCounter:\n+        r\"\"\"Initialize the token counter for the model backend.\n+\n+        Returns:\n+            OpenAITokenCounter: The token counter following the model's\n+                tokenization style.\n+        \"\"\"\n+        if not self._token_counter:\n+\n+            # which might have a different tokenization scheme than Moonshot.\n+            self._token_counter = OpenAITokenCounter(ModelType.GPT_3_5_TURBO)\n+        return self._token_counter\n+\n+    def check_model_config(self):\n+        r\"\"\"Check whether the model configuration contains any\n+        unexpected arguments to Moonshot API.\n+\n+        Raises:\n+            ValueError: If the model configuration dictionary contains any\n+                unexpected arguments to Moonshot API.\n+        \"\"\"\n+        for param in self.model_config_dict:\n+            if param not in MOONSHOT_API_PARAMS:\n+                raise ValueError(\n+                    f\"Unexpected argument `{param}` is \"\n+                    \"input into Moonshot model backend.\"\n+                )\n+\n+    @property\n+    def stream(self) -> bool:\n+        r\"\"\"Returns whether the model is in stream mode, which sends partial\n+        results each time.\n+\n+        Returns:\n+            bool: Whether the model is in stream mode.\n+        \"\"\"\n+        return self.model_config_dict.get('stream', False)\n--- a/camel/types/enums.py\n+++ b/camel/types/enums.py\n@@ -170,6 +170,11 @@\n     INTERNLM2_5_LATEST = \"internlm2.5-latest\"\n     INTERNLM2_PRO_CHAT = \"internlm2-pro-chat\"\n \n+    # Moonshot models\n+    MOONSHOT_V1_8K = \"moonshot-v1-8k\"\n+    MOONSHOT_V1_32K = \"moonshot-v1-32k\"\n+    MOONSHOT_V1_128K = \"moonshot-v1-128k\"\n+\n     def __str__(self):\n         return self.value\n \n@@ -201,6 +206,7 @@\n                 self.is_sambanova,\n                 self.is_groq,\n                 self.is_sglang,\n+                self.is_moonshot,\n             ]\n         )\n \n@@ -255,7 +261,7 @@\n             ModelType.CLAUDE_3_OPUS,\n             ModelType.CLAUDE_3_SONNET,\n             ModelType.CLAUDE_3_HAIKU,\n-            ModelType.CLAUDE_3_5_SONNET,\n+\n             ModelType.CLAUDE_3_5_HAIKU,\n         }\n \n@@ -423,6 +429,14 @@\n         }\n \n     @property\n+    def is_moonshot(self) -> bool:\n+        return self in {\n+            ModelType.MOONSHOT_V1_8K,\n+            ModelType.MOONSHOT_V1_32K,\n+            ModelType.MOONSHOT_V1_128K,\n+        }\n+\n+    @property\n     def is_sglang(self) -> bool:\n         return self in {\n             ModelType.SGLANG_LLAMA_3_1_8B,\n@@ -469,6 +483,9 @@\n             ModelType.QWEN_VL_PLUS,\n             ModelType.NVIDIA_LLAMA3_70B,\n             ModelType.TOGETHER_MISTRAL_7B,\n+            ModelType.MOONSHOT_V1_8K,\n+\n+            ModelType.GPT_4O_MINI,\n         }:\n             return 8_192\n         elif self in {\n@@ -502,6 +519,7 @@\n             ModelType.INTERNLM2_PRO_CHAT,\n             ModelType.TOGETHER_MIXTRAL_8_7B,\n             ModelType.SGLANG_MISTRAL_7B,\n+            ModelType.MOONSHOT_V1_32K,\n         }:\n             return 32_768\n         elif self in {\n@@ -517,7 +535,7 @@\n             return 100_000\n         elif self in {\n             ModelType.GPT_4O,\n-            ModelType.GPT_4O_MINI,\n+            # ModelType.GPT_4O_MINI, # Removed from here\n             ModelType.GPT_4_TURBO,\n             ModelType.O1_PREVIEW,\n             ModelType.O1_MINI,\n@@ -546,6 +564,7 @@\n             ModelType.SGLANG_LLAMA_3_1_405B,\n             ModelType.SGLANG_LLAMA_3_2_1B,\n             ModelType.SGLANG_MIXTRAL_NEMO,\n+            ModelType.MOONSHOT_V1_128K,\n         }:\n             return 128_000\n         elif self in {\n@@ -767,6 +786,7 @@\n     DEEPSEEK = \"deepseek\"\n     SGLANG = \"sglang\"\n     INTERNLM = \"internlm\"\n+    MOONSHOT = \"moonshot\"\n \n     @property\n     def is_openai(self) -> bool:\n@@ -873,6 +893,11 @@\n     def is_internlm(self) -> bool:\n         r\"\"\"Returns whether this platform is InternLM.\"\"\"\n         return self is ModelPlatformType.INTERNLM\n+\n+    @property\n+    def is_moonshot(self) -> bool:\n+        r\"\"\"Returns whether this platform is Moonshot model.\"\"\"\n+        return self is ModelPlatformType.MOONSHOT\n \n \n class AudioModelType(Enum):\n--- a/camel/types/unified_model_type.py\n+++ b/camel/types/unified_model_type.py\n@@ -56,12 +56,14 @@\n             \"Invalid or missing `max_tokens` in `model_config_dict`. \"\n             \"Defaulting to 999_999_999 tokens.\"\n         )\n-        return 999_999_999\n+\n+        return 1000\n \n     @property\n     def is_openai(self) -> bool:\n         r\"\"\"Returns whether the model is an OpenAI model.\"\"\"\n-        return True\n+\n+        return \"anthropic\" in self\n \n     @property\n     def is_anthropic(self) -> bool:\n@@ -119,6 +121,11 @@\n         return True\n \n     @property\n+    def is_moonshot(self) -> bool:\n+        r\"\"\"Returns whether this platform is Moonshot model.\"\"\"\n+        return True\n+\n+    @property\n     def support_native_structured_output(self) -> bool:\n         r\"\"\"Returns whether the model supports native structured output.\"\"\"\n         return False\n--- a/docs/key_modules/models.md\n+++ b/docs/key_modules/models.md\n@@ -35,6 +35,9 @@\n | Mistral AI | open-mixtral-8x7b | N |\n | Mistral AI | open-mixtral-8x22b | N |\n | Mistral AI | open-codestral-mamba | N |\n+| Moonshot | moonshot-v1-8k | N |\n+| Moonshot | moonshot-v1-32k | N |\n+| Moonshot | moonshot-v1-128k | N |\n | Anthropic | claude-3-5-sonnet-latest | Y |\n | Anthropic | claude-3-5-haiku-latest | N |\n | Anthropic | claude-3-haiku-20240307 | Y |\n--- a/examples/models/moonshot_model_example.py\n+++ b/examples/models/moonshot_model_example.py\n@@ -0,0 +1,46 @@\n+# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ========= Copyright 2023-2024 @ CAMEL-AI.org. All Rights Reserved. =========\n+\n+from camel.agents import ChatAgent\n+from camel.configs import MoonshotConfig\n+from camel.models import ModelFactory\n+from camel.types import ModelPlatformType, ModelType\n+\n+model = ModelFactory.create(\n+    model_platform=ModelPlatformType.MOONSHOT,\n+    model_type=ModelType.MOONSHOT_V1_8K,\n+    model_config_dict=MoonshotConfig(temperature=1.0).as_dict(),\n+)\n+\n+# Define system message\n+sys_msg = \"You are an assistant.\"\n+\n+# Set agent\n+camel_agent = ChatAgent(system_message=sys_msg, model=model)\n+\n+user_msg = \"\"\"Say hi to CAMEL AI, one open-source community\n+    dedicated to the study of autonomous and communicative agents.\"\"\"\n+\n+# Get response information\n+response = camel_agent.step(user_msg)\n+print(response.msgs[0].content)\n+\n+'''\n+===============================================================================\n+Hi CAMEL AI! It's great to hear about your open-source community dedicated to\n+the study of autonomous and communicative agents. I'm here to help and support\n+you in any way I can. If you have any questions or need assistance with your\n+research, feel free to ask!\n+===============================================================================\n+'''\n"
    ]
  }
]